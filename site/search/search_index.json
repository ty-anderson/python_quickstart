{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"My Tech Notes","text":"<p>Click on any of the pages on the left to begin...</p> <pre><code>print('hello world')\n</code></pre>"},{"location":"AI/docs_105_local_llm/","title":"Ollama","text":"<p>Ollama is an open-source LLM runtime environment. </p> <p>Simply put:</p> <ol> <li>Rum Ollama</li> <li>Download the model you'd like to use (mistral, llama3, etc.)</li> <li>Go to the WebUI</li> <li>Select the model you want to use.</li> <li>Chat away!</li> </ol> <p>Download a model: <code>docker exec -it ollama ollama pull mistral</code> or <code>docker exec -it ollama ollama pull llama3</code></p>"},{"location":"Docker/docs_99_docker/","title":"Docker","text":"<p>Docker uses images of a software to create containers that run them. They will create their own isolated environment on the host system. They are typically very easy to install and even easier to uninstall. They can enhance security by isolating the host system from the system running the software.</p> <p>When you find an image you want to use do: <code>docker pull &lt;image_name&gt;</code></p> <p>To see images downloaded use: <code>docker images</code></p> <p>See containers: <code>docker ps</code></p> <p>Run container: <code>docker run image-name -d</code> -d means run in detached mode so when the terminal is closed, the container continues to run</p> <p>Some containers need other info to run, which can be passed after the command: Postgres example: <code>docker run --name postgres_db -e POSTGRES_PASSWORD=postgres -p 5432:5432 -d postgres</code></p> <ul> <li>--name is what the container will be named</li> <li>-p is for port mapping host computer port to image port (first is host, second is container)</li> <li>-e is for adding environment variables</li> </ul> <p>Remove an image by name and tag: <code>docker rmi postgres:latest</code> or by image id <code>docker rmi 123456789abc</code></p>"},{"location":"Docker/docs_99_docker/#docker-compose","title":"Docker Compose","text":"<p>Docker compose is a handy way to run docker with configuration files. This is nice when you have more complex containers to run, and its a difficult to type all the config  every time you want to run it.</p> <p>Important step: Make sure you have the latest version (currently V2).</p> <p>Docker compose V1 was built on python, V2 is built in Go. If you run <code>which docker compose</code>  and it shows the path <code>/usr/bin/docker-compose</code> then you still have V1. Remove it with <code>sudo rm /usr/bin/docker-compose</code>.</p> <p>Create a <code>docker-compose.yaml</code> file. Here's an example:</p> <pre><code>services: # Defines the containers (services)\n  app:\n    image: my_flask_app:latest  # Use an existing image\n    build: ..  # Or build from Dockerfile in the current directory\n    ports:\n      - \"5000:5000\"  # Map host port 5000 to container port 5000\n    volumes:\n      - ./app:/app  # Mount local folder to container folder\n    environment:\n      - FLASK_ENV=development  # Set environment variables\n    depends_on:\n      - db  # Wait for \"db\" service before starting\n\n  db:\n    image: postgres:15\n    restart: always  # Restart if it crashes\n    environment:\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n      POSTGRES_DB: mydatabase\n    volumes:\n      - pg_data:/var/lib/postgresql/data  # Persistent storage for DB\n\nvolumes:\n  pg_data:  # Named volume for PostgreSQL data\n</code></pre> <p>Run container <code>docker compose up -d</code></p> <p>Stop container <code>docker compose down</code></p> <p>Upgrade container <code>docker compose down</code> <code>docker compose pull</code> <code>docker compose up -d</code></p> <p>Restart container: <code>docker compose restart</code></p>"},{"location":"Docker/docs_99_docker/#build-a-docker-image","title":"Build a Docker Image","text":"<p>A Dockerfile is how to build an image. The contents might look something like this:</p> <p>Simple python app: <pre><code>FROM python:3.12-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY . .\nCMD [\"python\", \"app.py\"]\n</code></pre></p> <p>Flask app: <pre><code># Use a lightweight Python image\nFROM python:3.12-slim\n\n# Set the working directory inside the container\nWORKDIR /app\n\n# Copy app files to the container\nCOPY . /app\n\n# Install dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Set environment variables\nENV FLASK_ENV=production\nENV PYTHONUNBUFFERED=1\n\n# Expose the port (optional if using Docker Compose)\nEXPOSE 8090\n\n# Run Gunicorn when the container starts\nCMD [\"gunicorn\", \"-w\", \"4\", \"-b\", \"0.0.0.0:8090\", \"web_app.app:app\"]\n</code></pre></p> <p>When your dockerfile is ready, run: <code>docker build -t my-image-name:tag .</code></p> <p>\u2022 Replace my-image-name with the name you want to give your image. \u2022 Replace tag with an optional version (e.g., latest or v1.0).</p> <p>End example might look like <code>docker build -t my-flask-app:latest .</code></p> <p>If you are building on a Mac, but will run on Linux, when you build an image, specify to run on amd64: <code>docker build --platform linux/amd64 -t yt_download:latest .</code></p> <p>Save the image to a tar file:  <code>docker save -o /Users/tyleranderson/Downloads/yt_downloads_250226.tar yt_download:latest</code></p> <p>Load the file as an image: <code>docker load -i /srv/flask_yt_download/yt_downloads_250226.tar</code></p>"},{"location":"Docker/docs_99_docker/#docker-compose-with-custom-images","title":"Docker Compose with Custom Images","text":"<p>When you use docker compose with an image, you have two options:</p> <ul> <li>To build the image with docker compose when its run, use the <code>build: .</code> option.  This builds the image from the Dockerfile in the directory. <pre><code>services:\n  flask_app:\n    build: .  # This tells Compose to use the Dockerfile in the current directory\n    container_name: flask_gunicorn\n    ports:\n      - \"8090:8090\"\n</code></pre></li> <li>If you already built the image with <code>docker build -t myflaskapp:latest .</code> then you can tell docker compose what image to use: <pre><code>services:\n  flask_app:\n    image: my_flask_app  # Use the existing built image\n    container_name: flask_gunicorn\n    ports:\n      - \"8090:8090\"\n</code></pre></li> </ul>"},{"location":"Docker/docs_99_docker/#docker-image-files","title":"Docker Image Files","text":"<p>Save image to file: <code>docker save -o /path/to/destination/image-name.tar my-image-name:tag</code></p> <p>Load image from file: <code>docker load -i /path/to/destination/image-name.tar</code></p>"},{"location":"Docker/docs_99_docker/#access-docker-container","title":"Access Docker Container","text":"<p>Access the container environment <code>docker exec -it &lt;container-name-or-id&gt; /bin/sh</code> /bin/sh is the terminal experience (shell in this case) you could do /bin/bash</p> <p>Access container logs <code>docker logs &lt;container name or id&gt;</code></p> <p>For continuously showing log output <code>docker logs -f &lt;container-name-or-id&gt;</code></p>"},{"location":"Docker/docs_99_docker/#docker-volumes","title":"Docker Volumes","text":"<p>Files and Docker If you need to use files for anything involving docker, you need to configure volumes.</p> <p>Volumes are a mapping of the docker container file system to the host file system.</p> <p><code>-v /home/svr/immich:/usr/src/app/upload</code></p> <p>Just like port mapping, the first is host and second is container.</p>"},{"location":"Git/basics/","title":"Git Basics","text":"<p>Git is a code repository and version control system. It's great for tracking changes to a code base (or any other text based files).</p> <p>There are many tools that can help with basic git functions. The power really comes from using it in the terminal.</p> <p>Common commands: <pre><code># start a git project in working directory.\ngit init\n\n# add a file to git.\ngit add main.py\n\n# add all files in the directory to git.\ngit add .\n\n# commit your changes.\ngit commit -m \"Write a message here\"\n\n# send your commited git changes to your active remote.\ngit push\n\n# retrieve commited changes from remote that are not in your local repo.\ngit pull\n\n# remove all uncommited changes (USE WITH CAUTION, NO RECOVERY OPTIONS)\ngit reset --hard\n</code></pre></p> <p>The location of the git files is a directory in the root directory of your project. Its usually <code>.git</code>. This is a directory not a file. Remotes will have similar naming like python_quickstart.git. </p>"},{"location":"Git/branches/","title":"Branches","text":"<p>A branch in git is a way to create different versions of your code and keep them separated.</p> <p>Below are some examples of some reasons to create a branch.</p> <pre><code># create a new branch for a feature\ngit checkout -b feature/login-form\n\n# create a branch for a bug fix\ngit checkout -b fix/issue-123-crash\n\n# create a branch for experiments\ngit checkout -b experiment/redesign\n\n# separate production from development\ngit checkout -b production\ngit checkout -b development\n</code></pre> <p>Example workflow: <pre><code># make new branch\ngit checkout -b test_branch\n\n# make change to file\n\n# commit the change.\ngit commit -m \"update message\"\n\n# (optional) push to remote\ngit push\n# or\ngit push origin test_branch\n\n# switch back to main so we can merge the new branch\ngit checkout main\n\n# make sure main is update to date\ngit pull\n\n# merge the branch into main\ngit merge test_branch\n\n# you can also delete the branch afterward if you do not expect to use it\ngit branch -d test_branch\n\n# if it wasn't merged and you still want to delete\ngit branch -D test_branch\n\n# (optional) clean up the remote\ngit push origin --delete test_branch\n</code></pre></p>"},{"location":"Git/remotes/","title":"Git Remotes","text":"<p>A \"remote\" in the git environment is another location to  store a copy of your git project.</p> <p>GitHub is the most common remote.</p> <pre><code># To see all remotes that are in a project\ngit remote -v\n\n# to push commited updates to remote\ngit push\n</code></pre> <p>You can setup multiple remotes on one git project.</p> <pre><code>git remote add &lt;name of remote&gt; &lt;server uri&gt;\n\n# for example:\ngit remote add origin https://github.com/ty-anderson/python_quickstart.git\ngit remote add backup ssh://user@yourserver:/srv/git/myproject.git\n\n# when pushing be explicit to which remote\ngit push &lt;remote name&gt; &lt;branch&gt;\ngit push origin main\ngit push backup master\n</code></pre>"},{"location":"Git/remotes/#self-hosted-git-server","title":"Self Hosted Git Server","text":"<p>If you want to store your git projects on your own server, its pretty simple.</p> <p>Steps:</p> <p>On your server: <pre><code>mkdir -p /srv/git/myproject.git\ncd /srv/git/myproject.git\ngit init --bare\n</code></pre></p> <p>On your local machine: <pre><code>git remote add origin ssh://user@yourserver:/srv/git/myproject.git\ngit push -u origin main\n# origin is name of remote, change to whatever you want. \n# main is name of branch, change to whatever branch you want to commit.\n# the -u sets this remote as the active one for tracking. \n</code></pre></p>"},{"location":"Linux/docs_103_ssh/","title":"SSH","text":"<p>To SSH into a server its <code>ssh username@ipaddress</code> or <code>ssh username@servername</code>.</p> <p>SSH uses port 22.</p> <p>Typically when you SSH into a system, it will ask for your password. Instead of using a password everytime, you can create a ssh key. These keys are essentially  a file that sits on both machines and grants access without needing a password.</p> <p>To generate ssh keys:</p> <ol> <li>On local machine <code>ssh-keygen -t rsa -b 4096</code>. It will ask for a passphrase, but you can leave it empty.</li> <li>Copy the ssh key to the server <code>ssh-copy-id user@server</code></li> </ol> <p>If it worked correctly, you should now be able to login without needing a password.</p> <p>You can run commands through ssh, without logging in: <code>ssh user@server \"sudo chown -R user:user /srv/web_apps\"</code></p>"},{"location":"Linux/docs_104_linux/","title":"Linux","text":""},{"location":"Linux/docs_104_linux/#execute-files","title":"Execute files","text":"<p>To execute file: <code>./path/to/file</code> or <code>source /path/to/file</code></p> <p>Example: <code>./bash_script.sh</code> or <code>source /bash_script.sh</code></p> <ul> <li><code>./bash_script.sh</code> executes the script as a standalone process. This method requires execute permissions.</li> <li><code>source bash_script.sh</code> executes the script in the existing process (doesn't create standalone process). Allows modifying environment variables in the current shell. Allows <code>cd</code> command to change directory.</li> </ul> <p>TLDR: default to using <code>source</code> to execute files.</p>"},{"location":"Linux/docs_104_linux/#file-permissions","title":"File Permissions","text":"<p>How to check file permissions: </p> <ul> <li><code>ls -l</code> - view permissions of files in the directory.</li> <li><code>ls -ld</code> - view permissions of the directory itself.</li> </ul> <p>You will see something like below:</p> <p><code>drwxr-xr-x  2 root root 4096 Feb 12 12:34 /srv/web_apps</code></p> <p>The first section is read, write, execute for owner, group, others. </p> Section Meaning Who it applies to d Directory - rwx read(r), write(w), execute(x) Owner (root) r-x read(r), no write(-), execute(x) Group (root) r-x read(r), no write(-), execute(x) Others (everyone else) <p>To see if you're root do: <code>whoami</code>. If the output is 'root' then you can write. If your user is not root, check if your in the root group <code>groups</code>. If your not in 'root' group, you cannot write.</p>"},{"location":"Linux/docs_104_linux/#alter-permissions","title":"Alter Permissions","text":"<p>Use <code>chmod</code></p> <p>There are 2 main methods, numeric or symbolic. The numeric mode requires memorizing number codes for altering permissions. Symbolic is more straight forward.</p> <ul> <li><code>u+rwx</code> \u2192 Add read, write, execute for owner (<code>u</code>).</li> <li><code>g+r</code> \u2192 Add read for group (<code>g</code>).</li> <li><code>o-r</code> \u2192 Remove read for others (<code>o</code>).</li> </ul> <p>Examples:</p> <ul> <li>Change all permissions: <code>chmod u+rwx,g+r,o-r filename</code></li> <li>Change only owner permissions: <code>chmod u+rwx filename</code></li> <li>Change only group permissions: <code>chmod g-w filename</code></li> <li>Change only other permissions: <code>chmod o-r filename</code></li> <li>Give everyone execute permissions: <code>chmod +x filename</code></li> </ul>"},{"location":"Linux/docs_104_linux/#change-ownership","title":"Change Ownership","text":"<p>Change the owner of the file to a different user: <code>sudo chown -R user:user /srv/web_apps</code></p>"},{"location":"Linux/docs_104_linux/#copy-directory","title":"Copy Directory","text":"<p>Two baked-in commands are <code>scp</code> or <code>rsync</code>.</p> <p>Here's an example with scp: <code>scp -r ./site user@server:/srv/web_apps</code>.</p> <p>rsync is typically recommended over scp.</p> <p>Rsync usually comes on Linux and MacOS. </p> <p>Commands: </p> <ul> <li>Basic = <code>rsync -v /source/file/name.txt /dest/file</code></li> <li>Use literal string (preserve string for special characters) = <code>rsync -v '/source/file/na$me.txt' /dest/file</code></li> <li>Send multiple files = <code>rsync -v /source/file1.txt /source/file2.txt '/dest'</code></li> <li>Send over SSH = <code>rsync -av /source/file/name.txt user@server:/dest/file</code></li> <li>Send over SSH with custom port <code>rsync -avz -e \"ssh -p 2222\" /source/file/name.txt user@server:/dest/file</code></li> <li>Send over SSH with sudo command = <code>rsync -v --rsync-path=\"sudo rsync\" yt_download_image.tar user@server:/srv/flask_yt_download</code></li> <li>Send over entire directory = <code>rsync -avz /source/dir /dest/dir</code></li> <li>Send all files in directory = <code>rsync -avz /source/dir/ /dest/dir</code></li> <li><code>rsync -avz --rsync-path=\"sudo rsync\" -e \"ssh -p 2222\" /Users/tyleranderson/Bonus tyler@anderson.home:/home/tyler/backup</code></li> <li><code>rsync -avz --rsync-path=\"sudo rsync\" /Users/tyleranderson/Bonus tyler@anderson.home:/home/tyler/backup</code></li> </ul> <p>Flags: </p> <ul> <li><code>-v</code> = verbose mode.</li> <li><code>-a</code> = archive mode. saves permissions and timestamps.</li> <li><code>-z</code> = compress files to transfer, lossless compression.</li> <li><code>-e</code> = execute specific command.</li> <li><code>--progress</code> = show transfer progress.</li> <li><code>--delete</code> = remove files from the backup if they were removed from the source.</li> <li><code>--exclude</code> = Exclude specific files like <code>--exclude='*.log' --exclude='/cache/'</code></li> </ul> <p>Copy directory into another:<code>rsync -av ./site user@server:/srv/web_apps/notes</code></p> <p>Copy directory contents into another:<code>rsync -av ./site/ user@server:/srv/web_apps/notes/</code>  (trailing <code>/</code> on the source. Trailing source on the destination doesn't matter)</p>"},{"location":"Linux/docs_104_linux/#backups","title":"Backups","text":"<p>Options: Backup files/folders OR full system backup.</p> <p>Backup folders: daily <code>rsync</code> to a separate disk Full system: weekly <code>dd</code> or <code>timeshift</code></p> <p>Can setup with cron to run regularly.</p>"},{"location":"Linux/docs_104_linux/#tar-files","title":"Tar Files","text":"<p>A tar file <code>.tar</code> is an archive file that stores multiple files and  directories together in one file, without compression. Very common in Linux and Unix based systems. Common for backups and transfers.</p> <ul> <li>Create tar file = <code>tar -cvf archive.tar /path/to/files/</code></li> <li>Extract tar file = <code>tar -xvf archive.tar</code></li> <li>Create compressed tar file = <code>tar -czvf archive.tar.gz /path/to/files/</code></li> <li>Extract compressed tar file = <code>tar -xzvf archive.tar.gz</code></li> <li>List files in tar file = <code>tar -tvf archive.tar</code></li> <li>Extract specific file from tar file = <code>tar -xvf archive.tar file.txt</code></li> </ul> <p>Flags:</p> <ul> <li><code>-c</code> = Crete new archive</li> <li><code>-v</code> = Verbose mode (shows progress)</li> <li><code>-f</code> = Specifies filename (archive.tar)</li> <li><code>-x</code> = Extract</li> <li><code>-z</code> = Compress with gzip</li> </ul>"},{"location":"Linux/docs_104_linux/#philosophy","title":"Philosophy","text":"<p>Best practice for backing up files is called 3-2-1 backup strategy.</p> <ul> <li>3 copies of your data (original and two backups).</li> <li>2 different storage types (external drive, NAS, cloud storage, etc).</li> <li>1 off-site backup, a cloud backup or physical backup at a friends house.</li> </ul>"},{"location":"Linux/docs_104_linux/#encryption","title":"Encryption","text":"<p><code>age</code> is a lightweight focused encryption software for files.  It uses modern cryptography making it more secure, but less featured  than something like <code>gpg</code>.</p> <p>Simple example, one file: <code>age -o secretfile.age -p secretfile.txt</code></p> <p>Simple example, directory (use tar):      - <code>tar -czf myfolder.tar.gz myfolder/</code>     - <code>age -o myfolder.tar.gz.age -p myfolder.tar.gz</code></p> <ul> <li>MacOS Install: <code>brew install age</code></li> <li>Linux Ubuntu Install: <code>sudo apt install age</code></li> </ul>"},{"location":"Linux/docs_104_linux/#removing-sensitive-data","title":"Removing Sensitive Data","text":"<p>Hard-Disk Drives:</p> <p>Securely delete a file with <code>shred</code>. This will overwrite the file with  random data mutliple times, renames the file multiple times to obscure  its original name, then deletes the file. This makes it very difficult  for data recovery tools to extract meaningful information from the files.</p> <p><code>shred -u secret.txt</code></p> <ul> <li><code>-u</code> = truncate and delete the file after shredding.</li> <li><code>-n</code> = number of times to overwrite (defualt is 3) <code>shred -n 10 secret.txt</code></li> </ul> <p>Solid-State Drives:</p> <ol> <li>Use <code>fstrim</code> (Best for Full SSD) \u2705 Best for clearing free space on an SSD</li> </ol> <p>Modern SSDs support TRIM, which tells the SSD to permanently erase deleted data</p> <p><code>sudo fstrim -v /</code></p> <ol> <li>Use srm or wipe (For File-Level Deletion) \u2705 Best for deleting a single file securely (better than <code>shred</code> on SSDs)</li> </ol> <p>\ud83d\udd39 Install srm (Secure Remove)</p> <pre><code>sudo apt install secure-delete  # Debian/Ubuntu\nbrew install srm               # macOS\n</code></pre> <p>\ud83d\udd39 Securely delete a file = <code>srm -v my_secret_file.txt</code> </p> <p>\ud83d\udd39 Use wipe for Directories = <code>wipe -rf my_secret_folder/</code></p>"},{"location":"Linux/docs_199_backups/","title":"Backups","text":"<p>Options: Backup files/folders OR full system backup.</p> <p>Backup folders: daily <code>rsync</code> to a separate disk Full system: weekly <code>dd</code> or <code>timeshift</code></p> <p>Can setup with cron to run regularly.</p>"},{"location":"Linux/docs_199_backups/#philosophy","title":"Philosophy","text":"<p>Best practice for backing up files is called 3-2-1 backup strategy.</p> <ul> <li>3 copies of your data (original and two backups).</li> <li>2 different storage types (external drive, NAS, cloud storage, etc).</li> <li>1 off-site backup, a cloud backup or physical backup at a friends house.</li> </ul>"},{"location":"Linux/docs_199_backups/#copy-directory","title":"Copy Directory","text":"<p>Two baked-in commands are <code>scp</code> or <code>rsync</code>.</p> <p>Here's an example with scp: <code>scp -r ./site user@server:/srv/web_apps</code>.</p> <p>rsync is typically recommended over scp.</p> <p><code>rsync</code> usually comes on Linux and MacOS, it is lightweight, fast, can detect changes in files to do an incremental backup. </p> <p>Commands: </p> <pre><code># Basic save file to another folder\nrsync -v /source/file/name.txt /dest/file\n\n# Use literal string (preserve string for special characters)\nrsync -v '/source/file/na$me.txt' /dest/file\n\n# Send multiple files\nrsync -v /source/file1.txt /source/file2.txt '/dest'\n\n# Send over SSH\nrsync -av /source/file/name.txt user@server:/dest/file\n\n# Send over SSH with custom port\nrsync -avz -e \"ssh -p 2222\" /source/file/name.txt user@server:/dest/file\n\n# Send over SSH with sudo command\nrsync -v --rsync-path=\"sudo rsync\" yt_download_image.tar user@server:/srv/flask_yt_download\n\n# Send over entire directory\nrsync -avz /source/dir /dest/dir\n\n# Send all files in directory. Trailing slash. \n# Trailing slash only matters on source, it doesn't matter on dest\nrsync -avz /source/dir/ /dest/dir\n\n# actual use\nrsync -avz --rsync-path=\"sudo rsync\" /Users/tyleranderson/PycharmProjects tyler@anderson.home:/srv/backup_media\n</code></pre> <p>Flags: </p> <ul> <li><code>-v</code> = verbose mode.</li> <li><code>-a</code> = archive mode. saves permissions and timestamps.</li> <li><code>-z</code> = compress files to transfer, lossless compression.</li> <li><code>-e</code> = execute specific command.</li> <li><code>--progress</code> = show transfer progress.</li> <li><code>--delete</code> = remove files from the backup if they were removed from the source.</li> <li><code>--exclude</code> = Exclude specific files like <code>--exclude='*.log' --exclude='/cache/'</code></li> </ul>"},{"location":"Linux/docs_199_backups/#tar-files","title":"Tar Files","text":"<p>A tar file <code>.tar</code> is an archive file that stores multiple files and  directories together in one file, without compression. Very common in Linux and Unix based systems. Common for backups and transfers.</p> <pre><code># Create tar file (best to use relative path instead of absolute)\nsudo tar -cvf archive.tar path/to/files/\n\n# Extract tar file\nsudo tar -xvf archive.tar\n\n# Create compressed tar file\ntar -czvf archive.tar.gz /path/to/files/\n\n# Extract compressed tar file\ntar -xzvf archive.tar.gz\n\n# List files in tar file\ntar -tvf archive.tar\n\n# Extract specific file from tar file\ntar -xvf archive.tar file.txt\n</code></pre> <p>Flags:</p> <ul> <li><code>-c</code> = Crete new archive</li> <li><code>-v</code> = Verbose mode (shows progress)</li> <li><code>-f</code> = Specifies filename (archive.tar)</li> <li><code>-x</code> = Extract</li> <li><code>-z</code> = Compress with gzip</li> </ul>"},{"location":"Linux/docs_199_backups/#encryption","title":"Encryption","text":"<p><code>age</code> is a lightweight focused encryption software for files.  It uses modern cryptography making it more secure, but less featured  than something like <code>gpg</code>. Age  lets you use passwords or key files.</p> <p>Install:</p> <pre><code># Install\nbrew install age  # MacOS\nsudo apt install age # Linux\n</code></pre> <p>How to use - you have 2 options for encryption.</p> <ol> <li>Password - best for quick encryption, no need to manage keys. Less secure and harder to automate.</li> </ol> <pre><code># encrypt with password, prompt will ask for password\nage -o myfile.txt.age -p myfile.txt\n\n# decrypt with password\nage -d myfile.txt.age &gt; myfile.txt\n</code></pre> <ol> <li>Public/Private key Encryption - Most secure and good for automation.</li> </ol> <pre><code># generate a key file\nage-keygen -o ~/.age-key.txt  # stores key in ~/.age-key.txt\n# This file contains a public and private key that you can use to encrypt files.\n\n# encrypt file using public key (replace PUBLIC_KEY with your public key in the file)\nage -r PUBLIC_KEY -o myfile.txt.age myfile.txt\n\n# decrypt using private key\nage -d -i ~/.age-key.txt myfile.txt.age &gt; myfile.txt\n</code></pre> <p>To use the file directly instead of copying the public key:</p> <pre><code># use head to extract the public key\nage -r $(head -n 1 ~/.age-key.txt) -o myfile.txt.age myfile.txt\n\n# decrypt\nage -d -i ~/.age-key.txt myfile.txt.age &gt; myfile.txt\n</code></pre> <p>Encrypt directory by turning into tar file, then encrypting that file.</p> <pre><code># Simple example, directory (use tar).\ntar -czf myfolder.tar.gz myfolder/\nage -o myfolder.tar.gz.age -p myfolder.tar.gz\n\n# convert folder to tar file, encrypt it with age, one command,\n# no intermediate file created.\ntar -czf - myfolder/ | age -o myfolder.tar.gz.age -p\n\n# decrypt and extract in one command.\nage -d myfolder.tar.gz.age | tar -xz\n</code></pre> <p>Decrypt:</p> <pre><code># decrypt .age file\nage -d myfolder.tar.gz.age &gt; myfolder.tar.gz\n\n# extract the file\ntar -xzf myfolder.tar.gz\n\n# delete archive file\nshred -u myfolder.tar.gz\n</code></pre> <p>Flags:</p> <ul> <li><code>-o &lt;output&gt;</code> = output file name.</li> <li><code>-d</code> = decrypts an encrypted file.</li> <li><code>-p</code> = uses password encryption.</li> <li><code>-r &lt;public-key&gt;</code> = Encrypts using public key.</li> <li><code>-i &lt;identity-file&gt;</code> = uses private key for decryption. </li> </ul> <p>Other notes:</p> <ul> <li>The key file contains both public and private keys.</li> <li>Encryption uses only the public key. </li> <li>Decryption uses both keys</li> </ul>"},{"location":"Linux/docs_199_backups/#removing-sensitive-data","title":"Removing Sensitive Data","text":"<p>Hard-Disk Drives:</p> <p>Securely delete a file with <code>shred</code>. This will overwrite the file with  random data mutliple times, renames the file multiple times to obscure  its original name, then deletes the file. This makes it very difficult  for data recovery tools to extract meaningful information from the files.</p> <p><code>shred -u secret.txt</code></p> <ul> <li><code>-u</code> = truncate and delete the file after shredding.</li> <li><code>-n</code> = number of times to overwrite (defualt is 3) <code>shred -n 10 secret.txt</code></li> </ul> <p>For entire directories, you can use <code>wipe</code>. </p> <p><code>sudo apt install wipe</code> and then <code>wipe -r /path/to/directory/</code> this will remove the directory as well. If you want to keep the directory use  <code>wipe -r /path/to/directory/*</code></p> <p>Solid-State Drives:</p> <ol> <li>Use <code>fstrim</code> (Best for Full SSD) \u2705 Best for clearing free space on an SSD</li> </ol> <p>Modern SSDs support TRIM, which tells the SSD to permanently erase deleted data</p> <p><code>sudo fstrim -v /</code></p> <ol> <li>Use srm or wipe (For File-Level Deletion) \u2705 Best for deleting a single file securely (better than <code>shred</code> on SSDs)</li> </ol> <p>\ud83d\udd39 Install srm (Secure Remove)</p> <pre><code>sudo apt install secure-delete  # Debian/Ubuntu\nbrew install srm               # macOS\n</code></pre> <p>\ud83d\udd39 Securely delete a file = <code>srm -v my_secret_file.txt</code> </p> <p>\ud83d\udd39 Use wipe for Directories = <code>wipe -rf my_secret_folder/</code></p>"},{"location":"Networking/docs_100_networking/","title":"Networking","text":"<p>A network has many different things going on.</p> <p>In networking, devices sit behind routers. Routers have a public IP address that is used in accessing the internet, or a wide area network (WAN). The router also  manages local ip address for all devices through DHCP. </p>"},{"location":"Networking/docs_100_networking/#dhcp","title":"DHCP","text":"<p>DHCP - Dynamic Host Configuration Protocol</p> <p>DHCP is a server responsible for assigning local ip addresses to devices on the network.  It can also have local domain names assigned, frequently setup as <code>home.local</code> or <code>local</code>.</p> <p>You can use DHCP to reserve certain ip addresses for certain devices. You can also assign names that point to a certain device. This way you can refer to  the device on the network by an easy to remember name instead of the ip address. For example 192.168.1.104 -&gt; homeserver.</p> <p>When a device joins a network, it sends a \"DHCP discover\" message.  A DHCP server responds to the message and assigns an IP address and other configuration information.  DHCP also assigns new IP addresses when devices move to new locations. </p>"},{"location":"Networking/docs_100_networking/#dns","title":"DNS","text":"<p>DNS - Domain Name System</p> <p>This translates IP addresses into domain names. Think of a phonebook for the internet. Instead of names to phone numbers it points domain names to ip addresses. There are public DNS servers that route the public domains on the internet. </p>"},{"location":"Networking/docs_100_networking/#local-dns","title":"Local DNS","text":"<p>There are also DNS servers that are run on a router or a local machine. This establishes mappings from names to ip address only on your local network. This makes it so you can setup a device with, say ip <code>192.168.1.104</code> to name <code>mydevice.home</code>. Keep in mind that your  router might have a suffix that attaches to these names, such as <code>.home</code></p>"},{"location":"Networking/docs_100_networking/#ipv4-vs-ipv6","title":"IPv4 vs IPv6","text":""},{"location":"Networking/docs_100_networking/#ipv4","title":"IPv4","text":"<p>consists of devices having a 32-bit address that looks like 192.168.1.100. It consists of public and private addresses, where the public addresses are accessible over the internet and private addresses are only accessible over  a local network. This means for data to get passed from one device over the internet to another device, the protocol Network Area Translation (NAT) is used. NAT will convert every packet of your private ip to the public which costs compute.</p>"},{"location":"Networking/docs_100_networking/#ipv6","title":"IPv6","text":"<ul> <li>128-bit addresses, virtually infinite possible addresses.</li> <li>Device addresses are globally routable (not hidden behind a public IP).</li> <li>Because devices are globally routable, it eliminates the need for NAT  as devices can connect end-to-end. Allowing for better connection speeds for video, audio, peer-to-peer, gaming, etc.</li> <li>Devices can auto-configure with Stateless Address Autoconfiguration (SLAAC), no need for DHCP.</li> <li>Firewalls are more relevant with IPv6 due to addresses being globally routable.</li> </ul> <p><code>ip -6 addr show</code></p> <p>Local address: <code>fe80::</code></p> <ul> <li>Scope: Local network segment (cannot be routed on the internet).</li> <li>Usage: Used for internal network functions like router discovery and neighbor discovery.</li> <li>Common for: Internal device communication.</li> </ul> <p>Global address: Usually starts with <code>2xxx::</code> or <code>3xxx::</code></p> <ul> <li>Scope: Globally routable on the internet.</li> <li>Usage: For internet-facing applications or public communication.</li> <li>This is often the correct address for external access.</li> </ul> <p>Address types:</p> <ul> <li>scope global \u2192 for internet or external routing.</li> <li>scope link \u2192 for local network only.</li> </ul> <p>SSH: ssh user@[fe80::1a2b:3c4d:5e6f%eth0]</p>"},{"location":"Networking/docs_100_networking/#commands","title":"Commands","text":"<p><code>netstat</code> - </p>"},{"location":"Networking/docs_100_servers/","title":"Servers","text":""},{"location":"Networking/docs_100_servers/#web-server","title":"Web Server","text":"<p>Known for serving over HTTP/HTTPS protocol. Typically websites, API's, web services, anything using HTTP(S).</p>"},{"location":"Networking/docs_100_servers/#proxy-server","title":"Proxy Server","text":"<p>Redirects web traffic. A sort of \"middleman\" server that is responsible for directing requests.  For example, Nginx or Caddy can be hosted on a server. The proxy server program, once configured,  will push requests to other servers. Good for load balancing, obscuring other servers IP address. Specifically this reroutes web traffic like HTTP(S).</p>"},{"location":"Networking/docs_100_servers/#file-server","title":"File Server","text":"<p>A file server is a server that is like a central hub to hold files. </p> <p>Some common options: - Samba https://hub.docker.com/r/dperson/samba for your typical file explorer-like experience. - SFTP. - Files over HTTPS aka cloud API's like Google Drive.</p>"},{"location":"Networking/docs_101_caddy/","title":"Caddy","text":"<p>Official site: https://caddyserver.com/</p> <p>docker compose: <pre><code>services:\n  caddy:\n    image: caddy:latest\n    container_name: caddy\n    ports:\n      - \"80:80\"        # HTTP\n      - \"443:443\"\n      - \"8009:8009\"\n    volumes:\n      - ./Caddyfile:/etc/caddy/Caddyfile   # Mount your Caddyfile\n      - ./data:/data                       # Let\u2019s Encrypt certificates\n      - ./config:/config                   # Caddy configuration\n      - /srv/web_apps/notes:/srv/web_apps/notes\n    restart: unless-stopped\n</code></pre></p> <p>Don't forget to open the port(s) in the firewall.</p>"},{"location":"Networking/docs_101_caddy/#reverse-proxy","title":"Reverse-Proxy","text":"<p>Reverse Proxy - software that routes traffic from one endpoint to another, or multiple others.</p> <p>Caddy is very simple and it comes with built in certificate management through Lets Encrypt. </p> <pre><code>&lt;requested-domain&gt; {\n    reverse_proxy &lt;ip and port service is routed to&gt;\n}\n</code></pre>"},{"location":"Networking/docs_101_caddy/#examples-using-caddy","title":"Examples using Caddy:","text":"<p>Very simple config. This says using the host machine ip address, using the protocol on port 80 (http) respond with \"Hello from Caddy\".</p> <pre><code>:80 {\n    respond \"Hello from Caddy\"\n}\n</code></pre> <p>Internal and localhost certificates If you configure sites with local or internal addresses, Caddy will serve them over HTTPS  using a locally-trusted certificate authority with short-lived, auto-renewing certificates.  It even offers to install your unique root into your local trust stores for you. <pre><code>localhost {\n    respond \"Hello from HTTPS!\"\n}\n\n192.168.1.10 {\n    respond \"Also HTTPS!\"\n}\n\nhttp://localhost {\n    respond \"Plain HTTP\"\n}\n</code></pre></p> <p>When requests go to example.com, it will get routed to this reverse proxy, which then pushes it to localhost:5000. Just be sure the DNS records are updated to route the domain to this IP address. <pre><code>example.com {\n    reverse_proxy localhost:5000\n}\n</code></pre></p> <p>Here is Caddy as a reverse proxy doing load balancing, in a round-robin method. <pre><code>example.com {\n    reverse_proxy backend1:5000 backend2:5000 backend3:5000\n}\n</code></pre></p> <p>You can do path based proxying to serve different backends based on the url path. In this  example <code>example.com/api</code> will go to one web server, while <code>example.com/static</code> goes to another. <pre><code>example.com {\n    reverse_proxy /api backend1:5000\n    reverse_proxy /static backend2:5001\n    reverse_proxy /app backend3:5002\n}\n</code></pre></p> <p>You can setup to route subdomains as well. This will retrieve an SSL certificate for all domains added. <pre><code>example.com {\n    reverse_proxy localhost:3000\n}\n\napi.example.com {\n    reverse_proxy localhost:4000\n}\n</code></pre></p> <p>You can also proxy to external services. <pre><code>example.com {\n    reverse_proxy https:/api.example.com\n}\n</code></pre></p> <p>You can also configure domains to redirect to one domain. In this example all requests to  www.example.com will be rerouted to example.com.  <pre><code>example.com www.example.com {\n    reverse_proxy localhost:3000\n}\n</code></pre></p> <p>To run Caddy:</p> <ul> <li>Download to computer</li> <li>Setup config file <code>Caddyfile</code> typically in <code>/etc/caddy/Caddyfile</code></li> <li>Start the Caddy server <code>sudo systemctl start caddy</code></li> </ul> <p>To run in Docker:</p> <ul> <li> <p>Create a docker-compose.yml file. <pre><code>services:\n  caddy:\n    image: caddy:latest\n    container_name: caddy\n    ports:\n      - \"80:80\"        # HTTP\n      - \"443:443\"      # HTTPS\n    volumes:\n      - ./Caddyfile:/etc/caddy/Caddyfile   # Mount your Caddyfile\n      - ./data:/data                       # Let\u2019s Encrypt certificates\n      - ./config:/config                   # Caddy configuration\n    restart: unless-stopped\n</code></pre></p> </li> <li> <p>Create your Caddyfile (make sure to create it in the same location your volume is pointed to). <pre><code>example.com {\n    reverse_proxy backend:3000\n}\n</code></pre></p> </li> <li>Run <code>docker compose up -d</code> to start the server.</li> </ul> <p>Additional considerations to run in Docker:</p> <ul> <li>If you run Caddy in a docker container, <code>localhost</code> will be that container, due to dockers own DNS.</li> <li>If you run your web server that you're routing to in a docker container, you can use that container name     in the caddy file config.</li> </ul> <p>You can also route ports directly.</p> <pre><code>:8443 {\n    reverse_proxy 127.0.0.1:8000\n}\n</code></pre>"},{"location":"Networking/docs_101_caddy/#file-server","title":"File Server","text":"<p>Caddy has an option to serve static files over HTTP. This is not a file server like sFTP because it serves over http or https.</p> <pre><code>healthfin.solutions {\n    root * /srv/website\n    file_server\n}\n</code></pre> <p>Serve different sites with different paths of the same domain.</p> <pre><code>healthfin.solutions {\n    handle_path /notes* {\n        root * /srv/web_apps/notes\n        file_server\n    }\n\n    reverse_proxy homeserver.home:8000\n}\n\n:8009 {\n    root * /srv/web_apps/notes\n    file_server\n}\n</code></pre> <p>This config allows for access to the main server from the root domain, but  also changes to the static site when you add the /notes path.</p> <p>Don't forget you can setup a local DNS server on your machine and setup a  local domain DNS rewrite.</p> <pre><code>anderson.docs {\n    tls internal\n    root * /srv/web_apps/notes\n    file_server\n}\n</code></pre>"},{"location":"Networking/docs_102_dns_server/","title":"DNS Server","text":"<p>A DNS Server is a server that holds domain name records and point to the  corresponding IP addresses. </p>"},{"location":"Networking/docs_102_dns_server/#adguard","title":"Adguard","text":"<p>You can setup your own local DNS server on your network. Frequently these allow you to setup your own name resolutions on your network, but also block certain sites from resolving, which can prevent ads.</p>"},{"location":"Networking/docs_102_dns_server/#concept-1","title":"Concept 1","text":"<p>You can setup your own DNS server on your network. Open source servers include adguard, dnsmasq, and pihole. </p>  graph TD     A[Router]     B[Internet]     C[Home Server]     D[Computer 1]      B &lt;-..-&gt; A     C --Otherwise have your router use its default DNS--&gt; A     D --Resolve DNS Here First--&gt; C"},{"location":"Networking/docs_102_dns_server/#options","title":"Options","text":"<p>You have two options when setting up your own DNS server</p> <ol> <li>Set up individual devices on your network to use self-hosted DNS server</li> <li>Set up your router to point all devices to the self-hosted DNS server.</li> </ol>"},{"location":"Networking/docs_102_dns_server/#setup-adguard","title":"Setup Adguard","text":"<p>Docker compose file:</p> <pre><code>services:\n  adguardhome:\n    container_name: adguardhome\n    image: adguard/adguardhome\n    restart: unless-stopped\n    ports:\n      - \"53:53/udp\"\n      - \"53:53/tcp\"\n      - \"3000:80/tcp\"   # Web UI\n#      - \"443:443/tcp\" # (Optional, for HTTPS UI)\n    volumes:\n      - ./workdir:/opt/adguardhome/work\n      - ./confdir:/opt/adguardhome/conf\n</code></pre> <p><code>sudo docker compose up -d</code> - you now have a DNS server running.</p> <p>To access the web UI you can go to <code>http://&lt;ip address&gt;:3000</code>.</p> <p>To add a local domain, go to Filters&gt;DNS rewrites</p> <p>If you want to do option 1:</p> <p>Go to the device(s) you want to use your DNS server. Go to System Preferences -&gt; Network -&gt; Wifi (or Ethernet) -&gt; DNS. From there add the IP address of your  server running the DNS server.</p> <p>If you want option 2:</p> <p>Go into your router options. Find DNS settings (might be under DHCP Settings).</p> <p>Add your custom server, its a good idea to add a fallback server in case yours goes down. Google is 8.8.8.8 or cloudflare 1.1.1.1.</p>"},{"location":"Python/docs_1_what_is_python/","title":"What is Python","text":""},{"location":"Python/docs_1_what_is_python/#what-is-python","title":"What is Python?","text":"<ul> <li>Python is a general purpose programming language (its good at a lot of things).</li> <li>Its one of the most readable languages that exist. It also has one of the biggest communities which means there are a lot of people and resources to learn.</li> <li>Python is an interpreted language, there is no compiling required.</li> <li>Python is a dynamic language, no need to declare data types.</li> <li>There are several flavors of python, some of the most popular are:<ul> <li>CPython - the typical, most common, flavor of python. This is usually what people refer to when they say \"python\".    This is written in the C programming language and maintained by a large group of individuals.</li> <li>Jython - Instead of the C language, this flavor of python is written in Java.</li> <li>IronPython (aka IPython) - Instead of the C language, this flavor of python is written in .NET.</li> <li>Anaconda - This flavor of python comes with many common data analysis packages pre-installed. This is maintained by the Anaconda, Inc. company. Personally I think this is just CPython with bloat.</li> <li>PyPy - A fast, minimal version of python that uses a JIT compiler. A little more difficult to configure.</li> <li>Brython - A version of python that can run in a web browser, translating python code into JavaScript.</li> </ul> </li> </ul>"},{"location":"Python/docs_2_how_to_use/","title":"How to use Python","text":""},{"location":"Python/docs_2_how_to_use/#summary-steps","title":"Summary Steps","text":"<ol> <li>Create a virtual environment (venv).</li> <li>pip install any libraries to the venv.</li> <li>Write code in a .py file</li> <li>Run the .py file with the venv.</li> </ol> <p>Example:</p> <pre><code># create venv\nPython -m venv venv  \n\n# activate venv\nvenv\\Scripts\\activate  \n\n# install libraries to venv\npip install pandas sqlalchemy \n\n# run your Python file in your venv (if activated)\n. python_file.py  \n</code></pre>"},{"location":"Python/docs_2_how_to_use/#running-python","title":"Running Python","text":"<pre><code>[Python File]          1. write a .py file\n      |\n      v\n[Python Interpreter]   2. run .py file through interpreter\n      |\n      v\n  [Result]\n</code></pre> <ol> <li>Python files end with <code>.py</code> for example <code>new_file.py</code>. Notice all lowercase and underscore format.  Python heavily uses this snake case formatting.</li> <li>In your <code>.py</code> file, write python code and then run it through the interpreter by using the terminal. </li> <li>Reference the interpreter and then the .py file like the example below.</li> <li>Example: <code>C:\\username\\python\\bin\\python.exe path\\to\\your\\python\\file.py</code></li> <li>Note you'll modify this to the correct paths on your computer.</li> </ol>"},{"location":"Python/docs_2_how_to_use/#about-the-package-manager-pip","title":"About the package manager (pip)","text":"<ul> <li>The python community has a place where people can create and upload their own python packages. https://pypi.org/</li> <li>Python has a built-in manager that allows you to install packages from PyPI directly. This manager is called <code>pip</code></li> <li>There are many very popular libraries. Some popular and well utilized ones are:</li> <li>For data there is Pandas, SQLAlchemy,    DuckDB, Polars</li> <li>API handling - Requests, aiohttp</li> <li>Markup parsing - BeautifulSoup4, lxml</li> <li>Web servers - Flask, Django, FastAPI</li> <li>File transfer - Paramiko (SFTP)</li> <li>Environment variable usage - python-dotenv</li> <li>So much more...</li> <li>To make use of these, we need to use <code>pip</code> to install them into our python interpreter. But we don't want to install everything to the same python interpreter.  That would cause it to become bloated if every project used the same interpreter. For this reason, every python project  should have its own interpreter (see Best practices for using python)</li> </ul>"},{"location":"Python/docs_2_how_to_use/#best-practices-for-using-python","title":"Best practices for using python","text":"<ul> <li>Every python project should have its own interpreter, called a virtual environment or shortened to \"venv\". To do this, we need to create a copy of the python interpreter  for each project. PyCharm can manage this for you, or you can do it manually. If using PyCharm, you can skip to the pip commands.</li> <li>To do this manually, you should use the terminal to navigate to the folder your project files will exist in and type <code>python -m venv venv</code>   This runs a python built-in command (<code>python</code>) in module mode (<code>-m</code>) to run the module venv (<code>venv</code>) and names the new interpreter venv (<code>venv</code>).</li> <li>After you create the venv folder with the command, you'll need to activate it to install the libraries. To activate,    run the command <code>venv\\Scripts\\activate</code>. For linux or mac the command is <code>venv/bin/activate</code>.</li> <li>From here you should see the terminal change to <code>(venv) C:\\users\\username\\directory</code>. In PyCharm, you can open the terminal and this will already be setup.</li> <li>Now you can run pip commands like: </li> <li><code>pip install pandas</code> - install a package by using their PyPI name.</li> <li><code>pip install pandas sqlalchemy duckdb</code> - you can install more than one by chaining package names.</li> <li><code>pip uninstall pandas</code> - uninstall package by PyPI name.</li> <li><code>pip install --upgrade pandas</code> - upgrade already installed packages when a new version comes out.</li> <li><code>pip list</code> - show a list of installed packages.</li> <li><code>pip freeze &gt; requirements.txt</code> - save the names and versions of installed packages to a file.    Good for saving and distributing dependencies so others can clone and use your project.</li> <li>Use the venv the same way <code>C:\\path\\to\\venv\\python\\bin\\python.exe path\\to\\your\\python\\file.py</code>. PyCharm will automatically use these commands when you right-click and run a script.</li> </ul>"},{"location":"Python/docs_3_writing_code/","title":"Writing Python Code","text":"<p>Python uses white-space and tabs, instead of curly-braces and semicolons like other common languages.</p> <p>The simplest python script. This will print text to your terminal window. <pre><code>print('This is my first Python script')\n</code></pre></p>"},{"location":"Python/docs_3_writing_code/#data-types","title":"Data Types","text":"<p>Native data types in python include:</p> <ul> <li>Integer <code>int</code> - whole number ie: 5</li> <li>Float <code>float</code>- decimal number ie: 7.9</li> <li>String <code>str</code>- text surrounded by single or double quotes ie: 'This is a string' or \"This is a string\"</li> <li>List, Tuple <code>list, tuple</code>- grouped data combined into a collection that can be iterated over. Lists use square brackets ie: a  list of integers <code>[1, 2, 3, 4]</code>. Tuples use parenthesis ie: <code>(1, 2, 3, 4)</code>. Lists can be mutated and tuples cannot.</li> <li>Dictionary <code>dict</code>- key value pair collection of data, using curly-braces. ie: <code>{'key 1': 'value 1', 'key 2': 'value 2'}</code></li> </ul> <p>Dictionary methods:</p> <ul> <li>Get value from dictionary <code>dict_name['key_name']'</code> or <code>dict_name.get('key_name')</code></li> <li>Loop through the dict with: <pre><code>for k, v in dict_name.items():\n   print(k, v) \n</code></pre></li> </ul> <p>String methods:</p> <ul> <li>f-strings - inject variables into a string with curly braces <code>print(f'Variable = {x}')</code></li> <li>Combine iterable into string <code>''.join(iterable)</code></li> </ul>"},{"location":"Python/docs_3_writing_code/#variables","title":"Variables","text":"<p>Any of the data types can be loaded into a variable that can then be referenced later:</p> <pre><code>x = 14\nvariable_01 = 'This is a variable'\nalso_a_variable = {'foo': 'bar'}\n</code></pre> <p>Variables can be named almost anything, you just can't start a variable with a number and no special characters. </p> <p>You can perform mathematical operations, just like most programming languages.</p> <pre><code>x = 5  # set variable\nx = x + 1  # add 1 to x variable\nx = x * 10  # multiply x by 10\nx = x / 4  # divide x by 4\nx = x ** 2  # raise x to the power of 2\n\nx = 7 // 3  # floor division (divide and round down to the nearest whole number)\n# output: 2.333 but rounds down to 2\n</code></pre> <p>You can also use the math library in the python standard library to perform more complex mathematical functions.</p> <p>Math Standard Library Docs</p>"},{"location":"Python/docs_3_writing_code/#if-statements","title":"If Statements","text":"<p>If statements are great for checking conditions and running code if certain conditions are met.</p> <pre><code>x = 10\nif x &gt; 8:\n    print('x is greater than 8')\nelif x &lt;= 1:\n    print('x is less or equal to 1')\nelse:\n    print('x is between 1 and 8')\n</code></pre> <p>Keep in mind that checking equality should be done with two equal signs <code>==</code> because a single <code>=</code> is for assigning variables.</p> <pre><code>x = 100\n\nif x == 100:\n    print('x is equal to 100')\n</code></pre>"},{"location":"Python/docs_3_writing_code/#loops","title":"Loops","text":"<p>Types of loops: - For - iterate over an object. - While - loop until a condition is met. - List comprehension - single line loop over an object.</p> <p>For loop <pre><code>numbers = [2, 4, 6, 8, 10]\n\nfor number in numbers:\n    print(number)  # will print each number in the list\n</code></pre></p> <p>While loop <pre><code>x = 0\nwhile x &lt; 10:\n    print(x)\n    x = x + 1  # add 1 to x on each loop\n    # this loop will terminate once x is greater than 10\n</code></pre></p> <p>List comprehension - this is like a single line for loop <pre><code>numbers = [2, 4, 6, 8, 10]\n\n# if we want to do a simple change to all the numbers in this list, such as double the amounts, we can use list comprehension\nnumbers = [number * 2 for number in numbers]\n</code></pre></p>"},{"location":"Python/docs_3_writing_code/#comments","title":"Comments","text":"<p>Add comments to your code:</p> <ul> <li><code>#</code> - single line comments</li> <li><code>\"\"\" \"\"\"</code> - multi-line comments</li> </ul> <pre><code># single line comment.\n\n\"\"\"\nMulti like comment\nUse this to describe \nmany things.\n\"\"\"\n</code></pre>"},{"location":"Python/docs_3_writing_code/#importing-other-libraries","title":"Importing other libraries","text":"<p>Python comes with the \"Python Standard Library\" which has a lot of powerful modules ready to go, no installing required.</p> <p>Python Standard Library Docs</p> <p>To use any of these, or to use any libraries installed with pip, they must be imported. This is typically done at the top of the .py file.</p> <pre><code>import os  # standard library, lets you interact with objects on your operating system like files.\nimport pandas as pd  # third-party library installed with pip. This imports the package and renames it as pd\nfrom sqlalchemy import select, insert  # this imports specific parts of the sqlalchemy library, avoiding importing everything.\n</code></pre> <p>Load a csv file into a DataFrame using pandas.</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv('path/to/file.csv')\n</code></pre> <p>Connect to a database with sqlalchemy:</p> <pre><code># import library\nfrom sqlalchemy import create_engine, text\n\n# create an engine object to connect\nengine = create_engine('postgresql+psycopg2://username:password@servername:port/database')\n\n# connect to the database and query a table\nwith engine.connect() as conn:\n    query = 'SELECT * FROM table'\n    result = conn.execute(text(query))\n</code></pre>"},{"location":"Python/docs_3_writing_code/#functions","title":"Functions","text":"<p>Use the <code>def</code> keyword to define a function. You can use parameters to pass data into the function, process the data, and return a result.</p> <pre><code>def example_function():\n    pass\n\n\ndef repeat(message):\n    print(f'You said {message}')  # this will print back your message and uses f-strings to inject your message to another string.\n\n\ndef add_numbers(number_1, number_2):\n    return number_1 + number_2\n</code></pre> <p>You can create dynamic parameters using args and *kwargs <pre><code>def add_numbers(*args):\n    return_value = 0\n    for value in args:\n        return_value = return_value + value\n\n    return return_value\n</code></pre></p>"},{"location":"Python/docs_3_writing_code/#classes","title":"Classes","text":"<p>Classes are good for grouping similar data and functions into one object.  It's great for maintaining state of data, and performing certain operations on that data.</p> <pre><code>import time\n\n\nclass Car:\n  def __init__(self, year, make, model):\n    # This function runs when the class is first used. Good for loading data right at the beginning.\n    self.year = year\n    self.make = make\n    self.model = model\n    self.current_speed = 0\n\n  def accelerate(self):\n    self.current_speed = 10\n    # make car go\n\n  def stop(self):\n    self.current_speed = 0\n    # make care brake\n\n\ncar = Car(2024, 'Tesla', 'Model Y')  # create the car object\ncar.accelerate()                     # run the accelerate function (when it's in a class it's called a method)\ntime.sleep(5)                        # wait 5 seconds\ncar.stop()                           # run the stop method\n</code></pre>"},{"location":"Python/docs_4_building_large_projects/","title":"Building Large Projects","text":"<p>Large python projects can be extremely powerful, but they must be organized a certain way to work properly.</p> <p>A project should always start with a root folder and other folders and files will be housed inside the root folder.</p> <p>The file structure of a project might look something like this: <pre><code>/project_root\n   /db\n       __init__.py\n       model.py\n       connectors.py\n   /views\n       __init__.py\n       views.py\n   /reports\n       __init__.py\n       report_01.py\n       report_02.py\n   .env\n</code></pre></p> <p>Notice other directories have a file called <code>__init__.py</code>. This is a python file that tells python \"this folder is a python module\" which means you can put python files with code inside the folder and use it. With this structure,  you have code in the views.py file like: <pre><code>from db import model\nfrom reports import report_01\n</code></pre></p> <p>If there is code in the <code>__init__.py</code> file, you can just import the module name like <code>import db</code>. If there is a function inside the <code>__init__.py</code> file called <code>new_func</code> you could <code>from db import new_func</code></p>"},{"location":"Python/docs_5_advanced_topics/","title":"Advanced Topics","text":""},{"location":"Python/docs_5_advanced_topics/#dynamic-function-parameters","title":"Dynamic Function Parameters","text":"<p>If you have a function and you want to be able to pass any number of arguments or keyword arguments, you can use args and kwargs.</p> <pre><code>def function_name(*args, **kwargs):\n    # in the function you can access args and kwargs\n    for arg in args:\n        print(arg)\n    for kwarg in kwargs:\n        print(kwarg)\n</code></pre> <p>You can also unpack dictionaries directly into a function call. <pre><code>def greet(name, age):\n    print(f\"Hello, {name}. You are {age} years old.\")\n\ndata = {\"name\": \"Alice\", \"age\": 25}\ngreet(**data)  # Equivalent to greet(name=\"Alice\", age=25)\n</code></pre></p>"},{"location":"Python/docs_5_advanced_topics/#context-managers","title":"Context Managers","text":"<p>In programming there are a lot of instances where you'll need to open something (file, database connection, etc.) and you'll need to make sure to close it to free up resources and to protect the file from being corrupted or altered.</p> <p>A very safe way to handle this is with context managers, using the <code>with</code> keyword.</p> <p>Read, write, append to files.</p> <pre><code># write to file by opening in write mode 'w'\nwith open('file.txt', 'w') as write_file:\n    write_file.write('Hello there')\n# file is closed at this point\n\nwith open('file.txt', 'r') as read_only_file:\n    file_data = read_only_file.read()\n\n# close the file and use the data from it\nprint(file_data)\n\nwith open('file.txt', 'a') as append_file:\n    append_file.write('This is how to append text to file.')\n\nwith open('file.txt', 'rw') as read_write_file:\n    read_write_file.write('info')\n    data = read_write_file.read()\n</code></pre> <p>Safely manage connections to a database.</p> <pre><code>from sqlalchemy import create_engine\n\nengine = create_engine('connection_string')\n\nwith engine.connect() as conn:\n    conn.execute('query here')\n</code></pre> <p>You can create your own objects with context managers.</p> <pre><code>class NewObject:\n    def __init__(self, vars):\n        self.vars = vars\n\n    def __enter__(self):\n        # open the connection using with statement\n        self.open()\n\n    def __exit__(self):\n        # automatically run this on exiting with statement\n        self.close()\n</code></pre>"},{"location":"Python/docs_5_advanced_topics/#decorators","title":"Decorators","text":"<p>Decorators are a way to add extra functionality to other functions.</p> <pre><code>import functools\n\ndef decorator_name(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # do something before the wrapped function\n        wrapper_value = func(*args, **kwargs)\n        # do something after the wrapped function\n        return wrapper_value\n    return wrapper\n</code></pre> <p>To use this defined decorator, it would look something like this: <pre><code>@decorator_name\ndef wrapped_function():\n    # regular function stuff\n</code></pre></p> <p>If you've defined the decorator to also take arguments, you can add them: <pre><code>import functools\n\ndef decorator_name(arg1, arg2):\n    def decorator_wrap(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # do something before the wrapped function\n            wrapper_value = func(*args, **kwargs)\n            # do something after the wrapped function\n            return wrapper_value\n        return wrapper\n    return decorator_name\n\n\n@decorator_name(arg1, arg2)\ndef wrapped_function():\n    # regular function stuff\n</code></pre></p> <p>Here is a more in-depth tutorial on decorators https://realpython.com/primer-on-python-decorators/</p> <p>Some favorite decorators:</p> <pre><code>def time_it(func):\n    \"\"\"Prints the time it takes for a function to run\"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = datetime.datetime.now()\n        return_val = func(*args, **kwargs)\n        print(f'{func.__name__} ran successfully in {datetime.datetime.now() - start_time}')\n        return return_val\n    return wrapper\n</code></pre> <pre><code>def avoid_day_of_week(day_of_week: List[int]):\n    \"\"\"\n    Avoid running the decorated function on certain day of week.\n\n    day_of_week:\n        0 -&gt; Mon\n        1 -&gt; Tue\n        2 -&gt; Wed\n        3 -&gt; Thu\n        4 -&gt; Fri\n        5 -&gt; Sat\n        6 -&gt; Sun\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            current_dt = utc_now()\n            dow = calendar.weekday(current_dt.year, current_dt.month, current_dt.day)\n            if dow in day_of_week:\n                logit(f'Avoiding day of week {calendar.day_abbr[dow]} day integer ({dow})')\n                return\n            else:\n                func(*args, **kwargs)\n\n        return wrapper\n    return decorator\n</code></pre>"},{"location":"Python/docs_5_advanced_topics/#generators","title":"Generators","text":"<p>Generators can be thought of as iterables that are not fully loaded into memory. This allows you can handle the  same data, without worrying about memory consumption.</p> <p>A generator is defined by a function that uses the keyword <code>yield</code>. When a function is used, it is run and then  looses all state afterward. A generator will maintain state and can be called again multiple times.</p> <p>For example, imagine you need to pull API data using every individual for the last 20 years. Instead of loading all dates into one big list, you can create a generator function to calculate it.</p> <pre><code>import datetime\nimport requests\n\n# this is a generator because it uses the keyword yield.\ndef date_generator(start_date):\n    today = datetime.datetime.today()\n    while start_date &lt; today:\n        yield start_date  # this gets returned. If this function is called again, it will start here.\n        start_date = start_date + 1\n\n\nurl = 'example_url.com/api'\nfor date in date_generator(datetime.datetime.date(1990, 1, 1)):\n    response = requests.get(f'{url}/date_param={date}')\n</code></pre> <p>If you don't want to loop over a generator, you can also use the <code>.next()</code> method.</p> <pre><code>gen = generator_function()\nnext_value = gen.next()\n</code></pre> <p>More detailed info here https://wiki.python.org/moin/Generators</p>"},{"location":"Python/docs_5_advanced_topics/#python-import-system","title":"Python Import System","text":"<p>Importing libraries can be tricky in certain cases. The typical use is to just import any python package such as  <code>import sys</code></p> <p>The best option 99% of the time is going to be run scripts in module mode. Read below to learn more about how the python import system works.</p> <p>A python interpreter or virtual environment has a list of directories to look for imports from. It will start to look for the import in the first item of the list and go through each directory  until it finds the import or fails and raises an exception.  </p> <p>If you are running a module that imports another module that you've created, in a directory  outside of the current directory, then you might need to add that directory to the sys.path. </p> <ul> <li>Option 1: Add a pth file in the virtual environment with the path you want to add.</li> <li>Go to venv/lib/python/site-packages/ <li> <p>Add your path such as /home/tyranderson/snfStudyData</p> </li> <li> <p>Option 2: Hardcode the path directly into the activate file. In a venv you can edit the bin/activate file and include: <code>export PYTHONPATH=\"/the/path/you/want\"</code></p> </li> <li> <p>Option 3: Add into python script - within your script you can <code>sys.path.append(\"/the/path/you/want\")</code> but this is temporary and the path will be dropped once the script is done running.  </p> </li> <p>More info here: https://help.pythonanywhere.com/pages/DebuggingImportError</p>"},{"location":"Python/docs_5_advanced_topics/#run-scripts-in-module-mode","title":"Run scripts in module mode","text":"<p>You can run scripts from a venv 2 different ways: 1. As a standalone script     1. This is the format path/to/venv/bin/python path/to/script    2. If this script has imports to other files, it will have import errors. This is not the recommended way!!!!!!! 2. In module mode     1. Path/to/venv/bin/python -m path.to.script</p> <p>Using module mode is considered best practice because it allows all modules to import from the project root properly. If you have multiple python modules (.py files) you are importing from various directories within the project, you will likely have import errors if you try to run using the standalone method. This is why its considered best practice to run in module mode. (PyCharm does this for you when you run a script).</p> <p>Linux <pre><code>cd path/to/project\n. venv/bin/activate -m path.to.Python.file\n</code></pre></p> <p>Windows <pre><code>cd path\\to\\project\nvenv\\Scripts\\activate -m path.to.python.file\n</code></pre></p>"},{"location":"Python/docs_5_advanced_topics/#asyncio","title":"Asyncio","text":"<p>All the code up to this point has been synchronous, meaning everything happens one step at a time. Asynchronous (async) code can be used to handle multiple tasks at the same time.</p> <p>Be aware, async code is more complicated than synchronous in any language, including python. </p> <p>When should I use async code?</p> <p>If your code is...</p> <ul> <li>CPU Bound - use Multi Processing</li> <li>IO bound, fast IO, limited number of connections - use Threading</li> <li>IO bound, slow IO, many connections - use Asyncio</li> </ul> <p>In other words.... <pre><code>if io_bound:\n    if io_very_slow:\n        print('Use asyncio')\n    else:\n        print('Use threads')\nelse:\n    print('Use Multi processing')\n</code></pre></p>"},{"location":"Python/docs_5_advanced_topics/#general-info","title":"General Info","text":"<p>The most important keywords:  <code>async</code> and <code>await</code></p> <p>The most important functions: <code>asyncio.run()</code> and <code>asyncio.gather()</code></p> <ul> <li><code>async</code> - this keyword is used to define a function that will be capable of running async. This turns the  function into whats called a coroutine. If a function is a coroutine (has the async keyword), then it has to  utilize the await keyword as well.</li> <li><code>await</code> - is the keyword that is used to let python know that a function is going to take some time to resolve.  It will stop trying to run the function and move on to another task while it waits for the task to resolve.</li> <li><code>asyncio.run()</code> - this is the most common way to run a coroutine (function with async). A coroutine cannot be  called like a normal function.</li> <li><code>asyncio.gather()</code> - this is not required but is extremely useful and common. <code>gather</code> allows you to run  multiple coroutines at the same time.</li> </ul> <p>The best way to understand async code is to experiment with it. Lets look at some examples.</p> <pre><code>import asyncio\n\n# this is a coroutine\nasync def async_function(request_data):\n    data = await some_async_task(request_data)\n    return data\n\n# this is a coroutine\nasync def main():\n    results = await asyncio.gather(\n      async_function('x'), \n      async_function('y'),\n      async_function('z')\n    )\n    print(results)\n\nif __name__ == '__main__':\n    asyncio.run(main())  # this is how you run a coroutine\n</code></pre>"},{"location":"Python/docs_5_advanced_topics/#cython","title":"Cython","text":"<p>One of the biggest criticisms of python is its performance. When you compare it to statically typed, compiled languages, it doesn't have near the same speed. One option to improve performance is with something called Cython. Cython is an extension of python that allows statically typed python that can be compiled to C code for performance enhancements.</p> <p>Official Cython Docs</p> <p>Example:</p> <pre><code># example.pyx file\n\ndef sum_integers(int n):\n    cdef int i\n    cdef int total = 0\n    for i in range(n):\n        total += i\n    return total\n</code></pre> <p>Create setup.py file:</p> <p><pre><code>from setuptools import setup\nfrom Cython.Build import cythonize\n\nsetup(\n    ext_modules = cythonize(\"example.pyx\")\n)\n</code></pre> run python setup.py build_ext --inplace</p>"},{"location":"Python/docs_5_advanced_topics/#global-interpreter-lock-gil","title":"Global Interpreter Lock (GIL)","text":"<p>To achieve thread safety in python, there is something called the Global Interpreter Lock (GIL). The GIL is  a bit of a double-edged sword because it achieves thread safety, but it also makes python slower due to running everything on one thread. Python 3.13 has introduced an experimental mode where the GIL can be deactivated. The most important thing here for now is to know it exists.</p>"},{"location":"Python/docs_5_advanced_topics/#other-helpful-tips","title":"Other Helpful Tips","text":"<p>Unpacking Iterables is a useful trick. If you have a list or other iterable that you want to perform an operation on you can unpack it with an asterisk.</p> <pre><code># with unpacking\nnumbers = [1, 2, 3, 4]\nprint(*numbers)\n\n# this gives the same result, but requires more code\nnumbers = [1, 2, 3, 4]\nfor number in numbers:\n    print(number)\n</code></pre> <p>The double asterisk ** is for unpacking keyword arguments. This is why they are used in functions with args and kwargs.</p> <pre><code>def function_name(*args, **kwargs):\n    # in the function you can access args and kwargs\n    print(args[0])\n    print(kwargs['item_01'])\n\n\nfunction_name(1, 2, 3, item_01='value 1', item_02='value 2')\n</code></pre>"},{"location":"Python/docs_6_publishing_project/","title":"Publishing a Project","text":"<p>You can create your own python library and publish it to PyPI. First you'll need to create an account and download your API keys. Once you have those established, you can create your project and then:</p> <p>Super summary:</p> <ol> <li>Create the pyproject.toml file and fill out the fields </li> <li>run <code>python -m build</code></li> <li>use twine to send to pypi.</li> </ol> <p>More detailed steps:</p> <ol> <li>Make sure all of your files are created inside a folder structure</li> <li>Create pyproject.toml    <pre><code>[build-system] \nrequires = [\"setuptools\", \"wheel\"] \nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"example_package_YOUR_USERNAME_HERE\"\nversion = \"0.0.1\"\nauthors = [\n{ name=\"Example Author\", email=\"author@example.com\" },\n]\ndescription = \"A small example package\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.8\"\nclassifiers = [\n  \"Programming Language :: Python :: 3\",\n  \"License :: OSI Approved :: MIT License\",\n  \"Operating System :: OS Independent\",\n]\n\n[project.urls]\nHomepage = \"https://github.com/pypa/sampleproject\"\nIssues = \"https://github.com/pypa/sampleproject/issues\"\n</code></pre></li> <li>Make sure all relevant build libs are installed:    <pre><code>pip install --upgrade build\npip install twine\n</code></pre></li> <li>Run the build - <code>python -m build</code></li> <li>Send to PyPI test - <code>twine upload --repository testpypi dist/*</code></li> <li>Send to PyPI - <code>twine upload dist/*</code></li> </ol> <p>Make sure your build dependencies are not stored in your project dependencies.</p> <p>Another TOML file might look like this:</p> <pre><code>[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"py_simple_sharepoint\"  # actual name that will get used for pip install.\nversion = \"0.1.1\"  # anytime you submit an update to PyPI, you must change the version.\ndescription = \"A SharePoint file management tool for python programs.\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.10\"\ndependencies = [  # package dependencies, does install when pip installed\n    \"office365-rest-python-client&gt;=2.6.2\",\n]\n\n[project.optional-dependencies]  # build dependencies (does not install when pip installed)\ndev = [\n    \"build\",\n    \"twine\"\n]\n</code></pre> <p>https://packaging.python.org/en/latest/tutorials/packaging-projects/#creating-the-package-files</p> <p>There are also new tools coming out such as poetry and uv.  The community has started to heavily embrace UV due to its speed and tooling.</p>"},{"location":"Python/docs_7_important_libraries/","title":"Important Libraries - Getting Started","text":""},{"location":"Python/docs_7_important_libraries/#pandas","title":"Pandas","text":"<p>Pandas is a well-known data handling library. It has the ability to extract, manipulate, and load data.</p> <p>The central component to pandas is the DataFrame. This is where tabular data is loaded and lives in memory. <pre><code>import pandas as pd\n\n# blank dataframe\ndf = pd.DataFrame()\n\n# load a dict into a dataframe\nsample_data = {'id', [1, 2, 3], 'value', [200, 300, 400]}\ndf = pd.DataFrame(sample_data)\n\n# load direct from other formats\ndf = pd.read_csv('path/to/csv/file.csv')\ndf = pd.read_parquet('path')\ndf = pd.read_excel('path')\ndf = pd.read_json('path')\ndf = pd.read_sql('sql statement', sqlalchemy_conn)\ndf = pd.read_html('io')\ndf = pd.read_xml('io')\ndf = pd.read_clipboard()\n</code></pre></p> <p>More here https://pandas.pydata.org/docs/reference/io.html</p>"},{"location":"Python/docs_7_important_libraries/#transformations","title":"Transformations","text":"<p>Once you have data loaded into a dataframe, you can perform all kinds of operations on the values. There are generally two ways of performing modifications. Iterating through each value (not recommended) and across columns (called vectorization). A vectorized operation can look like this:</p> <pre><code>df = pd.read_csv('path/to/file.csv') # sample data\n\n# performing math computations across columns. You can overwrite existing columns or create new columns\ndf['int_column_new'] = df['int_column_01'] + df['int_column_02']\ndf['int_column_01'] = df['int_column_01'] - df['int_column_02']\ndf['float_column_new'] = df['float_column_01'] * df['float_column_02']\ndf['float_column_01'] = df['float_column_01'] / df['float_column_02']\n\n# perform string operations\ndf['string_col'] = df['string_col'].str.replace('-', '')\n</code></pre>"},{"location":"Python/docs_7_important_libraries/#filtering","title":"Filtering","text":"<pre><code>df = pd.read_csv('path/to/file.csv') # sample data\n\n# filter where column_name is equal to a value\ndf = df[df['column_name'] == 'certain_value']\n# not equal\ndf = df[df['column_name'] != 'certain_value']\n# substring\ndf = df[df['column_name'].str.contains('partial_string_match')]\n# drop rows where column_name equals any of the list values\ndf = df[~df['column_name'].isin(['list', 'of', 'values'])]\n</code></pre>"},{"location":"Python/docs_7_important_libraries/#change-data-types","title":"Change Data Types","text":"<pre><code>df = pd.read_csv('path/to/file.csv') # sample data\n\ndf['str_num_values'] = df['str_num_values'].astype(int)\ndf['str_num_values'] = df['str_num_values'].astype(str)\ndf['str_num_values'] = df['str_num_values'].astype(float)\n\n# dates\ndf['date_str_values'] = pd.to_datetime(df['date_str_values'])  # gives datetime format\ndf['date_str_values'] = pd.to_datetime(df['date_str_values']).dt.date  # gives just date, no time\n</code></pre> <p>Helpful functions:</p> <p>Merge (like SQL join) Melt (convert columns to rows)</p>"},{"location":"Python/docs_7_important_libraries/#sqlalchemy","title":"SQLAlchemy","text":"<p>SQLAlchemy is a robust database management utility library. There are two main components; Core and ORM. Core is more base level, closer to the database API, while ORM aims to abstract some of the complexity of managing connections with sessions. Generally Core is better for pure database operations while the  ORM is geared toward web applications.</p> <p>Has full compatibility with Postgres, MySQL, SQLite, SQL Server, and Oracle.</p> <p>Connection strings look like this:</p> <pre><code>db_str = 'dialect+driver://username:password@host:port/database'\n\npostgres = \"postgresql+psycopg2://scott:tiger@localhost/public\"\n\nsql_server = 'mssql+pyodbc://host/database?trusted_connection=yes&amp;driver=ODBC+Driver+18+for+SQL+Server&amp;TrustServerCertificate=yes'\n</code></pre> <p>Modeling Tables</p> <pre><code>from sqlalchemy import (create_engine, Table, MetaData, Column, String, \n                        Integer, Double, Date, DateTime, Boolean, func)\n\nengine = create_engine('postgresql+psycopg2://scott:tiger@localhost/public')  # engines are what connects to the db\n\nmeta = MetaData()  # create a metadata object to attach tables to\n\n# model out the table\ntable1 = Table('table_name',\n               meta,\n               Column('id', Integer, primary_key=True),\n               Column('column1', String(255), unique=True),\n               Column('column2', Double),\n               Column('column3', Boolean, default=False),\n               Column('column4', Date),\n               Column('created_date', DateTime,\n                        server_default=func.now()),\n               schema='public'\n               )\n\nmeta.create_all(engine)  # create all the tables that are connected to the metadata object, in the database\n</code></pre> <p>Once you have your model and engine ready, you can connect and start running operations:</p> <p><pre><code>from sqlalchemy import create_engine, text\n\nengine = create_engine('connection_string_to_db')\n\nwith engine.connect() as conn:\n    query = 'SELECT * FROM table_name'\n    result = conn.execute(text(query)).fetchall()\n</code></pre> This will return a list of Row objects which contain the values in tuples (even if you only selected one column). To pull these out of the nested tuples, you can use list comprehension.</p> <pre><code>result = [r[0] for r in result]\n</code></pre> <p>Note that to run a raw SQL query we had to put it into a text() function. This function will sanitize the query to  make sure there is no malicious injection happening. Sometimes it is difficult to transform data values in python into a raw SQL query. SQLAlchemy has objects that can perform the same database functions, but in a more pythonic way.</p> <pre><code>from sqlalchemy import select, insert, update, delete\n\ndf = pd.DataFrame(values)\ndata_dict = df.to_dict(orient='records')  # transform pandas dataframe into a dictionary of values\n\nwith engine.connect() as conn:                              # connect to database\n    insert_stmt = insert(table_object).values(data_dict)    # create the insert statement\n    conn.execute(insert_stmt)                               # execute the statement\n    conn.commit()                                           # commit the changes\n\n    value = conn.execute(select(table_object)).fetchone()\n    values = conn.execute(select(table_object)).fetchall()\n\n    update_stmt = update(user_table).where(user_table.c.id == 5).values(name=\"user #5\")\n    conn.execute(update_stmt)\n\n    delete_stmt = delete(user_table).where(user_table.c.id == 5)\n    conn.execute(delete_stmt)\n    conn.commit()\n</code></pre>"},{"location":"Python/docs_7_important_libraries/#sqlite","title":"SQLite","text":"<p>SQLite is an embeddable OLTP database. In other words, it is a database that can be run in memory or from a file. It is the most common database in the world being used in web apps, phone apps, IoT devices, and more.</p> <p>SQLite Docs</p>"},{"location":"Python/docs_7_important_libraries/#duckdb","title":"DuckDB","text":"<p>DuckDB is similar to SQLite, but is an OLAP database. It is incredibly fast and effective and processing large amounts of data in memory. It can connect to other databases and perform direct queries on it, even being able to  write queries on multiple separate databases  Mix and Match.</p> <p>DuckDB Docs</p>"},{"location":"Python/docs_7_important_libraries/#pyspark","title":"Pyspark","text":"<p>Spark is a distributed data handling library written in Java. It has a python API that allows users to use Spark with Pyspark. In some ways it is very similar to pandas, but it can directly read and write to Delta Lake tables, and it spreads the data handling tasks across multiple machines (distributed).</p> <pre><code>df = spark.createDataFrame(data)\ncurrent_records = spark.sql(\"SELECT COALESCE(MAX(created_date), '2000-01-01') FROM lh_gold_01.fact_cms_star_rating\").first()[0]\n</code></pre>"},{"location":"Python/docs_7_important_libraries/#dotenv","title":"dotenv","text":"<p>dotenv is a lightweight package that allows you to load environment variables into memory from a file. </p> <p><code>pip install python-dotenv</code></p> <p>Create a <code>.env</code> file in your project like:</p> <pre><code>ENV_VAR_NAME='ENV_VAR_VALUE'\nAPI_KEY_01='2JHDKFJH3KF'\n</code></pre> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nenv_var = os.getenv('ENV_VAR_NAME')\napi_key = os.getenv('API_KEY_01')\n</code></pre>"},{"location":"Python/docs_7_important_libraries/#requests","title":"Requests","text":"<p>Requests is a user-friendly way to make HTTP calls. Very good for API's.</p> <p><code>pip install requests</code></p> <pre><code>import requests\n\nurl = 'https://data.cms.gov/provider-data/api/1/metastore/schemas/dataset/items/4pq5-n9py?show-reference-ids=false'\nresponse = requests.get(url)\nprint(response.status_code)\nprint(response.headers)\nprint(response.text)\n</code></pre> <p>Send post requests with auth details and payload.</p> <pre><code>import requests\nfrom io import BytesIO\n\nresponse = requests.get(url, \n                        auth={'user': 'username', 'password': 'password_here'},\n                        data={'payload_json': {'data': 'value'}},\n                        files={'file': BytesIO()})\n</code></pre>"},{"location":"Python/docs_7_important_libraries/#airflow","title":"Airflow","text":"<p>Airflow is an orchestration tool to run and monitor jobs. It allows you to program it in a way that it can have dependencies. For example, only run one script after another has run successfully.</p> <p>Airflow Docs</p>"},{"location":"Python/docs_7_important_libraries/#flask","title":"Flask","text":"<p>Flask is a popular web app framework. It is very lightweight and has many \"plugin\" type packages that are built to  be pieced together to achieve all desired features.</p> <p>Web workers rule of thumb:</p> <ul> <li>Sync workers: ~50 - 100 concurrent users per worker.</li> <li>Async workers: ~500 - 1000+ concurrent users per worker.</li> </ul> <p>Running a flask server:</p> <ul> <li>Standard run Flask: <code>flask --app &lt;.py file&gt; run</code> like <code>flask --app app run</code></li> <li>Run Flask on different port: <code>flask --app app run --port 8080</code></li> <li>Run Flask and expose to network: <code>flask --app web_app/app run --host 0.0.0.0 --port 8080</code></li> <li>Run Flask in prod with gunicorn: <code>gunicorn -w 4 -b 0.0.0.0:8080 web_app.app:app</code></li> </ul>"},{"location":"Python/docs_7_important_libraries/#self-hosting","title":"Self Hosting","text":"<p>Make sure to activate your python interpreter!</p> <p>If you want to host your own flask app, gunicorn is a WSGI pure python server used for production. You'll need to <code>pip install gunicorn</code> and then run the commands.</p> <p>Running locally:</p> <p><code>gunicorn -w 4 'module_name:app_name'</code></p> <p><code>-w</code> is the number of workers, default is 1.</p> <p>If you want to be able to access the site on the network, you need to bind to 0.0.0.0:</p> <p><code>gunicorn -w 4 -b 0.0.0.0:8000 'app:app''</code></p> <p><code>0.0.0.0</code> binds the app to all available network interfaces, making it accessible on your network. This is a special ip address that tells your application to listen on all available network interfaces of the machine, instead of just localhost. </p> <p>To make this accessible from the internet, you need to configure your router to forward traffic that  goes to your chosen port (8000 in this case) to the machine that is running the app. This is done with port forwarding in your router admin settings. Visually it might look like  <code>public_ip:8000 -&gt; server_local_ip:8000</code>. BTW this is called port forwarding.</p> <p>You may need to update firewall config on the server to allow external connections  on port 8000 (linux <code>sudo ufw allow 8000</code>). </p> <p>If your ISP changes your public IP, you make need to use a dynamic DNS (DDNS) service like No-IP or DynDNS.</p> <p>Security Warning: exposing your app to the internet comes with security risks. To mitigate:</p> <ol> <li>Use HTTPS: Serve your app behind a reverse proxy like nginx with an SSL certificate.</li> <li>Restrict Access: Limit access to specific IPs or use authentication to secure your app.</li> <li>Monitor Logs: Monitor your server logs for unusual activity.</li> </ol> <p>Stopping the app:</p> <p>Use <code>pgrep -fl gunicorn</code> to show a list of gunicorn processes with their PIDs.</p> <p>kill by bid: <code>kill &lt;pid&gt;</code> (kill by process ID)</p> <p>OR</p> <p>kill all gunicorn: <code>pkill gunicorn</code> (kill by process name)</p> <p>For a more stable production environment, a process manager is better.</p> <p>Create a systemd service file.</p> <p>Then you can enable/disable/start/stop the app as a service.</p> <pre><code>sudo systemctl enable gunicorn  # auto start on boot\nsudo systemctl start gunicorn  # prevent auto start on boot\nsudo systemctl status gunicorn\nsudo systemctl stop gunicorn\nsudo systemctl restart &lt;service-name&gt;\n</code></pre> <p>When you make changes to a service file, you need to reload the service:</p> <p><code>sudo systemctl daemon-reload</code></p> <p>Flask Docs</p>"},{"location":"Python/docs_7_important_libraries/#fastapi","title":"FastAPI","text":"<p>FastAPI is one of the most performant web frameworks available for python.</p>"},{"location":"Python/docs_7_important_libraries/#other-interesting-libraries","title":"Other Interesting Libraries","text":""},{"location":"Python/docs_7_important_libraries/#locust","title":"Locust","text":"<p>Locust is a load testing framework that can send millions of requests.</p> <p>Locust Docs</p>"},{"location":"Python/docs_7_important_libraries/#reportlab","title":"Reportlab","text":"<p>Creating PDF's can be difficult or require external software like LaTeX or wkhtmltopdf.</p> <p>If you don't want to have these dependencies, reportlab is a pure python library to create PDFs.</p> <p><code>pip install reportlab</code></p> <p>ReportLab includes a low-level API for generating PDF documents directly from Python,  and a higher-level template language\u2014similar to HTML and the template systems used in  web development\u2014called RML. Generally, the second option is usually more convenient  for those who must make exhaustive use of the capabilities of the library when  generating documents. For the rest of the cases, the low-level API that we will  describe in this article will suffice. However, you can find the official  documentation here https://docs.reportlab.com/.</p> <pre><code>from reportlab.pdfgen import canvas\n\nc = canvas.Canvas(\"hello-world.pdf\")\nc.save()\n</code></pre> <pre><code># import the canvas object\nfrom reportlab.pdfgen import canvas\n\n# create a Canvas object with a filename\nc = canvas.Canvas(\"rl-hello_again.pdf\", pagesize=(595.27, 841.89))  # A4 pagesize\n# draw a string at x=100, y=800 points\n# point ~ standard desktop publishing (72 DPI)\n# coordinate system:\n#   y\n#   |\n#   |   page\n#   |\n#   |\n#   0-------x\nc.drawString(50, 780, \"Hello Again\")\n# finish page\nc.showPage()\n# construct and save file to .pdf\nc.save()\n</code></pre> <p>Use standard page sizes like letter, A4, and more.</p> <pre><code>from reportlab.lib.pagesizes import letter\n\nc = canvas.Canvas(\"hello-world.pdf\", pagesize=letter)\n</code></pre> <p>Make shapes in PDF.</p> <pre><code>from reportlab.lib.pagesizes import A4\nfrom reportlab.pdfgen import canvas\n\nw, h = A4\nc = canvas.Canvas(\"shapes.pdf\", pagesize=A4)\nc.drawString(30, h - 50, \"Line\")\nx = 120\ny = h - 45\nc.line(x, y, x + 100, y)\nc.drawString(30, h - 100, \"Rectangle\")\nc.rect(x, h - 120, 100, 50)\nc.drawString(30, h - 170, \"Circle\")\nc.circle(170, h - 165, 20)\nc.drawString(30, h - 240, \"Ellipse\")\nc.ellipse(x, y - 170, x + 100, y - 220)\nc.showPage()\nc.save()\n</code></pre> <p>You can make tables from a pandas dataframe</p> <pre><code>from reportlab.lib.pagesizes import letter, landscape\nfrom reportlab.platypus import SimpleDocTemplate, Table, TableStyle\nfrom reportlab.lib import colors\n\nfilename = \"landscape_table_fit.pdf\"\ndoc = SimpleDocTemplate(filename, pagesize=landscape(letter))\n\n# Example DataFrame\nimport pandas as pd\ndf = pd.DataFrame({\n    \"Column 1\": [\"A\", \"B\", \"C\"],\n    \"Column 2\": [1, 2, 3],\n    \"Column 3\": [4.5, 5.5, 6.5],\n    \"Column 4\": [\"Long Text Example\", \"Another Example\", \"More Text\"],\n    \"Column 5\": [\"X\", \"Y\", \"Z\"]\n})\n\n# Convert DataFrame to list of lists\ndata = [df.columns.tolist()] + df.values.tolist()\n\n# Calculate dynamic column widths to fit the page\npage_width = landscape(letter)[0]\navailable_width = page_width - 40  # Subtract margins\nnum_columns = len(df.columns)\ncol_width = available_width / num_columns\n\n# Create a table with fixed column widths\ntable = Table(data, colWidths=[col_width] * num_columns)\ntable.setStyle(TableStyle([\n    ('BACKGROUND', (0, 0), (-1, 0), colors.lightgrey),\n    ('TEXTCOLOR', (0, 0), (-1, 0), colors.black),\n    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n    ('GRID', (0, 0), (-1, -1), 1, colors.grey),\n]))\n\n# Add the table to the elements list\nelements = [table]\n\n# Build the document\ndoc.build(elements)\n\nprint(f\"PDF saved as {filename}\")\n</code></pre>"},{"location":"Python/docs_8_documentation/","title":"Documentation Tools","text":""},{"location":"Python/docs_8_documentation/#sphinx","title":"Sphinx","text":""},{"location":"Python/docs_8_documentation/#mermaid","title":"Mermaid","text":""},{"location":"Python/docs_8_documentation/#mkdocs","title":"MKDocs","text":""},{"location":"Python/docs_8_documentation/#_1","title":"Documentation Libraries","text":""},{"location":"Python/docs_9_std_lib/","title":"Notes on Standard Library","text":""},{"location":"Python/docs_9_std_lib/#io-package","title":"IO package","text":"<p>https://docs.python.org/3/library/io.html</p> <p>The io module provides Python\u2019s main facilities for dealing with various types of I/O.  There are three main types of I/O: text I/O, binary I/O and raw I/O.  These are generic categories, and various backing stores can be used for each of them.  A concrete object belonging to any of these categories is called a file object.  Other common terms are stream and file-like object.</p> <p>Independent of its category, each concrete stream object will also have various  capabilities: it can be read-only, write-only, or read-write. It can also allow  arbitrary random access (seeking forwards or backwards to any location), or  only sequential access (for example in the case of a socket or pipe).</p>"},{"location":"Python/docs_9_std_lib/#text-io","title":"Text I/O","text":"<p>Text I/O expects and produces str objects. This means that whenever the backing store  is natively made of bytes (such as in the case of a file), encoding and decoding of  data is made transparently as well as optional translation of platform-specific newline characters.</p> <p>The easiest way to create a text stream is with open(), optionally specifying an encoding:</p> <p><code>f = open(\"myfile.txt\", \"r\", encoding=\"utf-8\")</code></p> <p>In-memory text streams are also available as StringIO objects:</p> <p><code>f = io.StringIO(\"some initial text data\")</code></p> <p>StringIO is useful when you need to read or write data to a string buffer as  if it were a file, rather than creating an actual file on disk. It can be  used to create strings that mimic file objects, allowing you to read and  write data to them in the same way you would with a file.</p> <p></p>"},{"location":"Python/docs_9_std_lib/#binary-io","title":"Binary I/O","text":"<p>Binary I/O (also called buffered I/O) expects bytes-like objects and produces bytes objects.  No encoding, decoding, or newline translation is performed. This category of streams can  be used for all kinds of non-text data, and also when manual control over  the handling of text data is desired.</p> <p>The easiest way to create a binary stream is with open() with 'b' in the mode string:</p> <p><code>f = open(\"myfile.jpg\", \"rb\")</code></p> <p>In-memory binary streams are also available as BytesIO objects:</p> <p><code>f = io.BytesIO(b\"some initial binary data: \\x00\\x01\")</code></p>"},{"location":"Python/docs_9_std_lib/#smtp-sending-emails","title":"SMTP - Sending Emails","text":"<p>You can send emails directly through python using the simple mail transmission protocol.</p> <pre><code>import smtplib\nfrom email.message import EmailMessage\n\nmsg = EmailMessage()\nmsg['From'] = 'email@address.com'\nmsg['To'] = 'recipient@address.com'\nmsg['Subject'] = 'This is an Email'\n\nmsg.set_content('Body of email here.')\n\nmailserver = smtplib.SMTP('smtp.office365.com', 587)  # connect to email service\nmailserver.starttls()  # starts TLS (security protocol that encrypts data sent over the internet\nmailserver.login(msg['From'], 'password')  # login to the email service account\nmailserver.sendmail(msg['From'], msg['To'], msg.as_string())  # send the email\nmailserver.quit()\n</code></pre> <p>Mail with attachment</p> <pre><code>import os\nimport smtplib\nfrom email.message import EmailMessage\n\nmsg = EmailMessage()\nmsg['From'] = 'email@address.com'\nmsg['To'] = 'recipient@address.com'\nmsg['Subject'] = 'This is an Email'\n\nmsg.set_content('Body of email here.')\n\nfile = 'path/to/file.txt'\nextension = os.path.basename(file).split('.')[1]\nwith open(file, 'rb') as f:\n    file_data = f.read()\nmsg.add_attachment(file_data, maintype='application', subtype=extension, filename=os.path.basename(file))\n\nmailserver = smtplib.SMTP('smtp.office365.com', 587)  # connect to email service\nmailserver.starttls()  # starts TLS (security protocol that encrypts data sent over the internet\nmailserver.login(msg['From'], 'password')  # login to the email service account\nmailserver.sendmail(msg['From'], msg['To'], msg.as_string())  # send the email\nmailserver.quit()\n</code></pre> <p>You can also send nicely formatted HTML emails with MIME</p> <pre><code>import smtplib\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\n\nmsg = MIMEMultipart('mixed')\nmsg['From'] = 'email@address.com'\nmsg['To'] = 'recipient@address.com'\nmsg['Subject'] = 'Subject Here'\n\nhtml_body = MIMEMultipart('alternative')\n\nhtml_body_text = MIMEText('&lt;html&gt;&lt;div&gt;HTML Content here&lt;/div&gt;&lt;/html&gt;', 'html')\nhtml_body.attach(html_body_text)\n\n# Attach the multipart/alternative part to the multipart/mixed message\nmsg.attach(html_body)\n\n# Remaining code for sending the email\nmailserver = smtplib.SMTP('smtp.office365.com', 587)\nmailserver.starttls()\nmailserver.login(msg['From'], 'password here')\nmailserver.send_message(msg)\nmailserver.quit()\n</code></pre>"},{"location":"Python/docs_9_std_lib/#python-http-server","title":"Python HTTP Server","text":"<p>Python has an HTTP server built into the standard library.</p> <p>Serving static files: <code>python -m http.server 8008</code> this will look for an index.html file to start serving, and can be accessed on <code>localhost:8008</code>.</p>"},{"location":"Python/docs_9_std_lib/#python-on-ios-and-android","title":"Python on iOS and Android","text":"<p>This info comes from this article Medium Article</p> <p>Python 3.13 has brought official support for iOS and Android as a platform.  On iOS, since Apple restricts being able to install system resources or run binaries,  developers are forced to run python in embedded mode. In other words, writing native iOS applications and embedding a python interpreter using <code>libPython</code>. This bundles the python interpreter and your code into a bundle that gets uploaded to the App store.</p> <p>Here is the offical docs https://docs.python.org/3/using/ios.html</p>"},{"location":"Video%20Processing/docs_200_ffmpeg/","title":"FFmpeg","text":"<p>ffmpeg is a video editing command tool. </p>"},{"location":"Video%20Processing/docs_200_ffmpeg/#how-a-video-works","title":"How a Video Works","text":"<ol> <li>A video file consists of a container (like MP4, MKV) and encoded video/audio streams inside.</li> <li>The codec is what encodes (compresses) the video/audio when saving it and decodes (decompresses) it when playing it.</li> </ol>"},{"location":"Video%20Processing/docs_200_ffmpeg/#converting-a-video-file-transcoding","title":"Converting a Video File (Transcoding)","text":"<p>If you want to convert a video file from one format to another (e.g., MKV to MP4 or H.264 to H.265),  you use transcoding software (like HandBrake, FFmpeg, or VLC). The process involves:</p> <ol> <li>Decoding (using the current codec).</li> <li>Re-encoding (using a new codec or settings).</li> <li>Packaging the video/audio into a new container format.</li> </ol> <p>For example:</p> <ul> <li>MKV (H.264 + AAC) \u2192 MP4 (H.264 + AAC) \u2192 Just a container change (remuxing, no transcoding needed).</li> <li>MP4 (H.264) \u2192 MP4 (H.265) \u2192 A full re-encoding is required.</li> </ul> <p>So while codecs are a part of the conversion process, they don't handle file conversion by themselves\u2014you need transcoding software for that.</p> <ul> <li> <p>Codec - software that compresses or decompresses media files (audio or video). Determines the encoding.  Important for compression, compatability, and streaming efficiency.</p> </li> <li> <p>Video Codecs - These compress video data to reduce file size while maintaining visual quality.</p> </li> <li> <p>H.264 (AVC) \u2013 Most common, widely supported, good balance of quality and size.</p> </li> <li>H.265 (HEVC) \u2013 More efficient than H.264 (better quality at smaller sizes), but requires more processing power.</li> <li>VP9 \u2013 Open-source alternative to H.265, widely used on YouTube.</li> <li>AV1 \u2013 Newer, even better compression than VP9 and H.265, but requires more CPU power to decode.</li> <li> <p>MPEG-2 \u2013 Older, used in DVDs and some broadcast TV.</p> </li> <li> <p>Audio Codecs - These compress audio data for storage and streaming.</p> </li> <li> <p>AAC \u2013 Common in MP4 files, better quality than MP3 at the same bitrate.</p> </li> <li>MP3 \u2013 Universal compatibility, but lower efficiency.</li> <li>Opus \u2013 High efficiency, used in VoIP and streaming.</li> <li>FLAC \u2013 Lossless, used for high-quality music storage.</li> <li>Dolby Digital (AC-3) / DTS \u2013 Used in surround sound systems.</li> </ul>"},{"location":"Video%20Processing/docs_200_ffmpeg/#codec-vs-container-mkv-mp4","title":"Codec vs. Container (MKV, MP4)","text":"<ul> <li>Container formats (like MKV, MP4, AVI, MOV) hold video, audio, and subtitle tracks.</li> <li>Codecs determine how the actual video and audio inside the container are compressed and played.</li> <li>Example: An MP4 file can contain H.264 video + AAC audio.</li> </ul> <p>So, when choosing a video format, you need to consider both the container (MKV, MP4, etc.) and the codec (H.264, H.265, etc.).</p>"},{"location":"Video%20Processing/docs_200_streaming/","title":"Streaming","text":"<p>The \"best\" streaming protocol depends on the use case. Here\u2019s a breakdown of what\u2019s ideal for different scenarios:</p>"},{"location":"Video%20Processing/docs_200_streaming/#best-streaming-protocols-by-use-case","title":"Best Streaming Protocols by Use Case","text":"<p>On-Demand Video Streaming (e.g., Netflix, YouTube)</p> <ul> <li>Best: DASH (MPEG-DASH) or HLS</li> <li>Why? Adaptive bitrate streaming (ABR) ensures smooth playback across different devices and network conditions.</li> <li>Used By: Netflix, YouTube (DASH), Apple (HLS), Amazon Prime Video (DASH + HLS)</li> </ul> <p>Live Streaming (e.g., Twitch, YouTube Live)</p> <ul> <li>Best: HLS (Low-Latency), DASH (LL-DASH), or WebRTC for real-time interactivity</li> <li>Why? HLS and DASH offer scalability, while WebRTC provides ultra-low latency for live interaction.</li> <li>Used By: Twitch (RTMP for ingest, HLS for playback), YouTube Live (DASH + HLS)</li> </ul> <p>Real-Time Communication (e.g., Zoom, Google Meet)</p> <ul> <li>Best: WebRTC</li> <li>Why? Peer-to-peer, ultra-low latency, no need for external servers.</li> <li>Used By: Google Meet, Zoom, Microsoft Teams</li> </ul> <p>Professional Broadcast / Contribution (e.g., TV Networks, Sports)</p> <ul> <li>Best: SRT or RTMP (for ingest)</li> <li>Why? SRT provides reliable, high-quality, low-latency transmission, while RTMP is still widely supported for ingestion.</li> <li>Used By: ESPN, news networks, live sports broadcasting</li> </ul>"},{"location":"Video%20Processing/docs_200_streaming/#what-does-netflix-use","title":"What Does Netflix Use?","text":"<p>Netflix uses MPEG-DASH for most of its streaming, alongside HLS for Apple devices. Their stack includes:</p> <ul> <li>DASH (MPEG-DASH) \u2013 Adaptive bitrate streaming for optimal playback across devices.</li> <li>HLS (HTTP Live Streaming) \u2013 For Apple devices, since Apple mandates HLS.</li> <li>CDN Optimization \u2013 Uses its own Open Connect CDN to reduce latency and improve video delivery.</li> <li>Per-Title Encoding \u2013 Dynamically adjusts compression based on the content to optimize quality vs. bandwidth.</li> </ul> <p>So HLS can be used to stream an mp4 file to another device?</p> <p>Yes, HLS (HTTP Live Streaming) can be used to stream an MP4 file to another device, but it requires some processing first. </p>"},{"location":"Video%20Processing/docs_200_streaming/#how-hls-streams-an-mp4-file","title":"How HLS Streams an MP4 File","text":"<p>Segment the MP4 File </p> <ul> <li>HLS requires the video to be split into small chunks (typically <code>.ts</code> files, though newer versions support fragmented MP4 <code>.m4s</code>).</li> <li>A tool like FFmpeg can do this.</li> </ul> <p>Create an M3U8 Playlist </p> <ul> <li>HLS uses a <code>.m3u8</code> file (a playlist) that lists the segmented video files.</li> <li>The player downloads and plays the segments in order.</li> </ul> <p>Host the Segments &amp; Playlist on a Server </p> <ul> <li>You need an HTTP server (e.g., Nginx, Apache, or a cloud service like S3) to serve the files.</li> </ul> <p>Play the Stream on Another Device </p> <ul> <li>Open the <code>.m3u8</code> URL in a compatible player (e.g., Safari, VLC, ExoPlayer, or hls.js in a browser).</li> </ul>"},{"location":"Video%20Processing/docs_200_streaming/#example-convert-mp4-to-hls-using-ffmpeg","title":"Example: Convert MP4 to HLS Using FFmpeg","text":"<p><pre><code>ffmpeg -i input.mp4 -codec: copy -start_number 0 -hls_time 10 -hls_list_size 0 -f hls output.m3u8\n</code></pre> - <code>-hls_time 10</code> \u2192 Splits video into 10-second segments. - <code>-hls_list_size 0</code> \u2192 Keeps all segments in the playlist. - <code>output.m3u8</code> \u2192 The playlist file.</p>"},{"location":"Video%20Processing/docs_200_streaming/#how-to-serve-the-stream","title":"How to Serve the Stream","text":"<ol> <li>Move the <code>output.m3u8</code> and <code>.ts</code> files to a web server.</li> <li> <p>Access the stream via: <pre><code>http://yourserver.com/path/output.m3u8\n</code></pre></p> </li> <li> <p>Play it in:</p> </li> </ol> <p>Web browser with <code>hls.js</code> OR    VLC media player (Open Network Stream) OR    iOS Safari (built-in HLS support)</p>"},{"location":"Video%20Processing/docs_200_streaming/#dash-command-with-ffmpeg","title":"\ud83c\udfaf DASH Command with FFmpeg","text":"<p>Run: <pre><code>ffmpeg -i vid_file.mp4 -codec: copy -map 0 -f dash output.mpd\n</code></pre></p>"},{"location":"Video%20Processing/docs_200_streaming/#explanation-of-flags","title":"\ud83d\udd0d Explanation of Flags:","text":"<ul> <li><code>-i vid_file.mp4</code> \u2192 Input video file.</li> <li><code>-codec: copy</code> \u2192 Copies the streams without re-encoding.</li> <li><code>-map 0</code> \u2192 Ensures all streams (audio &amp; video) are included.</li> <li><code>-f dash</code> \u2192 Specifies DASH output format.</li> <li><code>output.mpd</code> \u2192 The DASH manifest file.</li> </ul>"},{"location":"Video%20Processing/docs_200_streaming/#dash-output-files","title":"\ud83d\udcc2 DASH Output Files","text":"<p>This will generate:</p> <ul> <li><code>output.mpd</code> \u2192 The DASH manifest (index) file.</li> <li>Segmented <code>.m4s</code> video/audio chunks.</li> </ul>"},{"location":"Video%20Processing/docs_200_streaming/#serving-dash-with-python-http-server","title":"\ud83d\udda5\ufe0f Serving DASH with Python HTTP Server","text":"<p>Just like HLS, you can serve DASH using:</p> <pre><code>python3 -m http.server 9000\n</code></pre> <p>Then play in VLC: <pre><code>http://localhost:9000/output.mpd\n</code></pre> \u2714\ufe0f VLC supports DASH, so this should work without additional configuration.</p>"},{"location":"Video%20Processing/docs_200_streaming/#play-dash-in-a-browser","title":"\ud83c\udf10 Play DASH in a Browser","text":"<p>Browsers don\u2019t natively support DASH, so you need a DASH player like dash.js.</p> <p>To embed DASH in HTML, use: <pre><code>&lt;video id=\"dashPlayer\" controls&gt;\n    &lt;source src=\"http://localhost:9000/output.mpd\" type=\"application/dash+xml\"&gt;\n&lt;/video&gt;\n&lt;script src=\"https://cdn.dashjs.org/latest/dash.all.min.js\"&gt;&lt;/script&gt;\n&lt;script&gt;\n    var player = dashjs.MediaPlayer().create();\n    player.initialize(document.querySelector(\"#dashPlayer\"), \"http://localhost:9000/output.mpd\", true);\n&lt;/script&gt;\n</code></pre></p>"},{"location":"Video%20Processing/docs_200_streaming/#advanced-dash-multi-bitrate-encoding","title":"\ud83d\udccc Advanced DASH: Multi-Bitrate Encoding","text":"<p>To support adaptive streaming, generate multiple bitrates:</p> <pre><code>ffmpeg -i vid_file.mp4 \\\n  -map 0:v -map 0:a \\\n  -b:v:0 2000k -b:v:1 1000k \\\n  -s:v:0 1920x1080 -s:v:1 1280x720 \\\n  -c:v libx264 -c:a aac \\\n  -f dash -seg_duration 4 -use_timeline 1 -use_template 1 \\\n  output.mpd\n</code></pre> <p>\u2714\ufe0f This creates multiple video quality options that DASH players can switch between dynamically.</p>"},{"location":"Video%20Processing/docs_200_streaming/#summary","title":"\ud83d\ude80 Summary","text":"<ul> <li>Basic DASH: <code>ffmpeg -i vid_file.mp4 -codec: copy -map 0 -f dash output.mpd</code></li> <li>Serve DASH: <code>python3 -m http.server 9000</code></li> <li>Play in VLC: <code>http://localhost:9000/output.mpd</code></li> <li>Play in Browser: Use <code>dash.js</code></li> <li>Adaptive Streaming: Encode multiple bitrates with <code>-b:v</code> and <code>-s:v</code></li> </ul>"}]}