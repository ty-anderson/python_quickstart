{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"My Tech Notes","text":"<p>Click on any of the pages on the left to begin...</p> <pre><code>print('hello world')\n</code></pre>"},{"location":"AI/docs_105_local_llm/","title":"Ollama","text":"<p>Ollama is an open-source LLM runtime environment.</p>"},{"location":"AI/docs_105_local_llm/#run-with-docker-compose","title":"Run with Docker Compose","text":"<p>Here is a docker compose that runs it all.  Save this to a file called <code>docker-compose.yaml</code>.</p> <pre><code>services:\n  ollama:\n    image: ollama/ollama:latest\n    container_name: ollama\n    restart: unless-stopped\n    ports:\n      - \"11434:11434\"   # Ollama API exposed\n    volumes:\n      - ollama_data:/root/.ollama  # Persist models\n\n  open-webui:\n    image: ghcr.io/open-webui/open-webui:main\n    container_name: open-webui\n    restart: unless-stopped\n    ports:\n      - \"3000:8080\"   # WebUI runs here\n    environment:\n      - OLLAMA_API_BASE_URL=http://ollama:11434\n    depends_on:\n      - ollama\n\nvolumes:\n  ollama_data:\n</code></pre> <p>Run the docker compose file with <code>docker compose up -d</code>.</p>"},{"location":"AI/docs_105_local_llm/#access-via-webui","title":"Access via WebUI","text":"<p>You can go to https://localhost:3000 to use the web ui.</p>"},{"location":"AI/docs_105_local_llm/#api-call","title":"API Call","text":""},{"location":"AI/docs_105_local_llm/#raw-api-call","title":"Raw API Call","text":"<pre><code>### Generate text with llama3\nPOST http://localhost:11434/api/generate\nContent-Type: application/json\n\n{\n  \"model\": \"llama3\",\n  \"prompt\": \"Write a haiku about data pipelines\"\n}\n</code></pre>"},{"location":"AI/docs_105_local_llm/#python-api-call","title":"Python API Call","text":"<pre><code>import json\nimport requests\n\n\ndef ask_ai(prompt):\n   \"\"\"Ask local AI model a question and get a streaming response.\"\"\"\n   url = \"http://localhost:11434/api/generate\"\n   payload = {\n       \"model\": \"llama3\",\n       \"prompt\": prompt\n   }\n   headers = {\"Content-Type\": \"application/json\"}\n   return requests.post(url, json=payload, headers=headers, stream=True)\n\nresponse = ask_ai(\"What is the number pi?\")\n\nfor line in response.iter_lines():\n   if line:\n       decoded_json = json.loads(line.decode(\"utf-8\"))\n       print(decoded_json['response'], end='')\n</code></pre>"},{"location":"AI/docs_105_local_llm/#download-other-models","title":"Download Other Models","text":"<p>Available models can be found here: https://registry.ollama.ai/search</p> <pre><code>POST http://localhost:11434/api/pull\nContent-Type: application/json\n\n{\n  \"name\": \"mistral\"\n}\n</code></pre> <p>or download a model with the docker command:  <pre><code># download mistral model\ndocker exec -it ollama ollama pull mistral\n\n# download llama3 model\ndocker exec -it ollama ollama pull llama3\n</code></pre></p> <p>For more info on the REST API definition: https://github.com/ollama/ollama/blob/main/docs/api.md</p>"},{"location":"AI/mcp/","title":"MCP","text":"<ul> <li>It\u2019s the decision-making + orchestration layer between an LLM and the outside world.</li> <li>Whether it\u2019s a server, a local script, or a function call depends on your use case.</li> </ul>"},{"location":"AI/mcp/#mcp-is-simply-a-decision-making-ai-process","title":"MCP is simply a decision making AI process.","text":"<p>Conceptually, MCP is simply a decision\u2011making AI process that:</p> <ol> <li>Receives a request from the user.</li> <li>Understands the available capabilities (tools, APIs, data).</li> <li>Chooses what to do \u2014 which tools to use, in what order, and with what parameters.</li> <li>Executes those tools and feeds results back into the reasoning loop.</li> <li>Produces the final answer or action.</li> </ol> <p>You can think of it like:</p> <ul> <li>Without MCP \u2192 LLM just chats, no actions.</li> <li>With MCP \u2192 LLM chats and decides how to use other resources to accomplish a goal.</li> </ul> <p>Whether it lives in:</p> <ul> <li>A local script</li> <li>A web server</li> <li>A cloud service</li> </ul> <p>\u2026doesn\u2019t change the concept \u2014 the \u201cMCP\u201d part is the orchestration brain.</p>"},{"location":"AI/mcp/#use-cases","title":"Use Cases","text":"<ul> <li>MCP is a great decision-maker when things are ambiguous or user-driven.</li> <li>But in a strict, well-bounded pipeline, procedural code will always win for speed, cost, and control.</li> </ul>"},{"location":"AI/mcp/#simple-how-to","title":"Simple How-To","text":"<ol> <li>Pick your LLM you want to use for MCP.</li> <li>Define the tools you want the model to have access to (python functions).</li> <li>Give the model access to those tools.</li> <li>Run.</li> </ol> <p>Demo....</p>"},{"location":"AI/mcp/#what-kind-of-server-is-an-mcp-server","title":"What kind of server is an MCP server?","text":"<p>An MCP server is not as complex as you might think. It's really just:</p> <p>A web server that hosts your MCP logic and exposes it over an API so other apps (or people) can use it.</p>"},{"location":"AI/mcp/#why-you-might-make-mcp-into-a-server","title":"Why you might make MCP into a server","text":"<ul> <li>You want multiple clients (web apps, mobile apps, scripts) to use the same tools and orchestration logic.</li> <li>You want a centralized place to manage:<ul> <li>API keys</li> <li>Data sources</li> <li>Permissions</li> <li>Logging and monitoring of tool calls</li> </ul> </li> <li>You want it always running instead of firing up the Python script each time.</li> </ul>"},{"location":"AI/mcp/#what-it-looks-like","title":"What it looks like","text":"<p>If we turned our current LangGraph example into a server:</p> <ul> <li>The LangGraph agent would still decide which tools to run.</li> <li>The tools would still be Python functions calling APIs or databases.</li> <li>But instead of calling <code>.invoke()</code> in a local script, you\u2019d send a POST request like:</li> </ul> <pre><code>POST /ask\n{\n  \"question\": \"What's the population of Chicago and the weather there?\"\n}\n</code></pre> <p>And the server would respond with the final text answer.</p> <p>So yes \u2014 an \u201cMCP server\u201d is basically:</p> <pre><code>Web server + MCP logic + LLM + tools\n</code></pre> <p>The \u201cMCP\u201d part is still the decision-making layer, the \u201cserver\u201d part is just about how it\u2019s deployed.</p> <pre><code>\"\"\"\nMCP Demo\n--------\n\nThis is a demo of how to use LangChain with MCP.\n\nTo run this demo, first install the dependencies: ``uv add langchain-anthropic python-dotenv langgraph``\n\n\n\"\"\"\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.tools import tool\nfrom langgraph.prebuilt import create_react_agent\n\nload_dotenv()\n\n# 1. Define your LLM\nllm = ChatAnthropic(\n    model=\"claude-3-opus-20240229\",\n    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n    temperature=0  # tells the llm how varied responses should be. 0=always same answer (highest probability).\n)\n\n# 2. Define a tool\n@tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Gets the weather for a given city.\"\"\"\n    print(f\"[Tool] Getting weather for {city}\")\n    return f\"The weather in {city} is sunny.\"\n\n@tool\ndef get_population(city: str) -&gt; str:\n    \"\"\"Gets the population of a given city.\"\"\"\n    return f\"The population of {city} is about 3 million.\"\n\n\n@tool\ndef get_latitude_and_longitude(city: str) -&gt; str:\n    \"\"\"Gets the latitude and longitude of a given city.\"\"\"\n    return f\"The population of {city} is about 3 million.\"\n\n\n\n# 3. Create the agent (LangGraph style)\nagent = create_react_agent(llm, [get_weather, get_population, get_latitude_and_longitude], debug=True)\n\n# 4. Invoke it\nresult = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather like in San Francisco?.\"}]})\n\n# Get the last message\nlast_message = result[\"messages\"][-1]\nprint(last_message.content)\n</code></pre>"},{"location":"Docker/basics/","title":"Common Commands","text":"<pre><code># When you find an image you want to use do\ndocker pull &lt;image_name&gt;\n\n# To see images downloaded use\ndocker images\n\n# see running containers\ndocker ps\n\n# see all containers\ndocker ps -a\n\n# run a new container.\ndocker run image-name -d\n# -d means run in detached mode so when the terminal is closed, the container continues to run\n\n# run existing container \ndocker start container-name\n\n# stop docker container\ndocker stop image-name\n\n# remove an image by name and tag\ndocker rmi postgres:latest\n\n# remove a container by ID or name\ndocker rm 123456789abc\ndocker rm container_01\n\n# Docker containers are their own sort of OS. You can SSH into it.\ndocker exec -it &lt;container-name-or-id&gt; /bin/sh\n# /bin/sh is the terminal experience (shell in this case) you could do /bin/bash\n\n# access container logs\ndocker logs &lt;container name or id&gt;\n\n# continuously showing log output\ndocker logs -f &lt;container-name-or-id&gt;\n</code></pre>"},{"location":"Docker/basics/#run-a-container-examples","title":"Run a Container Examples","text":"<pre><code># simplest docker image run\ndocker run hello-world\n\n# run alpine linux (tiny image). sh lets you explore inside the container\ndocker run -it alpine sh\n\n# Some containers need other info to run, which can be passed after the command\n# --name is what the container will be named\n# -p is for port mapping host computer port to image port (first is host, second is container)\n# -e is for adding environment variables\ndocker run --name postgres_db -e POSTGRES_PASSWORD=postgres -p 5432:5432 -d postgres\n</code></pre>"},{"location":"Docker/basics/#connecting-resources-to-host","title":"Connecting Resources to Host","text":""},{"location":"Docker/basics/#docker-ports","title":"Docker Ports","text":"<p>Accessing endpoints from the host machine is typically done through ports. You can map the  host machine port to the port that is being used in the docker container.</p> <p><code>docker run -p 5432:5432 -d &lt;image_name&gt;</code></p>"},{"location":"Docker/basics/#docker-volumes","title":"Docker Volumes","text":"<p>Files and Docker If you need to use files for anything involving docker, you need to configure volumes.</p> <p>Volumes are a mapping of the docker container file system to the host file system.</p> <p><code>-v /home/svr/immich:/usr/src/app/upload</code></p> <p>Just like port mapping, the first is host and second is container.</p>"},{"location":"Docker/basics/#list-of-flags","title":"List of Flags","text":"<p>These are the ones you\u2019ll use 90% of the time:</p> <ul> <li><code>-d</code> \u2192 Run in detached mode (in the background).</li> <li><code>-it</code> \u2192 Interactive + TTY (keeps a terminal open).</li> <li><code>--name &lt;name&gt;</code> \u2192 Give your container a name.</li> <li><code>-p &lt;host:container&gt;</code> \u2192 Publish ports (e.g., <code>-p 8080:80</code>).</li> <li><code>-e &lt;KEY=VALUE&gt;</code> \u2192 Set environment variables.</li> <li><code>-v &lt;host:container&gt;</code> \u2192 Mount a volume (bind mount or Docker volume).</li> <li><code>--rm</code> \u2192 Automatically remove the container when it exits.</li> <li><code>--network &lt;network&gt;</code> \u2192 Connect to a specific network.</li> <li><code>--restart &lt;policy&gt;</code> \u2192 Restart policy (<code>no</code>, <code>always</code>, <code>on-failure</code>, <code>unless-stopped</code>).</li> <li><code>--cpus</code>, <code>--memory</code> \u2192 Limit CPU/memory usage.</li> <li><code>--entrypoint &lt;command&gt;</code> \u2192 Override the default entrypoint.</li> <li><code>--workdir &lt;dir&gt;</code> \u2192 Set working directory inside the container.</li> <li><code>--user &lt;uid:gid&gt;</code> \u2192 Run as a different user.</li> </ul>"},{"location":"Docker/docker_compose/","title":"Docker Compose","text":"<p>Docker compose is a handy way to run docker with configuration files. This is nice when you have more complex containers to run, and its a difficult to type all the config every time you want to run it.</p> <p>Important step: Make sure you have the latest version (currently V2).</p> <p>Docker compose V1 was built on python, V2 is built in Go. If you run <code>which docker compose</code> and it shows the path <code>/usr/bin/docker-compose</code> then you still have V1. Remove it with <code>sudo rm /usr/bin/docker-compose</code>.</p> <p>Create a <code>docker-compose.yaml</code> file. Here's an example:</p> <pre><code>services: # Defines the containers (services)\n  app:\n    image: my_flask_app:latest  # Use an existing image\n    build: ..  # Or build from Dockerfile in the current directory\n    ports:\n      - \"5000:5000\"  # Map host port 5000 to container port 5000\n    volumes:\n      - ./app:/app  # Mount local folder to container folder\n    environment:\n      - FLASK_ENV=development  # Set environment variables\n    depends_on:\n      - db  # Wait for \"db\" service before starting\n\n  db:\n    image: postgres:15\n    restart: always  # Restart if it crashes\n    environment:\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n      POSTGRES_DB: mydatabase\n    volumes:\n      - pg_data:/var/lib/postgresql/data  # Persistent storage for DB\n\nvolumes:\n  pg_data:  # Named volume for PostgreSQL data\n</code></pre> <pre><code># \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# \ud83d\udc33 DOCKER COMPOSE COMMAND REFERENCE\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n# \u25b6\ufe0f Run container (detached mode)\ndocker compose up -d\n\n# \u25b6\ufe0f Stop and remove container(s)\ndocker compose down\n\n# \u25b6\ufe0f Upgrade container(s)\ndocker compose down\ndocker compose pull\ndocker compose up -d\n\n# \u25b6\ufe0f Restart container(s)\ndocker compose restart\n\n# \u25b6\ufe0f Chain commands\ndocker compose down &amp;&amp; docker compose up -d\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# \u2699\ufe0f OTHER USEFUL COMMANDS\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n# View logs (follow mode)\ndocker compose logs -f\n\n# View running containers\ndocker compose ps\n\n# Rebuild image(s) from Dockerfile\ndocker compose build\n\n# Rebuild and restart\ndocker compose build --no-cache &amp;&amp; docker compose up -d\n\n# Stop containers but keep volumes/networks\ndocker compose stop\n\n# Start containers again after stopping\ndocker compose start\n\n# Remove unused images and volumes\ndocker system prune -a --volumes\n</code></pre>"},{"location":"Docker/docker_images/","title":"Build a Docker Image","text":"<p>A Dockerfile is how to build an image. The contents might look something like this:</p>"},{"location":"Docker/docker_images/#build-dockerfile","title":"Build Dockerfile","text":"<p>Simple python app: <pre><code>FROM python:3.12-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY . .\nCMD [\"python\", \"app.py\"]\n</code></pre></p> <p>Flask app (more complex): <pre><code># Use a lightweight Python image\nFROM python:3.12-slim\n\n# Set the working directory inside the container\nWORKDIR /app\n\n# Copy app files to the container\nCOPY . /app\n\n# Install dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Set environment variables\nENV FLASK_ENV=production\nENV PYTHONUNBUFFERED=1\n\n# Expose the port (optional if using Docker Compose)\nEXPOSE 8090\n\n# Run Gunicorn when the container starts\nCMD [\"gunicorn\", \"-w\", \"4\", \"-b\", \"0.0.0.0:8090\", \"web_app.app:app\"]\n</code></pre></p>"},{"location":"Docker/docker_images/#build-image","title":"Build Image","text":"<pre><code># build the image\ndocker build -t my-image-name:tag .  # &lt;- don't forget that period\n\n# example 1\ndocker build -t my-flask-app:latest .\n\n# example 2\ndocker build -t my-flask-app:v1.0 .\n\n# building on a Mac but will run on linux, specify to run on amd64.\ndocker build --platform linux/amd64 -t yt_download:latest .\n\n# once the image has been created:\n# save image to a tar file for sharing/copying\ndocker save -o /Users/tyleranderson/Downloads/yt_downloads_250226.tar yt_download:latest\n\n# load the file as an image\ndocker load -i /srv/flask_yt_download/yt_downloads_250226.tar`\n</code></pre>"},{"location":"Docker/docker_images/#docker-compose-build-on-run","title":"Docker Compose Build on Run","text":"<p>When you use docker compose with an image, you have two options:</p> <ol> <li>Build the image on run.</li> <li>Run from an existing image.</li> </ol> <p>Build image as part of docker compose start up. <pre><code>services:\n  flask_app:\n    build: .  # This tells Compose to use the Dockerfile in the current directory\n    container_name: flask_gunicorn\n    ports:\n      - \"8090:8090\"\n</code></pre></p> <p>Run when image already exists. <pre><code>services:\n  flask_app:\n    image: my_flask_app  # Use the existing built image\n    container_name: flask_gunicorn\n    ports:\n      - \"8090:8090\"\n</code></pre></p>"},{"location":"Docker/docker_images/#automating-build-and-deploy","title":"Automating Build and Deploy","text":"<p>There are many steps when it comes to building and deploying. They can all be automated in one script. Here's an example:</p> <p>Dockerfile <pre><code># Use a lightweight Python image\nFROM python:3.12-slim\n\n# Set the working directory inside the container\nWORKDIR /app\n\n# Install system dependencies, including ffmpeg\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\\n    ffmpeg \\\n    &amp;&amp; apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN apt update &amp;&amp; apt install -y curl\n\n# Copy app files to the container\nCOPY . /app\n\n# Install dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Set environment variables\nENV FLASK_ENV=production\nENV PYTHONUNBUFFERED=1\n\n# Expose the port (optional if using Docker Compose)\nEXPOSE 8090\n\n# Run Gunicorn when the container starts\nCMD [\"gunicorn\", \"-w\", \"4\", \"-b\", \"0.0.0.0:8090\", \"app:app\"]\n</code></pre></p> <p>Deployment script <pre><code>#!/usr/bin/env bash\n\necho \"Remove existing image to make a new one.\"\ndocker rmi yt_download:latest\n\nrm data.db || echo \"No db to remove, moving on\"\n\necho \"Build image for Linux server.\"\ndocker build --platform linux/amd64 -t yt_download:latest .  # uses Dockerfile to build\ndocker save -o yt_download_image.tar yt_download:latest\n\necho \"Transfer file to server.\"\nrsync -v --rsync-path=\"sudo rsync\" yt_download_image.tar tyler@anderson.home:/srv/flask_yt_download\n\necho \"Remove local copy of Docker image and docker image file.\"\ndocker rmi yt_download:latest\nrm yt_download_image.tar\n\necho \"Log into server and spin up image.\"\nssh tyler@anderson.home &lt;&lt; 'EOF'\ncd /srv/flask_yt_download || { echo \"Directory not found\"; exit 1; }\n\n# create database file\nsudo touch data.db\n\n# Spin down container if running\nsudo docker compose down || echo \"No container running\"\n\n# Remove old Docker image\nsudo docker rmi yt_download || echo \"Image not found\"\n\n# Load new image\nsudo docker load -i yt_download_image.tar\n\n# Remove the transferred image file\nsudo rm yt_download_image.tar\n\n# Restart the container\nsudo docker compose up -d\nEOF\n</code></pre></p>"},{"location":"Docker/docker_images/#add-uv-to-image","title":"Add uv to image","text":"<pre><code># build a wheel file inside a docker image\nFROM python:3.13\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv\n</code></pre>"},{"location":"Docker/docker_images/#multi-stage-build","title":"Multi-Stage build","text":"<p>You can build an image in stages.  It creates separate images during the build.  The last one that runs becomes the actual image.</p> <pre><code># Build stage\nFROM python:3.13 AS builder\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv\nCOPY . /src\nWORKDIR /src\nRUN uv build\n\n# Runtime stage\nFROM python:3.13-slim AS runtime\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv\nCOPY --from=builder /src/dist/*.whl &amp;&amp; rm /tmp/*.whl\n\nRUN uv pip install --system /tmp/*.whl &amp;&amp; rm /tmp/*.whl\n\nCMD [\"invoice_start\", \"--sharepoint\"]\n</code></pre>"},{"location":"Docker/intro/","title":"Intro to Docker","text":""},{"location":"Docker/intro/#general","title":"General","text":"<p>Docker creates and runs containers - software bundled to run on a minimal isolated linux OS on the host system. Containers are great because they are typically very easy to install and even easier to uninstall. Allows for reproducing environments, avoiding the \"it runs on my machine\" problem. They can enhance security by isolating the host system from the system running the software.</p>"},{"location":"Docker/intro/#why-should-i-care-about-docker","title":"Why should I care about Docker?","text":"<ul> <li>It is the modern way of running production programs.</li> <li>Docker is what runs cloud services. For example, if we spin up a resource in Azure, it's probably running on Docker.</li> <li>Fixes the issue of \"it runs on my machine\" for development (reproducible environment).</li> <li>Containers are isolated from the host machine, which makes it more secure.</li> </ul> <p>Docker is great for both development and production.</p>"},{"location":"Docker/intro/#what-is-docker","title":"What is Docker?","text":"<p>Docker is a utility that hosts other programs in their own containers.</p> <p>Think of a container like an ultra-light weight version of a virtual machine.</p> <p>The difference between a VM and a container: - VM's require allocating a chunk of a computer's hardware resources to its own kernel and OS. - A container can share the kernel and user space.</p> <p>Terms:</p> <ul> <li>Image: A copy of the software that is ready to run on docker.</li> <li>Container: An instance of an image, specific to the image that has been setup to run. More simply,   a container is an image running on your machine. Can be turned on or off, etc.</li> </ul>"},{"location":"Git/basics/","title":"Git Basics","text":"<p>Git is a code repository and version control system. It's great for tracking changes to a code base (or any other text based files).</p> <p>There are many tools that can help with basic git functions. The power really comes from using it in the terminal.</p>"},{"location":"Git/basics/#common-commands","title":"Common Commands","text":"<pre><code># start a git project in working directory.\ngit init\n\n# add a file to git.\ngit add main.py\n\n# add all files in the directory to git.\ngit add .\n\n# commit your changes.\ngit commit -m \"Write a message here\"\n\n# send your commited git changes to your active remote.\ngit push\n\n# retrieve commited changes from remote that are not in your local repo.\ngit pull\n\n# remove all uncommited changes (USE WITH CAUTION, NO RECOVERY OPTIONS)\ngit reset --hard\n</code></pre> <p>The location of the git files is a directory in the root directory of your project. Its usually <code>.git</code>. This is a directory not a file. Remotes will have similar naming like python_quickstart.git. </p>"},{"location":"Git/basics/#head","title":"HEAD","text":"<p>HEAD in git is a pointer to which branch and commit you are on.  Typically, it is on the most recent commit of the selected branch.</p>"},{"location":"Git/basics/#navigate-old-commits","title":"Navigate Old Commits","text":"<pre><code>git checkout &lt;commit hash&gt;\ngit checkout HEAD~1     # 1 commit before current HEAD\ngit checkout HEAD~3     # 3 commits before current HEAD\n</code></pre> <p>This puts you in a detached HEAD state which means any changes you make wont be tracked on any branch, they will just float. </p> <p>To safely work on an old commit: <pre><code>git checkout abc1234        # Go to the commit\ngit checkout -b fix/legacy-bug   # Create a branch from it\n</code></pre></p> <p>What if you want to rollback a commit?</p> <p>If you haven't pushed to remote: <pre><code># undo commit but keep code changes.\ngit reset --soft HEAD~1\n\n# completely remove the commit and rollback files to what the last commit was.\ngit reset --hard HEAD~1 # USE WITH CAUTION\n</code></pre></p> <p>To get back to head just checkout the branch: <code>git checkout main</code></p>"},{"location":"Git/branches/","title":"Branches","text":"<p>A branch in git is a way to create different versions of your code and keep them separated.</p>"},{"location":"Git/branches/#checkout","title":"Checkout","text":"<p>Checkout is for switching branches: <code>git checkout main</code></p> <p>Checkout is also how you can create new branches: <pre><code>git checkout -b new_branch\n# this is shorthand for\ngit branch new_branch\ngit checkout new_branch\n</code></pre></p> <p>You can use checkout to do a lot more: <pre><code># pull a file from another branch\ngit checkout main -- README.md\n</code></pre></p> <p>Below are some examples of some reasons to create a branch.</p> <pre><code># create a new branch for a feature\ngit checkout -b feature/login-form\n\n# create a branch for a bug fix\ngit checkout -b fix/issue-123-crash\n\n# create a branch for experiments\ngit checkout -b experiment/redesign\n\n# separate production from development\ngit checkout -b production\ngit checkout -b development\n</code></pre>"},{"location":"Git/branches/#example-workflow","title":"Example Workflow","text":"<pre><code># make new branch\ngit checkout -b test_branch\n\n# make change to file\n\n# commit the change.\ngit commit -m \"update message\"\n\n# (optional) push to remote\ngit push\n# or\ngit push origin test_branch\n\n# switch back to main so we can merge the new branch\ngit checkout main\n\n# make sure main is up-to-date\ngit pull\n\n# merge the branch into main\ngit merge test_branch\n\n# you can also delete the branch afterward if you do not expect to use it\ngit branch -d test_branch\n\n# if it wasn't merged and you still want to delete\ngit branch -D test_branch\n\n# (optional) clean up the remote\ngit push origin --delete test_branch\n</code></pre> <p>Note: when you push to remote, you are only pushing the active branch. If you want to push all from local to remote: <code>git push --all origin</code> BE CAREFUL WITH THIS.</p>"},{"location":"Git/remotes/","title":"Git Remotes","text":"<p>A \"remote\" in the git environment is another location to  store a copy of your git project. It is great for backing up projects and sharing code/collaboration.</p> <p>GitHub is the most common remote but there are many other options.</p>"},{"location":"Git/remotes/#using-remotes","title":"Using Remotes","text":"<pre><code># To see all remotes that are in a project\ngit remote -v\n\n# to push commited updates to remote\ngit push\n</code></pre> <p>You can setup multiple remotes on one git project.</p> <pre><code>git remote add &lt;name of remote&gt; &lt;server uri&gt;\n\n# for example:\ngit remote add origin https://github.com/ty-anderson/python_quickstart.git\ngit remote add backup ssh://user@yourserver:/srv/git/myproject.git\n\n# when pushing be explicit to which remote\ngit push &lt;remote name&gt; &lt;branch&gt;\ngit push origin main\ngit push backup master\n</code></pre>"},{"location":"Git/remotes/#self-hosted-git-server","title":"Self Hosted Git Server","text":"<p>If you want to store your git projects on your own server, its pretty simple.</p> <p>Steps:</p> <p>On your server: <pre><code>mkdir -p /srv/git/myproject.git\ncd /srv/git/myproject.git\ngit init --bare\n</code></pre></p> <p>On your local machine: <pre><code>git remote add origin ssh://user@yourserver:/srv/git/myproject.git\ngit push -u origin main\n# origin is name of remote, change to whatever you want. \n# main is name of branch, change to whatever branch you want to commit.\n# the -u sets this remote as the active one for tracking. \n</code></pre></p>"},{"location":"Linux/docs_103_ssh/","title":"SSH","text":"<p>To SSH into a server its <code>ssh username@ipaddress</code> or <code>ssh username@servername</code>.</p> <p>SSH uses port 22.</p> <p>Typically when you SSH into a system, it will ask for your password. Instead of using a password everytime, you can create a ssh key. These keys are essentially  a file that sits on both machines and grants access without needing a password.</p> <p>To generate ssh keys:</p> <ol> <li>On local machine <code>ssh-keygen -t rsa -b 4096</code>. It will ask for a passphrase, but you can leave it empty.</li> <li>Copy the ssh key to the server <code>ssh-copy-id user@server</code></li> </ol> <p>If it worked correctly, you should now be able to login without needing a password.</p> <p>You can run commands through ssh, without logging in: <code>ssh user@server \"sudo chown -R user:user /srv/web_apps\"</code></p>"},{"location":"Linux/docs_104_linux/","title":"Linux","text":""},{"location":"Linux/docs_104_linux/#execute-files","title":"Execute files","text":"<pre><code>./path/to/file\n# or\nsource /path/to/file\n</code></pre> <ul> <li><code>./bash_script.sh</code> executes the script as a standalone process. This method requires execute permissions.</li> <li><code>source bash_script.sh</code> executes the script in the existing process (doesn't create standalone process). Allows modifying environment variables in the current shell. Allows <code>cd</code> command to change directory.</li> </ul> <p>TLDR: default to using <code>source</code> to execute files.</p>"},{"location":"Linux/docs_104_linux/#file-permissions","title":"File Permissions","text":"<pre><code># view permissions\nls -l  # &lt;--view permissions of files in the directory.\nls -ld  # &lt;--view permissions of the directory itself.\n\n# change permissions - full read write all access\nsudo chmod -R 777 /path/to/folder\n</code></pre> <p>View permissions:</p> <p>You will see something like below:</p> <p><code>drwxr-xr-x  2 root root 4096 Feb 12 12:34 /srv/web_apps</code></p> <p>The first section is read, write, execute for owner, group, others. </p> Section Meaning Who it applies to d Directory - rwx read(r), write(w), execute(x) Owner (root) r-x read(r), no write(-), execute(x) Group (root) r-x read(r), no write(-), execute(x) Others (everyone else) <p>To see if you're root do: <code>whoami</code>. If the output is 'root' then you can write. If your user is not root, check if your in the root group <code>groups</code>. If your not in 'root' group, you cannot write.</p> <pre><code># \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# \ud83d\udd11 PERMISSIONS &amp; OWNERSHIP REFERENCE\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n# \u2500\u2500 VIEW PERMISSIONS\nls -l          # list files with permissions\nls -ld folder  # view folder permissions only\nstat filename  # detailed view (shows numeric + symbolic modes)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# \ud83e\udded ALTER PERMISSIONS (CHMOD)\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# chmod = change mode (permissions)\n# Two methods: symbolic and numeric\n\n# \u2500\u2500 SYMBOLIC MODE\n# u = user (owner)\n# g = group\n# o = others\n# a = all (u+g+o)\n# + = add permission\n# - = remove permission\n# = = set exact permission\n\n# \u25b6\ufe0f Common examples\nchmod u+rwx,g+r,o-r filename     # give user full, group read, remove read for others\nchmod u+rwx filename             # owner full permissions\nchmod g-w filename               # remove write for group\nchmod o-r filename               # remove read for others\nchmod +x filename                # give everyone execute permission\nchmod a+r foldername             # allow all users to read a folder\nchmod a-x script.sh              # remove execute for everyone\n\n\n# \u2500\u2500 NUMERIC (OCTAL) MODE\n# Each permission type (r=4, w=2, x=1)\n# Add them up for each role (user, group, others)\n\n# Example table:\n# r-- = 4, rw- = 6, rwx = 7, --- = 0\n# Format: chmod [user][group][others] file\n\nchmod 777 file    # rwx for all\nchmod 755 file    # rwx for owner, r-x for group/others\nchmod 700 file    # rwx for owner only\nchmod 644 file    # rw for owner, r for group/others\nchmod 600 file    # rw for owner, no access for others\n\n# \u25b6\ufe0f Directories often use 755\nchmod 755 /srv/web_apps\n# Files typically use 644\nchmod 644 index.html\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# \ud83d\udc51 CHANGE OWNERSHIP (CHOWN)\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# chown = change owner (and optionally group)\n\n# \u25b6\ufe0f Change owner of a file\nsudo chown user file.txt\n\n# \u25b6\ufe0f Change owner and group\nsudo chown user:group file.txt\n\n# \u25b6\ufe0f Change owner recursively for directory and contents\nsudo chown -R user:group /srv/web_apps\n\n# \u25b6\ufe0f View ownership\nls -l filename\n# Output: owner and group are shown in the middle columns\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# \ud83e\udde9 OTHER USEFUL COMMANDS\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Change only the group of a file\nsudo chgrp groupname file.txt\n\n# Add user to group (to give access)\nsudo usermod -aG groupname username\n\n# Check which groups a user belongs to\ngroups username\n\n# Apply same permissions to all files in a directory\nchmod -R 644 /srv/web_apps\n# Apply directory execute bit so users can enter directories\nfind /srv/web_apps -type d -exec chmod 755 {} \\;\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# \u2705 TIPS\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# \u2022 Always test permissions with 'ls -l' after changes.\n# \u2022 Directories need execute (x) permission to be accessible.\n# \u2022 Use numeric mode for scripts or automation, symbolic for clarity.\n# \u2022 For public web directories, 755 for folders and 644 for files is standard.\n</code></pre>"},{"location":"Linux/docs_104_linux/#copy-directory","title":"Copy Directory","text":"<p>Two baked-in commands are <code>scp</code> or <code>rsync</code>.</p> <p>Here's an example with scp: <code>scp -r ./site user@server:/srv/web_apps</code>.</p> <p>rsync is typically recommended over scp.</p> <p>Rsync usually comes on Linux and MacOS. </p> <pre><code># \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# \ud83e\udded RSYNC COMMAND REFERENCE\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n# \u25b6\ufe0f Basic usage\nrsync -v /source/file/name.txt /dest/file\n\n# \u25b6\ufe0f Use literal string (preserve special characters)\nrsync -v '/source/file/na$me.txt' /dest/file\n\n# \u25b6\ufe0f Send multiple files\nrsync -v /source/file1.txt /source/file2.txt '/dest'\n\n# \u25b6\ufe0f Send over SSH\nrsync -av /source/file/name.txt user@server:/dest/file\n\n# \u25b6\ufe0f Send over SSH with custom port\nrsync -avz -e \"ssh -p 2222\" /source/file/name.txt user@server:/dest/file\n\n# \u25b6\ufe0f Send over SSH with sudo on remote\nrsync -v --rsync-path=\"sudo rsync\" yt_download_image.tar user@server:/srv/flask_yt_download\n\n# \u25b6\ufe0f Send entire directory (keeps parent folder)\nrsync -avz /source/dir /dest/dir\n\n# \u25b6\ufe0f Send all files *inside* a directory (no parent folder)\nrsync -avz /source/dir/ /dest/dir\n\n# \u25b6\ufe0f Full example with sudo and custom SSH port\nrsync -avz --rsync-path=\"sudo rsync\" -e \"ssh -p 2222\" /Users/tyleranderson/Bonus tyler@anderson.home:/home/tyler/backup\n\n# \u25b6\ufe0f Full example with sudo (default port)\nrsync -avz --rsync-path=\"sudo rsync\" /Users/tyleranderson/Bonus tyler@anderson.home:/home/tyler/backup\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# \u2699\ufe0f FLAGS\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# -v            verbose mode\n# -a            archive mode (preserves permissions, timestamps, symlinks)\n# -z            compress files during transfer\n# -e            specify remote shell (e.g., \"ssh -p 2222\")\n# --progress    show transfer progress\n# --delete      delete files in destination that are not in source\n# --exclude     exclude specific files or folders (e.g., --exclude='*.log' --exclude='/cache/')\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# \ud83d\udcc1 DIRECTORY COPY EXAMPLES\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Copy directory into another (keeps 'site' folder)\nrsync -av ./site user@server:/srv/web_apps/notes\n\n# Copy directory contents into another (no 'site' folder)\nrsync -av ./site/ user@server:/srv/web_apps/notes/\n# Note: trailing \u201c/\u201d on source copies contents only; trailing \u201c/\u201d on destination does not matter.\n</code></pre>"},{"location":"Linux/docs_104_linux/#tar-files","title":"Tar Files","text":"<p>A tar file (.tar) is an archive that bundles multiple files or directories into one file (optionally compressed). Commonly used for backups or transfers on Linux and Unix systems.</p> <pre><code># \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# \ud83d\udce6 TAR FILE COMMAND REFERENCE\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n# \u25b6\ufe0f Create a tar file (no compression)\ntar -cvf archive.tar /path/to/files/\n\n# \u25b6\ufe0f Extract a tar file\ntar -xvf archive.tar\n\n# \u25b6\ufe0f Create a compressed tar file (.tar.gz)\ntar -czvf archive.tar.gz /path/to/files/\n\n# \u25b6\ufe0f Extract a compressed tar file\ntar -xzvf archive.tar.gz\n\n# \u25b6\ufe0f List files inside a tar file (without extracting)\ntar -tvf archive.tar\n\n# \u25b6\ufe0f Extract a specific file from a tar archive\ntar -xvf archive.tar file.txt\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# \u2699\ufe0f FLAGS\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# -c   create a new archive\n# -v   verbose mode (show progress)\n# -f   specify the archive filename\n# -x   extract from an archive\n# -z   compress/decompress using gzip\n</code></pre>"},{"location":"Linux/docs_104_linux/#encryption","title":"Encryption","text":"<pre><code># \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# \ud83d\udd10 ENCRYPTION &amp; SECURE DELETION REFERENCE\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n# \u2500\u2500 ENCRYPTION (AGE)\n# age is a modern, lightweight encryption tool for files.\n# It\u2019s simpler and more secure than GPG, but with fewer features.\n\n# \u25b6\ufe0f Encrypt a single file\nage -o secretfile.age -p secretfile.txt\n\n# \u25b6\ufe0f Encrypt a directory (use tar first)\ntar -czf myfolder.tar.gz myfolder/\nage -o myfolder.tar.gz.age -p myfolder.tar.gz\n\n# \u25b6\ufe0f Install age\n# macOS:\nbrew install age\n\n# Ubuntu / Debian:\nsudo apt install age\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# \ud83e\uddf9 REMOVING SENSITIVE DATA\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n# \u2500\u2500 HARD DISK DRIVES (HDD)\n# Use 'shred' to securely delete files by overwriting with random data multiple times.\n\n# \u25b6\ufe0f Securely delete a file\nshred -u secret.txt\n\n# \u25b6\ufe0f Overwrite file multiple times before deleting\nshred -n 10 secret.txt\n\n# \u2699\ufe0f FLAGS\n# -u   truncate and delete file after shredding\n# -n   number of overwrite passes (default is 3)\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# \u26a1 SOLID STATE DRIVES (SSD)\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n# \u25b6\ufe0f 1. Use fstrim (best for clearing free space on SSD)\n# Tells SSD to permanently erase deleted data\nsudo fstrim -v /\n\n# \u25b6\ufe0f 2. Use srm or wipe (best for secure file deletion)\n# Better than shred for SSDs (handles wear leveling properly)\n\n# \ud83d\udd39 Install secure delete tools\nsudo apt install secure-delete    # Debian/Ubuntu\nbrew install srm                  # macOS\n\n# \ud83d\udd39 Securely delete a file\nsrm -v my_secret_file.txt\n\n# \ud83d\udd39 Securely delete a directory\nwipe -rf my_secret_folder/\n</code></pre>"},{"location":"Linux/docs_199_backups/","title":"Backups","text":"<p>Options: Backup files/folders OR full system backup.</p> <p>Backup folders: daily <code>rsync</code> to a separate disk Full system: weekly <code>dd</code> or <code>timeshift</code></p> <p>Can setup with cron to run regularly.</p>"},{"location":"Linux/docs_199_backups/#philosophy","title":"Philosophy","text":"<p>Best practice for backing up files is called 3-2-1 backup strategy.</p> <ul> <li>3 copies of your data (original and two backups).</li> <li>2 different storage types (external drive, NAS, cloud storage, etc).</li> <li>1 off-site backup, a cloud backup or physical backup at a friends house.</li> </ul>"},{"location":"Linux/docs_199_backups/#copy-directory","title":"Copy Directory","text":"<p>Two baked-in commands are <code>scp</code> or <code>rsync</code>.</p> <p>Here's an example with scp: <code>scp -r ./site user@server:/srv/web_apps</code>.</p> <p>rsync is typically recommended over scp.</p> <p><code>rsync</code> usually comes on Linux and MacOS, it is lightweight, fast, can detect changes in files to do an incremental backup. </p> <p>Commands: </p> <pre><code># Basic save file to another folder\nrsync -v /source/file/name.txt /dest/file\n\n# Use literal string (preserve string for special characters)\nrsync -v '/source/file/na$me.txt' /dest/file\n\n# Send multiple files\nrsync -v /source/file1.txt /source/file2.txt '/dest'\n\n# Send over SSH\nrsync -av /source/file/name.txt user@server:/dest/file\n\n# Send over SSH with custom port\nrsync -avz -e \"ssh -p 2222\" /source/file/name.txt user@server:/dest/file\n\n# Send over SSH with sudo command\nrsync -v --rsync-path=\"sudo rsync\" yt_download_image.tar user@server:/srv/flask_yt_download\n\n# Send over entire directory\nrsync -avz /source/dir /dest/dir\n\n# Send all files in directory. Trailing slash. \n# Trailing slash only matters on source, it doesn't matter on dest\nrsync -avz /source/dir/ /dest/dir\n\n# actual use\nrsync -avz --rsync-path=\"sudo rsync\" /Users/tyleranderson/PycharmProjects tyler@anderson.home:/srv/backup_media\n\n# \u2500\u2500 RSYNC FLAGS\n# -v  verbose mode\n# -a  archive mode (preserves permissions/timestamps)\n# -z  compress files\n# -e  specify remote shell command\n# -r  recursive (copy subdirectories)\n# -h  human-readable file sizes\n# --progress  show progress\n# --delete    remove files in dest not in source\n# --exclude   exclude files (e.g., --exclude='*.log' --exclude='/cache/')\n</code></pre>"},{"location":"Linux/docs_199_backups/#tar-files","title":"Tar Files","text":"<p>A tar file <code>.tar</code> is an archive file that stores multiple files and  directories together in one file, without compression. Very common in Linux and Unix based systems. Common for backups and transfers.</p> <pre><code># Create tar file (best to use relative path instead of absolute)\nsudo tar -cvf archive.tar path/to/files/\n\n# Extract tar file\nsudo tar -xvf archive.tar\n\n# Create compressed tar file\ntar -czvf archive.tar.gz /path/to/files/\n\n# Extract compressed tar file\ntar -xzvf archive.tar.gz\n\n# List files in tar file\ntar -tvf archive.tar\n\n# Extract specific file from tar file\ntar -xvf archive.tar file.txt\n\n# \u2500\u2500 TAR FLAGS\n# -c  create new archive\n# -v  verbose\n# -f  specify archive filename\n# -x  extract\n# -z  gzip compression\n</code></pre>"},{"location":"Linux/docs_199_backups/#encryption","title":"Encryption","text":"<p><code>age</code> is a lightweight focused encryption software for files.  It uses modern cryptography making it more secure, but less featured  than something like <code>gpg</code>. Age  lets you use passwords or key files.</p> <pre><code># \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# \ud83d\udd10 ENCRYPTION (AGE)\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n# Install age\nbrew install age           # macOS\nsudo apt install age       # Linux\n\n# Option 1: Password Encryption (simple)\nage -o myfile.txt.age -p myfile.txt\nage -d myfile.txt.age &gt; myfile.txt\n\n# Option 2: Public/Private Key Encryption (secure)\nage-keygen -o ~/.age-key.txt     # create key file\nage -r PUBLIC_KEY -o myfile.txt.age myfile.txt\nage -d -i ~/.age-key.txt myfile.txt.age &gt; myfile.txt\n\n# Encrypt using key file directly\nage -r $(head -n 1 ~/.age-key.txt) -o myfile.txt.age myfile.txt\nage -d -i ~/.age-key.txt myfile.txt.age &gt; myfile.txt\n\n# Encrypt directory (tar + age)\ntar -czf myfolder.tar.gz myfolder/\nage -o myfolder.tar.gz.age -p myfolder.tar.gz\n\n# One-liner: tar and encrypt simultaneously\ntar -czf - myfolder/ | age -o myfolder.tar.gz.age -p\n\n# Decrypt and extract in one step\nage -d myfolder.tar.gz.age | tar -xz\n\n# Manual decrypt\nage -d myfolder.tar.gz.age &gt; myfolder.tar.gz\ntar -xzf myfolder.tar.gz\nshred -u myfolder.tar.gz\n\n# \u2500\u2500 AGE FLAGS\n# -o &lt;output&gt;     specify output file\n# -d              decrypt\n# -p              use password mode\n# -r &lt;pubkey&gt;     encrypt with public key\n# -i &lt;identity&gt;   specify private key for decryption\n</code></pre> <p>Other notes:</p> <ul> <li>The key file contains both public and private keys.</li> <li>Encryption uses only the public key. </li> <li>Decryption uses both keys</li> </ul>"},{"location":"Linux/docs_199_backups/#removing-sensitive-data","title":"Removing Sensitive Data","text":"<pre><code># \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# \ud83e\uddf9 REMOVING SENSITIVE DATA\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n# \u2500\u2500 HDD: Use shred\nshred -u secret.txt\nshred -n 10 secret.txt\n\n# -u  delete file after shredding\n# -n  overwrite N times (default 3)\n\n# \u2500\u2500 Directories: Use wipe\nsudo apt install wipe\nwipe -r /path/to/directory/\nwipe -r /path/to/directory/*  # keep directory but delete contents\n\n# \u2500\u2500 SSD: Use fstrim or srm\nsudo fstrim -v /              # clears free space on SSD\n\n# Secure delete tools\nsudo apt install secure-delete  # Ubuntu/Debian\nbrew install srm                # macOS\n\nsrm -v my_secret_file.txt       # securely delete a file\nwipe -rf my_secret_folder/      # securely delete a folder\n\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# \ud83e\udde0 IN PRACTICE\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Recommended workflow:\n\n# - rsync \u2192 for incremental local backups\n# - rclone \u2192 for cloud backups\n# - syncthing \u2192 for live sync between devices (peer-to-peer)\n\n# \ud83d\udd01 Good routine:\n# 1. Daily rsync to local drive\n# 2. Daily/weekly rclone to cloud\n\n# \u2500\u2500 Example cron jobs\n0 2 * * * rsync -rv --delete /srv/plex_media /mnt/usb/plex_media_backup\n0 3 * * * rclone sync /srv/ gdrive:server-backup --log-file=/var/log/rclone.log\n\n# Setup rclone\nrclone config\n</code></pre>"},{"location":"Low%20Level/drivers/","title":"Drivers","text":"<p>A driver is a special kind of program that acts as the \"translator\" between the OS and a piece of hardware.</p> <p>The hardware speaks its own low-level language (signals, registers, protocols). The OS needs a way to understand that language in a consistent way.</p> <p>Because it needs to interact with the OS, good drivers are written with a language that compiles to machine code (C, Rust, Zig, etc.).</p>"},{"location":"Low%20Level/drivers/#when","title":"When","text":"<p>The only time a developer would need to create a driver is if there is new hardware. Maybe a brand new hardware your company made or that you invented. This is because it requires deep knowledge of how the hardware  works internally.</p> <p>Creating drivers falls more into systems engineering/hardware engineering.</p> <p>Summary: Make hardware? You'll need to make drivers...</p>"},{"location":"Low%20Level/foundations/","title":"\ud83d\udd39 Foundations","text":"<ul> <li>Binary &amp; Hexadecimal \u2192 how computers represent data.</li> <li>Boolean logic &amp; gates \u2192 the \u201cmath\u201d of 0s and 1s.</li> <li>Memory addressing \u2192 how data lives in RAM and how the CPU finds it.</li> </ul>"},{"location":"Low%20Level/foundations/#cpu-instructions","title":"\ud83d\udd39 CPU &amp; Instructions","text":"<ul> <li>Assembly language \u2192 the CPU\u2019s direct instructions (add, move, jump).</li> <li>Instruction set architectures (ISA) \u2192 x86 vs ARM vs RISC-V.</li> <li>Registers &amp; stack \u2192 tiny but super-fast memory inside the CPU.</li> <li>Interrupts \u2192 how hardware says \u201chey CPU, stop and listen to me.\u201d</li> </ul>"},{"location":"Low%20Level/foundations/#operating-system-concepts","title":"\ud83d\udd39 Operating System Concepts","text":"<ul> <li>System calls \u2192 how user programs ask the OS to do privileged work (like file access).</li> <li>Processes &amp; threads \u2192 how the OS runs multiple programs \u201cat once.\u201d</li> <li>Virtual memory \u2192 how programs see their own memory space, even though RAM is shared.</li> <li>Scheduling \u2192 how the OS decides which process runs next.</li> <li>Device drivers \u2192 how the OS talks to hardware.</li> </ul>"},{"location":"Low%20Level/foundations/#hardware-io","title":"\ud83d\udd39 Hardware &amp; I/O","text":"<ul> <li>Buses &amp; ports \u2192 how components (CPU, RAM, GPU, disk) communicate.</li> <li>Storage devices \u2192 how disks store and retrieve data.</li> <li>Networking \u2192 packets, NICs, how TCP/IP actually leaves your computer.</li> <li>Keyboard/mouse (HID), display, GPU basics \u2192 common hardware roles.</li> </ul>"},{"location":"Low%20Level/foundations/#systems-programming","title":"\ud83d\udd39 Systems Programming","text":"<ul> <li>Memory management \u2192 stack vs heap, allocation, fragmentation.</li> <li>Pointers &amp; references \u2192 how languages like C and Rust directly manipulate memory.</li> <li>Concurrency \u2192 threads, locks, atomics, deadlocks.</li> <li>File systems \u2192 how bytes on disk become folders/files.</li> <li>Compilers &amp; interpreters \u2192 how code becomes machine instructions.</li> </ul>"},{"location":"Low%20Level/foundations/#cool-projects-to-learn-by-doing","title":"\ud83d\udd39 Cool Projects to Learn By Doing","text":"<ul> <li>Write a tiny program in assembly.</li> <li>Implement a toy memory allocator in C or Rust.</li> <li>Build a very simple command-line shell.</li> <li>Follow an OS tutorial (like \u201cwrite your own kernel in Rust\u201d \u2192 Phil Opp\u2019s blog).</li> <li>Write a basic network packet sniffer.</li> </ul>"},{"location":"Networking/docs_100_networking/","title":"Networking","text":"<p>A network has many different things going on.</p> <p>In networking, devices sit behind routers. Routers have a public IP address that is used in accessing the internet, or a wide area network (WAN). The router also  manages local ip address for all devices through DHCP. </p>"},{"location":"Networking/docs_100_networking/#dhcp","title":"DHCP","text":"<p>DHCP - Dynamic Host Configuration Protocol</p> <p>DHCP is a server responsible for assigning local ip addresses to devices on the network.  It can also have local domain names assigned, frequently setup as <code>home.local</code> or <code>local</code>.</p> <p>You can use DHCP to reserve certain ip addresses for certain devices. You can also assign names that point to a certain device. This way you can refer to  the device on the network by an easy to remember name instead of the ip address. For example 192.168.1.104 -&gt; homeserver.</p> <p>When a device joins a network, it sends a \"DHCP discover\" message.  A DHCP server responds to the message and assigns an IP address and other configuration information.  DHCP also assigns new IP addresses when devices move to new locations. </p>"},{"location":"Networking/docs_100_networking/#dns","title":"DNS","text":"<p>DNS - Domain Name System</p> <p>This translates IP addresses into domain names. Think of a phonebook for the internet. Instead of names to phone numbers it points domain names to ip addresses. There are public DNS servers that route the public domains on the internet. </p>"},{"location":"Networking/docs_100_networking/#local-dns","title":"Local DNS","text":"<p>There are also DNS servers that are run on a router or a local machine. This establishes mappings from names to ip address only on your local network. This makes it so you can setup a device with, say ip <code>192.168.1.104</code> to name <code>mydevice.home</code>. Keep in mind that your  router might have a suffix that attaches to these names, such as <code>.home</code></p>"},{"location":"Networking/docs_100_networking/#ipv4-vs-ipv6","title":"IPv4 vs IPv6","text":""},{"location":"Networking/docs_100_networking/#ipv4","title":"IPv4","text":"<p>consists of devices having a 32-bit address that looks like 192.168.1.100. It consists of public and private addresses, where the public addresses are accessible over the internet and private addresses are only accessible over  a local network. This means for data to get passed from one device over the internet to another device, the protocol Network Area Translation (NAT) is used. NAT will convert every packet of your private ip to the public which costs compute.</p>"},{"location":"Networking/docs_100_networking/#ipv6","title":"IPv6","text":"<ul> <li>128-bit addresses, virtually infinite possible addresses.</li> <li>Device addresses are globally routable (not hidden behind a public IP).</li> <li>Because devices are globally routable, it eliminates the need for NAT  as devices can connect end-to-end. Allowing for better connection speeds for video, audio, peer-to-peer, gaming, etc.</li> <li>Devices can auto-configure with Stateless Address Autoconfiguration (SLAAC), no need for DHCP.</li> <li>Firewalls are more relevant with IPv6 due to addresses being globally routable.</li> </ul> <p><code>ip -6 addr show</code></p> <p>Local address: <code>fe80::</code></p> <ul> <li>Scope: Local network segment (cannot be routed on the internet).</li> <li>Usage: Used for internal network functions like router discovery and neighbor discovery.</li> <li>Common for: Internal device communication.</li> </ul> <p>Global address: Usually starts with <code>2xxx::</code> or <code>3xxx::</code></p> <ul> <li>Scope: Globally routable on the internet.</li> <li>Usage: For internet-facing applications or public communication.</li> <li>This is often the correct address for external access.</li> </ul> <p>Address types:</p> <ul> <li>scope global \u2192 for internet or external routing.</li> <li>scope link \u2192 for local network only.</li> </ul> <p>SSH: ssh user@[fe80::1a2b:3c4d:5e6f%eth0]</p>"},{"location":"Networking/docs_100_networking/#commands","title":"Commands","text":"<p><code>netstat</code> - </p>"},{"location":"Networking/docs_100_servers/","title":"Servers","text":""},{"location":"Networking/docs_100_servers/#web-server","title":"Web Server","text":"<p>Known for serving over HTTP/HTTPS protocol. Typically websites, API's, web services, anything using HTTP(S).</p>"},{"location":"Networking/docs_100_servers/#proxy-server","title":"Proxy Server","text":"<p>Redirects web traffic. A sort of \"middleman\" server that is responsible for directing requests.  For example, Nginx or Caddy can be hosted on a server. The proxy server program, once configured,  will push requests to other servers. Good for load balancing, obscuring other servers IP address. Specifically this reroutes web traffic like HTTP(S).</p>"},{"location":"Networking/docs_100_servers/#file-server","title":"File Server","text":"<p>A file server is a server that is like a central hub to hold files. </p> <p>Some common options: - Samba https://hub.docker.com/r/dperson/samba for your typical file explorer-like experience. - SFTP. - Files over HTTPS aka cloud API's like Google Drive.</p>"},{"location":"Networking/docs_101_caddy/","title":"Caddy","text":"<p>Official site: https://caddyserver.com/</p> <p>docker compose: <pre><code>services:\n  caddy:\n    image: caddy:latest\n    container_name: caddy\n    ports:\n      - \"80:80\"        # HTTP\n      - \"443:443\"\n      - \"8009:8009\"\n    volumes:\n      - ./Caddyfile:/etc/caddy/Caddyfile   # Mount your Caddyfile\n      - ./data:/data                       # Let\u2019s Encrypt certificates\n      - ./config:/config                   # Caddy configuration\n      - /srv/web_apps/notes:/srv/web_apps/notes\n    restart: unless-stopped\n</code></pre></p> <p>Don't forget to open the port(s) in the firewall.</p>"},{"location":"Networking/docs_101_caddy/#reverse-proxy","title":"Reverse-Proxy","text":"<p>Reverse Proxy - software that routes traffic from one endpoint to another, or multiple others.</p> <p>Caddy is very simple and it comes with built in certificate management through Lets Encrypt. </p> <pre><code>&lt;requested-domain&gt; {\n    reverse_proxy &lt;ip and port service is routed to&gt;\n}\n</code></pre>"},{"location":"Networking/docs_101_caddy/#examples-using-caddy","title":"Examples using Caddy:","text":"<p>Very simple config. This says using the host machine ip address, using the protocol on port 80 (http) respond with \"Hello from Caddy\".</p> <pre><code>:80 {\n    respond \"Hello from Caddy\"\n}\n</code></pre> <p>Internal and localhost certificates If you configure sites with local or internal addresses, Caddy will serve them over HTTPS  using a locally-trusted certificate authority with short-lived, auto-renewing certificates.  It even offers to install your unique root into your local trust stores for you. <pre><code>localhost {\n    respond \"Hello from HTTPS!\"\n}\n\n192.168.1.10 {\n    respond \"Also HTTPS!\"\n}\n\nhttp://localhost {\n    respond \"Plain HTTP\"\n}\n</code></pre></p> <p>When requests go to example.com, it will get routed to this reverse proxy, which then pushes it to localhost:5000. Just be sure the DNS records are updated to route the domain to this IP address. <pre><code>example.com {\n    reverse_proxy localhost:5000\n}\n</code></pre></p> <p>Here is Caddy as a reverse proxy doing load balancing, in a round-robin method. <pre><code>example.com {\n    reverse_proxy backend1:5000 backend2:5000 backend3:5000\n}\n</code></pre></p> <p>You can do path based proxying to serve different backends based on the url path. In this  example <code>example.com/api</code> will go to one web server, while <code>example.com/static</code> goes to another. <pre><code>example.com {\n    reverse_proxy /api backend1:5000\n    reverse_proxy /static backend2:5001\n    reverse_proxy /app backend3:5002\n}\n</code></pre></p> <p>You can setup to route subdomains as well. This will retrieve an SSL certificate for all domains added. <pre><code>example.com {\n    reverse_proxy localhost:3000\n}\n\napi.example.com {\n    reverse_proxy localhost:4000\n}\n</code></pre></p> <p>You can also proxy to external services. <pre><code>example.com {\n    reverse_proxy https:/api.example.com\n}\n</code></pre></p> <p>You can also configure domains to redirect to one domain. In this example all requests to  www.example.com will be rerouted to example.com.  <pre><code>example.com www.example.com {\n    reverse_proxy localhost:3000\n}\n</code></pre></p> <p>To run Caddy:</p> <ul> <li>Download to computer</li> <li>Setup config file <code>Caddyfile</code> typically in <code>/etc/caddy/Caddyfile</code></li> <li>Start the Caddy server <code>sudo systemctl start caddy</code></li> </ul> <p>To run in Docker:</p> <ul> <li> <p>Create a docker-compose.yml file. <pre><code>services:\n  caddy:\n    image: caddy:latest\n    container_name: caddy\n    ports:\n      - \"80:80\"        # HTTP\n      - \"443:443\"      # HTTPS\n    volumes:\n      - ./Caddyfile:/etc/caddy/Caddyfile   # Mount your Caddyfile\n      - ./data:/data                       # Let\u2019s Encrypt certificates\n      - ./config:/config                   # Caddy configuration\n    restart: unless-stopped\n</code></pre></p> </li> <li> <p>Create your Caddyfile (make sure to create it in the same location your volume is pointed to). <pre><code>example.com {\n    reverse_proxy backend:3000\n}\n</code></pre></p> </li> <li>Run <code>docker compose up -d</code> to start the server.</li> </ul> <p>Additional considerations to run in Docker:</p> <ul> <li>If you run Caddy in a docker container, <code>localhost</code> will be that container, due to dockers own DNS.</li> <li>If you run your web server that you're routing to in a docker container, you can use that container name     in the caddy file config.</li> </ul> <p>You can also route ports directly.</p> <pre><code>:8443 {\n    reverse_proxy 127.0.0.1:8000\n}\n</code></pre>"},{"location":"Networking/docs_101_caddy/#file-server","title":"File Server","text":"<p>Caddy has an option to serve static files over HTTP. This is not a file server like sFTP because it serves over http or https.</p> <pre><code>healthfin.solutions {\n    root * /srv/website\n    file_server\n}\n</code></pre> <p>Serve different sites with different paths of the same domain.</p> <pre><code>healthfin.solutions {\n    handle_path /notes* {\n        root * /srv/web_apps/notes\n        file_server\n    }\n\n    reverse_proxy homeserver.home:8000\n}\n\n:8009 {\n    root * /srv/web_apps/notes\n    file_server\n}\n</code></pre> <p>This config allows for access to the main server from the root domain, but  also changes to the static site when you add the /notes path.</p> <p>Don't forget you can setup a local DNS server on your machine and setup a  local domain DNS rewrite.</p> <pre><code>anderson.docs {\n    tls internal\n    root * /srv/web_apps/notes\n    file_server\n}\n</code></pre>"},{"location":"Networking/docs_102_dns_server/","title":"DNS Server","text":"<p>A DNS Server is a server that holds domain name records and point to the  corresponding IP addresses. </p>"},{"location":"Networking/docs_102_dns_server/#adguard","title":"Adguard","text":"<p>You can setup your own local DNS server on your network. Frequently these allow you to setup your own name resolutions on your network, but also block certain sites from resolving, which can prevent ads.</p>"},{"location":"Networking/docs_102_dns_server/#concept-1","title":"Concept 1","text":"<p>You can setup your own DNS server on your network. Open source servers include adguard, dnsmasq, and pihole. </p>  graph TD     A[Router]     B[Internet]     C[Home Server]     D[Computer 1]      B &lt;-..-&gt; A     C --Otherwise have your router use its default DNS--&gt; A     D --Resolve DNS Here First--&gt; C"},{"location":"Networking/docs_102_dns_server/#options","title":"Options","text":"<p>You have two options when setting up your own DNS server</p> <ol> <li>Set up individual devices on your network to use self-hosted DNS server</li> <li>Set up your router to point all devices to the self-hosted DNS server.</li> </ol>"},{"location":"Networking/docs_102_dns_server/#setup-adguard","title":"Setup Adguard","text":"<p>Docker compose file:</p> <pre><code>services:\n  adguardhome:\n    container_name: adguardhome\n    image: adguard/adguardhome\n    restart: unless-stopped\n    ports:\n      - \"53:53/udp\"\n      - \"53:53/tcp\"\n      - \"3000:80/tcp\"   # Web UI\n#      - \"443:443/tcp\" # (Optional, for HTTPS UI)\n    volumes:\n      - ./workdir:/opt/adguardhome/work\n      - ./confdir:/opt/adguardhome/conf\n</code></pre> <p><code>sudo docker compose up -d</code> - you now have a DNS server running.</p> <p>To access the web UI you can go to <code>http://&lt;ip address&gt;:3000</code>.</p> <p>To add a local domain, go to Filters&gt;DNS rewrites</p> <p>If you want to do option 1:</p> <p>Go to the device(s) you want to use your DNS server. Go to System Preferences -&gt; Network -&gt; Wifi (or Ethernet) -&gt; DNS. From there add the IP address of your  server running the DNS server.</p> <p>If you want option 2:</p> <p>Go into your router options. Find DNS settings (might be under DHCP Settings).</p> <p>Add your custom server, its a good idea to add a fallback server in case yours goes down. Google is 8.8.8.8 or cloudflare 1.1.1.1.</p>"},{"location":"Python/deployment/","title":"Deploying Python Programs","text":"<p>Deployment of a python program depends on what type of program you are deploying. Desktop apps, web apps, ETL scripts, API's, internal tools, etc. all have different requirements.</p>"},{"location":"Python/deployment/#deployment-types","title":"Deployment Types","text":"<p>There are 3 main forms:</p> <ol> <li>Standard script/venv.</li> <li>Docker container.</li> <li>Executable file.</li> </ol>"},{"location":"Python/deployment/#local-script-cron-job","title":"\ud83d\ude80 Local Script / Cron Job","text":"<p>Best for simple scripts (ETL, backups, notifications, etc.)</p> <ul> <li>Deploy: copy code to server (via <code>git clone</code>, <code>scp</code>, or syncing)</li> <li>Env: use a virtual environment (<code>uv venv</code>, <code>python -m venv</code>, or <code>conda</code>)</li> <li>Run: directly with <code>python script.py</code> or via cron/systemd</li> </ul> <p>\ud83d\udc49 Example:</p> <pre><code>cd ~/my_project                       # create folder\nuv venv                               # create the venv\nsource .venv/bin/activate             # activate the venv\nuv pip install -r requirements.txt    # install dependencies\n\n# run the script like\npython my_job.py\n# or \npython -m my_job\n</code></pre> <p>From here you can setup to run with a cronjob or another program. </p> <p>Cronjob: <pre><code>crontab -e\n\n# save this in the crontab file that opens\n0/20 * * * * source /Users/tyleranderson/PyCharmProjects/project/backup_tmp_dir.sh \n</code></pre></p> <p>or Airflow: <pre><code>import subprocess\nimport datetime\nfrom airflow.decorators import dag, task\n\n@dag(default_args={\"owner\": \"airflow\"},\n     schedule_interval=\"*/10 * * * *\",\n     dagrun_timeout=datetime.timedelta(hours=1),\n     max_active_runs=1,\n     max_active_tasks=1,\n     start_date=datetime.datetime(2025, 5, 1),\n     catchup=False)\ndef run_job():\n    @task\n    def job_1():\n        subprocess.run([\"bash\", \"-c\", \"source /path/to/file\"], check=True)\n\n    @task\n    def job_2():\n        subprocess.run([\"bash\", \"-c\", \"source /path/to/another_file\"], check=True)\n\n    job_1 &gt;&gt; job_2\n\n\nrun_job()\n</code></pre></p> <ul> <li>Pros: simple, fast.</li> <li>Cons: less portable, harder to reproduce if env drifts.</li> </ul>"},{"location":"Python/deployment/#docker-container","title":"\ud83d\udc33 Docker Container","text":"<p>Best for services you want consistent across dev/prod. Much more robust.</p> <ul> <li>Write a Dockerfile:</li> </ul> <pre><code>FROM python:3.12-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY .. .\nCMD [\"python\", \"my_job.py\"]\n</code></pre> <ul> <li>Deploy: <code>docker build -t my_job .</code> then <code>docker run my_job</code></li> <li>Schedule: cron on the host, or Kubernetes Job</li> </ul> <p>Pros: consistent, portable. Cons: more setup overhead than bare venv.</p>"},{"location":"Python/deployment/#executable-file","title":"\ud83e\udd16Executable File","text":"<p>You can also use pyinstaller (or other libraries that freeze your python code) to create an executable file. This would be a good idea for a desktop app or something that a user chooses to run on their own. </p> <p>Pro: no need for user to have python installed on their computer, easy to distribute.</p>"},{"location":"Python/deployment/#docker-python-and-multiple-execution-points","title":"Docker, Python, and Multiple Execution Points","text":"<p>In most modern setups, if you have a Python package that contains multiple ETL jobs,  the common pattern is to containerize the entire package and then deploy that image wherever you run jobs.</p> <p>The differences are in how you execute the individual jobs inside that container and how you manage orchestration.</p>"},{"location":"Python/deployment/#typical-pattern","title":"Typical Pattern","text":"<ol> <li> <p>Single container image</p> <ul> <li>You build one Docker image for your Python package (with all dependencies baked in).</li> <li>This image is versioned and pushed to a container registry (e.g., AWS ECR, GCP Artifact Registry, Docker Hub).</li> <li>Keeps dependencies consistent across jobs.</li> </ul> </li> <li> <p>Multiple entry points / commands</p> <ul> <li>Each ETL job is exposed via a CLI entry point in your <code>pyproject.toml</code> or <code>setup.py</code></li> </ul> <p><pre><code>[project.scripts]\netl_job_a = \"my_package.jobs.job_a:main\"\netl_job_b = \"my_package.jobs.job_b:main\"\n</code></pre> * At runtime, the orchestrator decides which command to run:</p> <p><pre><code>docker run my-etl-image:1.0 etl_job_a\n</code></pre> * This avoids rebuilding multiple images for each job.</p> </li> <li> <p>Orchestrator triggers containers</p> <ul> <li>Airflow, Prefect, Dagster, or Kubernetes CronJobs call the container with the appropriate arguments or entry point.</li> <li>Example in Airflow\u2019s <code>DockerOperator</code>:</li> </ul> <pre><code>DockerOperator(\n    task_id=\"run_etl_job_a\",\n    image=\"my-etl-image:1.0\",\n    command=\"etl_job_a\",\n    ...\n)\n</code></pre> </li> </ol>"},{"location":"Python/deployment/#why-this-approach-works-well","title":"Why This Approach Works Well","text":"<ul> <li>Reusability \u2014 One image can run all jobs, avoiding duplication.</li> <li>Consistency \u2014 All jobs share the same dependency versions and base environment.</li> <li>Smaller CI/CD surface \u2014 You only need to build and push one image per release.</li> <li>Version control \u2014 The image tag maps directly to your package release.</li> </ul> <p>\ud83d\udca1 Rule of thumb: If all ETL jobs share most dependencies and code, use one container image with multiple entry points. If jobs have very different stacks or run in completely different runtime environments, split them into separate images.</p>"},{"location":"Python/docs_1_what_is_python/","title":"What is Python","text":""},{"location":"Python/docs_1_what_is_python/#what-is-python","title":"What is Python?","text":"<ul> <li>Python is a general purpose programming language (its good at a lot of things).</li> <li>Its one of the most readable languages that exist. It also has one of the biggest communities which means there are a lot of people and resources to learn.</li> <li>Python is an interpreted language, there is no compiling required.</li> <li>Python is a dynamic language, no need to declare data types.</li> <li>There are several flavors of python, some of the most popular are:<ul> <li>CPython - the typical, most common, flavor of python. This is usually what people refer to when they say \"python\".    This is written in the C programming language and maintained by a large group of individuals.</li> <li>Jython - Instead of the C language, this flavor of python is written in Java.</li> <li>IronPython (aka IPython) - Instead of the C language, this flavor of python is written in .NET.</li> <li>Anaconda - This flavor of python comes with many common data analysis packages pre-installed. This is maintained by the Anaconda, Inc. company. Personally I think this is just CPython with bloat.</li> <li>PyPy - A fast, minimal version of python that uses a JIT compiler. A little more difficult to configure.</li> <li>Brython - A version of python that can run in a web browser, translating python code into JavaScript.</li> </ul> </li> </ul>"},{"location":"Python/docs_2_how_to_use/","title":"How to use Python","text":""},{"location":"Python/docs_2_how_to_use/#summary-steps","title":"Summary Steps","text":"<ol> <li>Create a virtual environment (venv).</li> <li>pip install any libraries to the venv.</li> <li>Write code in a .py file</li> <li>Run the .py file with the venv.</li> </ol> <p>Example:</p> <pre><code># create venv\nPython -m venv venv  \n\n# activate venv\nvenv\\Scripts\\activate  \n\n# install libraries to venv\npip install pandas sqlalchemy \n\n# run your Python file in your venv (if activated)\n. python_file.py  \n</code></pre>"},{"location":"Python/docs_2_how_to_use/#running-python","title":"Running Python","text":"<pre><code>[Python File]          1. write a .py file\n      |\n      v\n[Python Interpreter]   2. run .py file through interpreter\n      |\n      v\n  [Result]\n</code></pre> <ol> <li>Python files end with <code>.py</code> for example <code>new_file.py</code>. Notice all lowercase and underscore format.  Python heavily uses this snake case formatting.</li> <li>In your <code>.py</code> file, write python code and then run it through the interpreter by using the terminal. </li> <li>Reference the interpreter and then the .py file like the example below.</li> <li>Example: <code>C:\\username\\python\\bin\\python.exe path\\to\\your\\python\\file.py</code></li> <li>Note you'll modify this to the correct paths on your computer.</li> </ol>"},{"location":"Python/docs_2_how_to_use/#about-the-package-manager-pip","title":"About the package manager (pip)","text":"<ul> <li>The python community has a place where people can create and upload their own python packages. https://pypi.org/</li> <li>Python has a built-in manager that allows you to install packages from PyPI directly. This manager is called <code>pip</code></li> <li>There are many very popular libraries. Some popular and well utilized ones are:</li> <li>For data there is Pandas, SQLAlchemy,    DuckDB, Polars</li> <li>API handling - Requests, aiohttp</li> <li>Markup parsing - BeautifulSoup4, lxml</li> <li>Web servers - Flask, Django, FastAPI</li> <li>File transfer - Paramiko (SFTP)</li> <li>Environment variable usage - python-dotenv</li> <li>So much more...</li> <li>To make use of these, we need to use <code>pip</code> to install them into our python interpreter. But we don't want to install everything to the same python interpreter.  That would cause it to become bloated if every project used the same interpreter. For this reason, every python project  should have its own interpreter (see Best practices for using python)</li> </ul>"},{"location":"Python/docs_2_how_to_use/#best-practices-for-using-python","title":"Best practices for using python","text":"<ul> <li>Every python project should have its own interpreter, called a virtual environment or shortened to \"venv\". To do this, we need to create a copy of the python interpreter  for each project. PyCharm can manage this for you, or you can do it manually. If using PyCharm, you can skip to the pip commands.</li> <li>To do this manually, you should use the terminal to navigate to the folder your project files will exist in and type <code>python -m venv venv</code>   This runs a python built-in command (<code>python</code>) in module mode (<code>-m</code>) to run the module venv (<code>venv</code>) and names the new interpreter venv (<code>venv</code>).</li> <li>After you create the venv folder with the command, you'll need to activate it to install the libraries. To activate,    run the command <code>venv\\Scripts\\activate</code>. For linux or mac the command is <code>venv/bin/activate</code>.</li> <li>From here you should see the terminal change to <code>(venv) C:\\users\\username\\directory</code>. In PyCharm, you can open the terminal and this will already be setup.</li> <li>Now you can run pip commands like: </li> <li><code>pip install pandas</code> - install a package by using their PyPI name.</li> <li><code>pip install pandas sqlalchemy duckdb</code> - you can install more than one by chaining package names.</li> <li><code>pip uninstall pandas</code> - uninstall package by PyPI name.</li> <li><code>pip install --upgrade pandas</code> - upgrade already installed packages when a new version comes out.</li> <li><code>pip list</code> - show a list of installed packages.</li> <li><code>pip freeze &gt; requirements.txt</code> - save the names and versions of installed packages to a file.    Good for saving and distributing dependencies so others can clone and use your project.</li> <li>Use the venv the same way <code>C:\\path\\to\\venv\\python\\bin\\python.exe path\\to\\your\\python\\file.py</code>. PyCharm will automatically use these commands when you right-click and run a script.</li> </ul>"},{"location":"Python/docs_3_writing_code/","title":"Writing Python Code","text":"<p>Python uses white-space and tabs, instead of curly-braces and semicolons like other common languages.</p> <p>The simplest python script. This will print text to your terminal window. <pre><code>print('This is my first Python script')\n</code></pre></p>"},{"location":"Python/docs_3_writing_code/#data-types","title":"Data Types","text":"<p>Native data types in python include:</p> <ul> <li>Integer <code>int</code> - whole number ie: 5</li> <li>Float <code>float</code>- decimal number ie: 7.9</li> <li>String <code>str</code>- text surrounded by single or double quotes ie: 'This is a string' or \"This is a string\"</li> <li>List, Tuple <code>list, tuple</code>- grouped data combined into a collection that can be iterated over. Lists use square brackets ie: a  list of integers <code>[1, 2, 3, 4]</code>. Tuples use parenthesis ie: <code>(1, 2, 3, 4)</code>. Lists can be mutated and tuples cannot.</li> <li>Dictionary <code>dict</code>- key value pair collection of data, using curly-braces. ie: <code>{'key 1': 'value 1', 'key 2': 'value 2'}</code></li> </ul> <p>Dictionary methods:</p> <ul> <li>Get value from dictionary <code>dict_name['key_name']'</code> or <code>dict_name.get('key_name')</code></li> <li>Loop through the dict with: <pre><code>for k, v in dict_name.items():\n   print(k, v) \n</code></pre></li> </ul> <p>String methods:</p> <ul> <li>f-strings - inject variables into a string with curly braces <code>print(f'Variable = {x}')</code></li> <li>Combine iterable into string <code>''.join(iterable)</code></li> </ul>"},{"location":"Python/docs_3_writing_code/#variables","title":"Variables","text":"<p>Any of the data types can be loaded into a variable that can then be referenced later:</p> <pre><code>x = 14\nvariable_01 = 'This is a variable'\nalso_a_variable = {'foo': 'bar'}\n</code></pre> <p>Variables can be named almost anything, you just can't start a variable with a number and no special characters. </p> <p>You can perform mathematical operations, just like most programming languages.</p> <pre><code>x = 5  # set variable\nx = x + 1  # add 1 to x variable\nx = x * 10  # multiply x by 10\nx = x / 4  # divide x by 4\nx = x ** 2  # raise x to the power of 2\n\nx = 7 // 3  # floor division (divide and round down to the nearest whole number)\n# output: 2.333 but rounds down to 2\n</code></pre> <p>You can also use the math library in the python standard library to perform more complex mathematical functions.</p> <p>Math Standard Library Docs</p>"},{"location":"Python/docs_3_writing_code/#if-statements","title":"If Statements","text":"<p>If statements are great for checking conditions and running code if certain conditions are met.</p> <pre><code>x = 10\nif x &gt; 8:\n    print('x is greater than 8')\nelif x &lt;= 1:\n    print('x is less or equal to 1')\nelse:\n    print('x is between 1 and 8')\n</code></pre> <p>Keep in mind that checking equality should be done with two equal signs <code>==</code> because a single <code>=</code> is for assigning variables.</p> <pre><code>x = 100\n\nif x == 100:\n    print('x is equal to 100')\n</code></pre>"},{"location":"Python/docs_3_writing_code/#loops","title":"Loops","text":"<p>Types of loops: - For - iterate over an object. - While - loop until a condition is met. - List comprehension - single line loop over an object.</p> <p>For loop <pre><code>numbers = [2, 4, 6, 8, 10]\n\nfor number in numbers:\n    print(number)  # will print each number in the list\n</code></pre></p> <p>While loop <pre><code>x = 0\nwhile x &lt; 10:\n    print(x)\n    x = x + 1  # add 1 to x on each loop\n    # this loop will terminate once x is greater than 10\n</code></pre></p> <p>List comprehension - this is like a single line for loop <pre><code>numbers = [2, 4, 6, 8, 10]\n\n# if we want to do a simple change to all the numbers in this list, such as double the amounts, we can use list comprehension\nnumbers = [number * 2 for number in numbers]\n</code></pre></p>"},{"location":"Python/docs_3_writing_code/#comments","title":"Comments","text":"<p>Add comments to your code:</p> <ul> <li><code>#</code> - single line comments</li> <li><code>\"\"\" \"\"\"</code> - multi-line comments</li> </ul> <pre><code># single line comment.\n\n\"\"\"\nMulti like comment\nUse this to describe \nmany things.\n\"\"\"\n</code></pre>"},{"location":"Python/docs_3_writing_code/#importing-other-libraries","title":"Importing other libraries","text":"<p>Python comes with the \"Python Standard Library\" which has a lot of powerful modules ready to go, no installing required.</p> <p>Python Standard Library Docs</p> <p>To use any of these, or to use any libraries installed with pip, they must be imported. This is typically done at the top of the .py file.</p> <pre><code>import os  # standard library, lets you interact with objects on your operating system like files.\nimport pandas as pd  # third-party library installed with pip. This imports the package and renames it as pd\nfrom sqlalchemy import select, insert  # this imports specific parts of the sqlalchemy library, avoiding importing everything.\n</code></pre> <p>Load a csv file into a DataFrame using pandas.</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv('path/to/file.csv')\n</code></pre> <p>Connect to a database with sqlalchemy:</p> <pre><code># import library\nfrom sqlalchemy import create_engine, text\n\n# create an engine object to connect\nengine = create_engine('postgresql+psycopg2://username:password@servername:port/database')\n\n# connect to the database and query a table\nwith engine.connect() as conn:\n    query = 'SELECT * FROM table'\n    result = conn.execute(text(query))\n</code></pre>"},{"location":"Python/docs_3_writing_code/#functions","title":"Functions","text":"<p>Use the <code>def</code> keyword to define a function. You can use parameters to pass data into the function, process the data, and return a result.</p> <pre><code>def example_function():\n    pass\n\n\ndef repeat(message):\n    print(f'You said {message}')  # this will print back your message and uses f-strings to inject your message to another string.\n\n\ndef add_numbers(number_1, number_2):\n    return number_1 + number_2\n</code></pre> <p>You can create dynamic parameters using args and *kwargs <pre><code>def add_numbers(*args):\n    return_value = 0\n    for value in args:\n        return_value = return_value + value\n\n    return return_value\n</code></pre></p>"},{"location":"Python/docs_3_writing_code/#classes","title":"Classes","text":"<p>Classes are good for grouping similar data and functions into one object.  It's great for maintaining state of data, and performing certain operations on that data.</p> <pre><code>import time\n\n\nclass Car:\n  def __init__(self, year, make, model):\n    # This function runs when the class is first used. Good for loading data right at the beginning.\n    self.year = year\n    self.make = make\n    self.model = model\n    self.current_speed = 0\n\n  def accelerate(self):\n    self.current_speed = 10\n    # make car go\n\n  def stop(self):\n    self.current_speed = 0\n    # make care brake\n\n\ncar = Car(2024, 'Tesla', 'Model Y')  # create the car object\ncar.accelerate()                     # run the accelerate function (when it's in a class it's called a method)\ntime.sleep(5)                        # wait 5 seconds\ncar.stop()                           # run the stop method\n</code></pre>"},{"location":"Python/docs_4_building_large_projects/","title":"Building Large Projects","text":""},{"location":"Python/docs_4_building_large_projects/#general","title":"General","text":"<p>Large python projects can be extremely powerful,  but they must be organized a certain way to work properly.</p> <p>A project should always start with a root folder and other  folders and files will be housed inside the root folder.</p> <p>There are special <code>.py</code> files that python will use to do different things such as <code>__init__.py</code> and <code>__main__.py</code>.</p>"},{"location":"Python/docs_4_building_large_projects/#vocabulary","title":"Vocabulary","text":"<ul> <li>Module: .py file</li> <li>Package: a folder with <code>__init__.py</code> file in it. This will have code that can be imported.</li> <li>Library: One or more packages grouped together and distributed via PyPI. Built to be reusable blocks of code.</li> </ul>"},{"location":"Python/docs_4_building_large_projects/#the-classic-structure-double-name","title":"The Classic Structure (Double Name)","text":"<p>The classic method to package a project is with the double name method. This uses a root folder to hold all the metadata, README, LICENSE, pyproject, etc. and then  a subfolder that has the project code. Below is an example: <pre><code>pandas/               # repo root\n\u251c\u2500 pyproject.toml / setup.py / setup.cfg\n\u251c\u2500 README.md\n\u251c\u2500 LICENSE\n\u251c\u2500 pandas/            # actual Python package (import name = pandas)\n\u2502  \u251c\u2500 __init__.py\n\u2502  \u251c\u2500 core/\n\u2502  \u251c\u2500 io/\n\u2502  \u251c\u2500 util/\n\u2502  \u2514\u2500 ...\n\u251c\u2500 doc/               # documentation\n\u251c\u2500 tests/             # test suite\n\u2514\u2500 ...\n</code></pre></p> <p>This lets us run things like: <pre><code>python -m pandas\n\npython -m pandas.core\n</code></pre></p> <p>This setup gives the least amount of friction, is well established, and  gives good organization. The downside is that it can cause mask import mistakes where the script runs in dev but not in prod once you pip install it.</p> <p>You can avoid these issues by doing <code>pip install -e .</code> which installs your package in edit mode. Edit mode means you can make changes to the installed package and it will hot reload. </p>"},{"location":"Python/docs_4_building_large_projects/#the-workflow","title":"The Workflow","text":"<ol> <li>Build your project. </li> <li>Install your project <code>pip install -e .</code></li> <li>In <code>pyproject.toml</code> add: <pre><code>[project.scripts]\npackage_name = \"module:callable\"\n# this creates a command line command. Replace with a command and path to what gets executed.\n</code></pre></li> <li>Call like <code>package_name</code> if you want a terminal command.</li> </ol>"},{"location":"Python/docs_4_building_large_projects/#a-more-modern-system","title":"A More Modern System","text":""},{"location":"Python/docs_4_building_large_projects/#goals-of-a-good-structure","title":"Goals of a good structure","text":"<ul> <li>Separation of concerns: isolate domain logic, I/O, and interfaces.</li> <li>Testability &amp; maintainability: easy to unit test and refactor.</li> <li>Distributable: buildable wheel, clean dependency metadata.</li> <li>Usability: clear public API, stable CLI, sensible configuration.</li> </ul>"},{"location":"Python/docs_4_building_large_projects/#canonical-layout-src-structure","title":"Canonical layout (src/ structure)","text":"<pre><code>your-project/\n\u251c\u2500 pyproject.toml\n\u251c\u2500 README.md\n\u251c\u2500 LICENSE\n\u251c\u2500 .gitignore\n\u251c\u2500 src/\n\u2502  \u2514\u2500 your_package/\n\u2502     \u251c\u2500 __init__.py\n\u2502     \u251c\u2500 __main__.py           # optional: `python -m your_package`\n\u2502     \u251c\u2500 cli/                  # CLI commands\n\u2502     \u2502  \u2514\u2500 __init__.py\n\u2502     \u251c\u2500 core/                 # domain logic (pure, testable)\n\u2502     \u2502  \u251c\u2500 models.py\n\u2502     \u2502  \u251c\u2500 services.py\n\u2502     \u2502  \u2514\u2500 rules.py\n\u2502     \u251c\u2500 adapters/             # I/O boundaries (DB, HTTP, FS, APIs)\n\u2502     \u2502  \u251c\u2500 db.py\n\u2502     \u2502  \u251c\u2500 http.py\n\u2502     \u2502  \u2514\u2500 storage.py\n\u2502     \u251c\u2500 app/                  # application orchestration/wiring\n\u2502     \u2502  \u251c\u2500 config.py\n\u2502     \u2502  \u251c\u2500 logging.py\n\u2502     \u2502  \u2514\u2500 container.py       # dependency wiring/injection\n\u2502     \u2514\u2500 utils/                # cross-cutting helpers\n\u2502        \u2514\u2500 time.py\n\u251c\u2500 tests/\n\u2502  \u251c\u2500 conftest.py\n\u2502  \u251c\u2500 unit/\n\u2502  \u2514\u2500 integration/\n\u2514\u2500 examples/                    # optional usage demos / notebooks\n</code></pre> <p>Why <code>src/</code>? It prevents tests from accidentally importing  your working directory instead of the installed package,  catching packaging/import mistakes early.</p>"},{"location":"Python/docs_4_building_large_projects/#pyprojecttoml-modern-packaging","title":"pyproject.toml (modern packaging)","text":"<p>Use PEP 621 metadata; pick a backend (e.g., Hatchling, Setuptools).</p> <pre><code>[build-system]\nrequires = [\"hatchling&gt;=1.25\"]\nbuild-backend = \"hatchling.build\"\n\n[project]\nname = \"your-project\"\nversion = \"0.1.0\"\ndescription = \"Does X\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.10\"\nlicense = { file = \"LICENSE\" }\nauthors = [{ name = \"You\" }]\ndependencies = [\n  \"requests&gt;=2.32\",\n]\n\n[project.optional-dependencies]\ndev = [\"pytest\", \"mypy\", \"ruff\", \"black\", \"pytest-cov\"]\n\n[project.scripts]                 # creates console commands on install\nyour-cli = \"your_package.cli:main\"\n\n[tool.hatch.build.targets.wheel]\npackages = [\"src/your_package\"]\n</code></pre> <ul> <li>[project.scripts] wires a console script (entry point). It points to a callable (e.g., <code>def main(): ...</code>).</li> <li>Prefer runtime deps under <code>[project.dependencies]</code> and dev tools under an optional extra (e.g., <code>pip install .[dev]</code>).</li> </ul>"},{"location":"Python/docs_4_building_large_projects/#public-api-design","title":"Public API design","text":"<ul> <li>Use <code>__init__.py</code> to re-export the small, stable surface you commit to:</li> </ul> <p><pre><code># src/your_package/__init__.py\nfrom .core.models import Thing\nfrom .core.services import Service\n\n__all__ = [\"Thing\", \"Service\"]\n</code></pre> * Keep internal modules private-ish (don\u2019t re-export them) so refactors don\u2019t break users.</p>"},{"location":"Python/docs_4_building_large_projects/#layers-that-scale","title":"Layers that scale","text":"<ul> <li>core/: pure business rules, no network/filesystem. Most unit tests live here.</li> <li>adapters/: concrete I/O implementations (Postgres, S3, HTTP, etc.).</li> <li>app/ (or service/): composition, config, logging, dependency injection, runtime wiring.</li> <li>cli/: thin commands that call into app/ (don\u2019t bury logic in click/argparse handlers).</li> </ul> <p>This \u201cports &amp; adapters\u201d (hexagonal) approach keeps logic decoupled from I/O so you can swap adapters (e.g., SQLite \u2192 Postgres) or test with fakes.</p>"},{"location":"Python/docs_4_building_large_projects/#configuration-secrets","title":"Configuration &amp; secrets","text":"<ul> <li>Read config from environment first, with optional <code>.env</code> during development.</li> <li>Centralize in <code>app/config.py</code>:</li> </ul> <p><pre><code>from dataclasses import dataclass\nimport os\n\n@dataclass\nclass Settings:\n    db_url: str = os.environ.get(\"APP_DB_URL\", \"sqlite:///local.db\")\n    log_level: str = os.environ.get(\"APP_LOG_LEVEL\", \"INFO\")\n\nsettings = Settings()\n</code></pre> * Avoid global state; pass <code>settings</code> into your app wiring or use a small container.</p>"},{"location":"Python/docs_4_building_large_projects/#logging","title":"Logging","text":"<ul> <li>Configure once in <code>app/logging.py</code> and call it from your CLI entry:</li> </ul> <pre><code>import logging, sys\n\ndef setup(level: str = \"INFO\"):\n    logging.basicConfig(\n        level=level,\n        format=\"%(asctime)s %(levelname)s %(name)s: %(message)s\",\n        stream=sys.stdout,\n    )\n</code></pre>"},{"location":"Python/docs_4_building_large_projects/#testing-strategy","title":"Testing strategy","text":"<ul> <li>pytest with <code>tests/unit</code> for pure logic and <code>tests/integration</code> for external systems.</li> <li>Use fakes or fixtures for adapters; avoid hitting the network in unit tests.</li> <li>Keep test imports identical to users: <code>from your_package import Service</code>.</li> </ul>"},{"location":"Python/docs_4_building_large_projects/#data-and-resources","title":"Data and resources","text":"<ul> <li>Keep runtime data in the package only if truly needed; prefer external files or embedded resources loaded via <code>importlib.resources</code> rather than relative paths.</li> <li>For large assets, ship them separately or fetch at runtime.</li> </ul>"},{"location":"Python/docs_4_building_large_projects/#cli-vs-library","title":"CLI vs library","text":"<ul> <li>Treat the package as a library first; keep the CLI thin. That makes automation and testing easier and lets others use your code programmatically.</li> </ul>"},{"location":"Python/docs_4_building_large_projects/#common-pitfalls-to-avoid","title":"Common pitfalls to avoid","text":"<ul> <li>Logic in scripts: move code out of <code>if __name__ == \"__main__\":</code> into functions/classes.</li> <li>Tight coupling to I/O: keep core pure; push side effects to adapters.</li> <li>Leaky imports: don\u2019t rely on import side effects; be explicit.</li> <li>Messy init: avoid heavy work at import time; initialize in <code>main()</code> / app wiring.</li> </ul>"},{"location":"Python/docs_4_building_large_projects/#minimal-example-for-a-cli","title":"Minimal example for a CLI","text":"<pre><code># src/your_package/cli.py\nimport argparse\nfrom .app.logging import setup as setup_logging\nfrom .app.config import settings\nfrom .core.services import Service\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--log-level\", default=settings.log_level)\n    args = parser.parse_args()\n\n    setup_logging(args.log_level)\n    Service().run()\n</code></pre>"},{"location":"Python/docs_5_advanced_topics/","title":"Advanced Topics","text":""},{"location":"Python/docs_5_advanced_topics/#dynamic-function-parameters","title":"Dynamic Function Parameters","text":"<p>If you have a function and you want to be able to pass any number of arguments or keyword arguments, you can use args and kwargs.</p> <pre><code>def function_name(*args, **kwargs):\n    # in the function you can access args and kwargs\n    for arg in args:\n        print(arg)\n    for kwarg in kwargs:\n        print(kwarg)\n</code></pre> <p>You can also unpack dictionaries directly into a function call. <pre><code>def greet(name, age):\n    print(f\"Hello, {name}. You are {age} years old.\")\n\ndata = {\"name\": \"Alice\", \"age\": 25}\ngreet(**data)  # Equivalent to greet(name=\"Alice\", age=25)\n</code></pre></p>"},{"location":"Python/docs_5_advanced_topics/#context-managers","title":"Context Managers","text":"<p>In programming there are a lot of instances where you'll need to open something (file, database connection, etc.) and you'll need to make sure to close it to free up resources and to protect the file from being corrupted or altered.</p> <p>A very safe way to handle this is with context managers, using the <code>with</code> keyword.</p> <p>Read, write, append to files.</p> <pre><code># write to file by opening in write mode 'w'\nwith open('file.txt', 'w') as write_file:\n    write_file.write('Hello there')\n# file is closed at this point\n\nwith open('file.txt', 'r') as read_only_file:\n    file_data = read_only_file.read()\n\n# close the file and use the data from it\nprint(file_data)\n\nwith open('file.txt', 'a') as append_file:\n    append_file.write('This is how to append text to file.')\n\nwith open('file.txt', 'rw') as read_write_file:\n    read_write_file.write('info')\n    data = read_write_file.read()\n</code></pre> <p>Safely manage connections to a database.</p> <pre><code>from sqlalchemy import create_engine\n\nengine = create_engine('connection_string')\n\nwith engine.connect() as conn:\n    conn.execute('query here')\n</code></pre> <p>You can create your own objects with context managers.</p> <pre><code>class NewObject:\n    def __init__(self, vars):\n        self.vars = vars\n\n    def __enter__(self):\n        # open the connection using with statement\n        self.open()\n\n    def __exit__(self):\n        # automatically run this on exiting with statement\n        self.close()\n</code></pre>"},{"location":"Python/docs_5_advanced_topics/#decorators","title":"Decorators","text":"<p>Decorators are a way to add extra functionality to other functions.</p> <pre><code>import functools\n\ndef decorator_name(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # do something before the wrapped function\n        wrapper_value = func(*args, **kwargs)\n        # do something after the wrapped function\n        return wrapper_value\n    return wrapper\n</code></pre> <p>To use this defined decorator, it would look something like this: <pre><code>@decorator_name\ndef wrapped_function():\n    # regular function stuff\n</code></pre></p> <p>If you've defined the decorator to also take arguments, you can add them: <pre><code>import functools\n\ndef decorator_name(arg1, arg2):\n    def decorator_wrap(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # do something before the wrapped function\n            wrapper_value = func(*args, **kwargs)\n            # do something after the wrapped function\n            return wrapper_value\n        return wrapper\n    return decorator_name\n\n\n@decorator_name(arg1, arg2)\ndef wrapped_function():\n    # regular function stuff\n</code></pre></p> <p>Here is a more in-depth tutorial on decorators https://realpython.com/primer-on-python-decorators/</p> <p>Some favorite decorators:</p> <pre><code>def time_it(func):\n    \"\"\"Prints the time it takes for a function to run\"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = datetime.datetime.now()\n        return_val = func(*args, **kwargs)\n        print(f'{func.__name__} ran successfully in {datetime.datetime.now() - start_time}')\n        return return_val\n    return wrapper\n</code></pre> <pre><code>def avoid_day_of_week(day_of_week: List[int]):\n    \"\"\"\n    Avoid running the decorated function on certain day of week.\n\n    day_of_week:\n        0 -&gt; Mon\n        1 -&gt; Tue\n        2 -&gt; Wed\n        3 -&gt; Thu\n        4 -&gt; Fri\n        5 -&gt; Sat\n        6 -&gt; Sun\n    \"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            current_dt = utc_now()\n            dow = calendar.weekday(current_dt.year, current_dt.month, current_dt.day)\n            if dow in day_of_week:\n                logit(f'Avoiding day of week {calendar.day_abbr[dow]} day integer ({dow})')\n                return\n            else:\n                func(*args, **kwargs)\n\n        return wrapper\n    return decorator\n</code></pre>"},{"location":"Python/docs_5_advanced_topics/#generators","title":"Generators","text":"<p>Generators can be thought of as iterables that are not fully loaded into memory. This allows you can handle the  same data, without worrying about memory consumption.</p> <p>A generator is defined by a function that uses the keyword <code>yield</code>. When a function is used, it is run and then  looses all state afterward. A generator will maintain state and can be called again multiple times.</p> <p>For example, imagine you need to pull API data using every individual for the last 20 years. Instead of loading all dates into one big list, you can create a generator function to calculate it.</p> <pre><code>import datetime\nimport requests\n\n# this is a generator because it uses the keyword yield.\ndef date_generator(start_date):\n    today = datetime.datetime.today()\n    while start_date &lt; today:\n        yield start_date  # this gets returned. If this function is called again, it will start here.\n        start_date = start_date + 1\n\n\nurl = 'example_url.com/api'\nfor date in date_generator(datetime.datetime.date(1990, 1, 1)):\n    response = requests.get(f'{url}/date_param={date}')\n</code></pre> <p>If you don't want to loop over a generator, you can also use the <code>.next()</code> method.</p> <pre><code>gen = generator_function()\nnext_value = gen.next()\n</code></pre> <p>More detailed info here https://wiki.python.org/moin/Generators</p>"},{"location":"Python/docs_5_advanced_topics/#python-import-system","title":"Python Import System","text":"<p>Importing libraries can be tricky in certain cases. The typical use is to just import any python package such as  <code>import sys</code></p> <p>The best option 99% of the time is going to be run scripts in module mode. Read below to learn more about how the python import system works.</p> <p>A python interpreter or virtual environment has a list of directories to look for imports from. It will start to look for the import in the first item of the list and go through each directory  until it finds the import or fails and raises an exception.  </p> <p>If you are running a module that imports another module that you've created, in a directory  outside of the current directory, then you might need to add that directory to the sys.path. </p> <ul> <li> <p>Option 1: Add a pth file in the virtual environment with the path you want to add.</p> </li> <li> <p>Go to venv/lib/python/site-packages/ <li> <p>Add your path such as /home/tyranderson/snfStudyData</p> </li> <li> <p>Option 2: Hardcode the path directly into the activate file. In a venv you can edit the bin/activate file and include: <code>export PYTHONPATH=\"/the/path/you/want\"</code></p> </li> <li> <p>Option 3: Add into python script - within your script you can <code>sys.path.append(\"/the/path/you/want\")</code> but this is temporary and the path will be dropped once the script is done running.  </p> </li> <p>More info here: https://help.pythonanywhere.com/pages/DebuggingImportError</p>"},{"location":"Python/docs_5_advanced_topics/#run-scripts-in-module-mode","title":"Run scripts in module mode","text":"<p>You can run scripts from a venv 2 different ways:</p> <pre><code># standalone script\npath/to/venv/bin/python path/to/script\n# If this script has imports to other files, it will have import errors. NOT RECOMMENDED!\n\n# module mode. RECOMMENDED!!\nPath/to/venv/bin/python -m path.to.script\n</code></pre> <p>Linux <pre><code>cd path/to/project\n. venv/bin/activate -m path.to.python.file\n</code></pre></p> <p>Windows <pre><code>cd path\\to\\project\nvenv\\Scripts\\activate -m path.to.python.file\n</code></pre></p> <p>Using module mode is considered best practice because it allows all modules to import from the project root properly. If you have multiple python modules (.py files) you are importing from various directories within the project, you will likely have import errors if you try to run using the standalone method. This is why its considered best practice to run in module mode. (PyCharm does this for you when you run a script).</p>"},{"location":"Python/docs_5_advanced_topics/#asyncio","title":"Asyncio","text":"<p>All the code up to this point has been synchronous, meaning everything happens one step at a time. Asynchronous (async) code can be used to handle multiple tasks at the same time.</p> <p>Be aware, async code is more complicated than synchronous in any language, including python. </p> <p>When should I use async code?</p> <p>If your code is...</p> <ul> <li>CPU Bound - use Multi Processing</li> <li>IO bound, fast IO, limited number of connections - use Threading</li> <li>IO bound, slow IO, many connections - use Asyncio</li> </ul> <p>In other words.... <pre><code>if io_bound:\n    if io_very_slow:\n        print('Use asyncio')\n    else:\n        print('Use threads')\nelse:\n    print('Use Multi processing')\n</code></pre></p>"},{"location":"Python/docs_5_advanced_topics/#general-info","title":"General Info","text":"<p>The most important keywords:  <code>async</code> and <code>await</code></p> <p>The most important functions: <code>asyncio.run()</code> and <code>asyncio.gather()</code></p> <ul> <li><code>async</code> - this keyword is used to define a function that will be capable of running async. This turns the  function into whats called a coroutine. If a function is a coroutine (has the async keyword), then it has to  utilize the await keyword as well.</li> <li><code>await</code> - is the keyword that is used to let python know that a function is going to take some time to resolve.  It will stop trying to run the function and move on to another task while it waits for the task to resolve.</li> <li><code>asyncio.run()</code> - this is the most common way to run a coroutine (function with async). A coroutine cannot be  called like a normal function.</li> <li><code>asyncio.gather()</code> - this is not required but is extremely useful and common. <code>gather</code> allows you to run  multiple coroutines at the same time.</li> </ul> <p>The best way to understand async code is to experiment with it. Lets look at some examples.</p> <pre><code>import asyncio\n\n# this is a coroutine\nasync def async_function(request_data):\n    data = await some_async_task(request_data)\n    return data\n\n# this is a coroutine\nasync def main():\n    results = await asyncio.gather(\n      async_function('x'), \n      async_function('y'),\n      async_function('z')\n    )\n    print(results)\n\nif __name__ == '__main__':\n    asyncio.run(main())  # this is how you run a coroutine\n</code></pre>"},{"location":"Python/docs_5_advanced_topics/#cython","title":"Cython","text":"<p>One of the biggest criticisms of python is its performance. When you compare it to statically typed, compiled languages, it doesn't have near the same speed. One option to improve performance is with something called Cython. Cython is an extension of python that allows statically typed python that can be compiled to C code for performance enhancements.</p> <p>Official Cython Docs</p> <p>Example:</p> <pre><code># example.pyx file\n\ndef sum_integers(int n):\n    cdef int i\n    cdef int total = 0\n    for i in range(n):\n        total += i\n    return total\n</code></pre> <p>Create setup.py file:</p> <p><pre><code>from setuptools import setup\nfrom Cython.Build import cythonize\n\nsetup(\n    ext_modules = cythonize(\"example.pyx\")\n)\n</code></pre> run python setup.py build_ext --inplace</p>"},{"location":"Python/docs_5_advanced_topics/#global-interpreter-lock-gil","title":"Global Interpreter Lock (GIL)","text":"<p>To achieve thread safety in python, there is something called the Global Interpreter Lock (GIL). The GIL is  a bit of a double-edged sword because it achieves thread safety, but it also makes python slower due to running everything on one thread. Python 3.13 has introduced an experimental mode where the GIL can be deactivated. The most important thing here for now is to know it exists.</p>"},{"location":"Python/docs_5_advanced_topics/#mixin","title":"Mixin","text":"<p>A Mixin is a class that is built for the purpose of being inhereted. It adds functionality of one class to another. If you write several classes and all of them need the same methods, you  can make a Mixin class and then have the other classes inherit the Mixin.</p> <pre><code>class UserMixin:\n    def login_user(self):\n        login()\n\n\nclass User(UserMixin):\n</code></pre>"},{"location":"Python/docs_5_advanced_topics/#other-helpful-tips","title":"Other Helpful Tips","text":"<p>Unpacking Iterables is a useful trick. If you have a list or other iterable that you want to perform an operation on you can unpack it with an asterisk.</p> <pre><code># with unpacking\nnumbers = [1, 2, 3, 4]\nprint(*numbers)\n\n# this gives the same result, but requires more code\nnumbers = [1, 2, 3, 4]\nfor number in numbers:\n    print(number)\n</code></pre> <p>The double asterisk ** is for unpacking keyword arguments. This is why they are used in functions with args and kwargs.</p> <pre><code>def function_name(*args, **kwargs):\n    # in the function you can access args and kwargs\n    print(args[0])\n    print(kwargs['item_01'])\n\n\nfunction_name(1, 2, 3, item_01='value 1', item_02='value 2')\n</code></pre>"},{"location":"Python/docs_6_publishing_project/","title":"Publishing a Project","text":"<p>You can create your own python library and publish it to PyPI. First you'll need to create an account and download your API keys. Once you have those established, you can create your project and then:</p> <p>Super summary:</p> <ol> <li>Create the pyproject.toml file and fill out the fields </li> <li>Build the project with <code>uv build</code></li> <li>Publish the project to PyPI with <code>uv publish</code></li> </ol>"},{"location":"Python/docs_6_publishing_project/#with-uv","title":"With UV","text":"<pre><code>[project]\nname = \"wws-api\"\nversion = \"0.1.0\"\ndescription = \"Simplifies the process for calling the Workday Web Services API asyncronously.\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.11\"\ndependencies = [\n   \"aiohttp&gt;=3.12.15\",\n   \"lxml&gt;=6.0.0\",\n   \"pyarrow&gt;=21.0.0\",\n   \"xmltodict&gt;=0.14.2\",\n]\n\n[build-system]\nrequires = [\"uv_build&gt;=0.8.7,&lt;0.9.0\"]\nbuild-backend = \"uv_build\"\n\n[tool.uv.build-backend]\n# module name is the normalized project name (dash -&gt; underscore)\nmodule-name = \"wws_api\"  # folder to reference if src is not there\n# module root \"\" = repo root (i.e., not using src/)\nmodule-root = \"\"\n\n[project.scripts]\n# optional: cli commands such as\nwws_api = 'wws_api.cli:main' # folder.directory:function\n\n[[tool.uv.index]]\nname = \"testpypi\"\nurl = \"https://test.pypi.org/simple/\"\npublish-url = \"https://test.pypi.org/legacy/\"\nexplicit = true\n</code></pre> <p>Build wheel file with uv build system: <code>uv build</code></p> <p>Publish project to test PyPI <code>uv publish --index testpypi --token &lt;api token&gt;</code></p> <p>Publish project to production PyPI <code>uv publish --token &lt;api token&gt;</code></p> <p>Bump your project version automatically: <pre><code>uv version --bump major\nuv version --bump minor\nuv version --bump patch\n</code></pre></p> <p>Script to do the full build: <pre><code>rm -rf dist/\nuv version --bump minor\nuv build\nuv publish --token\n</code></pre></p>"},{"location":"Python/docs_6_publishing_project/#without-uv","title":"Without UV","text":"<p>More detailed steps:</p> <ol> <li>Make sure all of your files are created inside a folder structure</li> <li>Create pyproject.toml    <pre><code>[build-system] \nrequires = [\"setuptools\", \"wheel\"] \nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"example_package_YOUR_USERNAME_HERE\"\nversion = \"0.0.1\"\nauthors = [\n{ name=\"Example Author\", email=\"author@example.com\" },\n]\ndescription = \"A small example package\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.8\"\nclassifiers = [\n  \"Programming Language :: Python :: 3\",\n  \"License :: OSI Approved :: MIT License\",\n  \"Operating System :: OS Independent\",\n]\n\n[project.urls]\nHomepage = \"https://github.com/pypa/sampleproject\"\nIssues = \"https://github.com/pypa/sampleproject/issues\"\n</code></pre></li> <li>Make sure all relevant build libs are installed:    <pre><code>pip install --upgrade build\npip install twine\n</code></pre></li> <li>Run the build - <code>python -m build</code></li> <li>Send to PyPI test - <code>twine upload --repository testpypi dist/*</code></li> <li>Send to PyPI - <code>twine upload dist/*</code></li> </ol> <p>Make sure your build dependencies are not stored in your project dependencies.</p> <p>Another TOML file might look like this:</p> <pre><code>[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"py_simple_sharepoint\"  # actual name that will get used for pip install.\nversion = \"0.1.1\"  # anytime you submit an update to PyPI, you must change the version.\ndescription = \"A SharePoint file management tool for python programs.\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.10\"\ndependencies = [  # package dependencies, does install when pip installed\n    \"office365-rest-python-client&gt;=2.6.2\",\n]\n\n[project.optional-dependencies]  # build dependencies (does not install when pip installed)\ndev = [\n    \"build\",\n    \"twine\"\n]\n</code></pre> <p>https://packaging.python.org/en/latest/tutorials/packaging-projects/#creating-the-package-files</p> <p>There are also new tools coming out such as poetry and uv.  The community has started to heavily embrace UV due to its speed and tooling.</p>"},{"location":"Python/docs_7_important_libraries/","title":"Important Libraries - Getting Started","text":""},{"location":"Python/docs_7_important_libraries/#pandas","title":"Pandas","text":"<p>Pandas is a well-known data handling library. It has the ability to extract, manipulate, and load data.</p> <p>The central component to pandas is the DataFrame. This is where tabular data is loaded and lives in memory. <pre><code>import pandas as pd\n\n# blank dataframe\ndf = pd.DataFrame()\n\n# load a dict into a dataframe\nsample_data = {'id', [1, 2, 3], 'value', [200, 300, 400]}\ndf = pd.DataFrame(sample_data)\n\n# load direct from other formats\ndf = pd.read_csv('path/to/csv/file.csv')\ndf = pd.read_parquet('path')\ndf = pd.read_excel('path')\ndf = pd.read_json('path')\ndf = pd.read_sql('sql statement', sqlalchemy_conn)\ndf = pd.read_html('io')\ndf = pd.read_xml('io')\ndf = pd.read_clipboard()\n</code></pre></p> <p>More here https://pandas.pydata.org/docs/reference/io.html</p>"},{"location":"Python/docs_7_important_libraries/#transformations","title":"Transformations","text":"<p>Once you have data loaded into a dataframe, you can perform all kinds of operations on the values. There are generally two ways of performing modifications. Iterating through each value (not recommended) and across columns (called vectorization). A vectorized operation can look like this:</p> <pre><code>df = pd.read_csv('path/to/file.csv') # sample data\n\n# performing math computations across columns. You can overwrite existing columns or create new columns\ndf['int_column_new'] = df['int_column_01'] + df['int_column_02']\ndf['int_column_01'] = df['int_column_01'] - df['int_column_02']\ndf['float_column_new'] = df['float_column_01'] * df['float_column_02']\ndf['float_column_01'] = df['float_column_01'] / df['float_column_02']\n\n# perform string operations\ndf['string_col'] = df['string_col'].str.replace('-', '')\n</code></pre>"},{"location":"Python/docs_7_important_libraries/#filtering","title":"Filtering","text":"<pre><code>df = pd.read_csv('path/to/file.csv') # sample data\n\n# filter where column_name is equal to a value\ndf = df[df['column_name'] == 'certain_value']\n# not equal\ndf = df[df['column_name'] != 'certain_value']\n# substring\ndf = df[df['column_name'].str.contains('partial_string_match')]\n# drop rows where column_name equals any of the list values\ndf = df[~df['column_name'].isin(['list', 'of', 'values'])]\n</code></pre>"},{"location":"Python/docs_7_important_libraries/#change-data-types","title":"Change Data Types","text":"<pre><code>df = pd.read_csv('path/to/file.csv') # sample data\n\ndf['str_num_values'] = df['str_num_values'].astype(int)\ndf['str_num_values'] = df['str_num_values'].astype(str)\ndf['str_num_values'] = df['str_num_values'].astype(float)\n\n# dates\ndf['date_str_values'] = pd.to_datetime(df['date_str_values'])  # gives datetime format\ndf['date_str_values'] = pd.to_datetime(df['date_str_values']).dt.date  # gives just date, no time\n</code></pre> <p>Helpful functions:</p> <p>Merge (like SQL join) Melt (convert columns to rows)</p>"},{"location":"Python/docs_7_important_libraries/#sqlalchemy","title":"SQLAlchemy","text":"<p>SQLAlchemy is a robust database management utility library. There are two main components; Core and ORM. Core is more base level, closer to the database API, while ORM aims to abstract some of the complexity of managing connections with sessions. Generally Core is better for pure database operations while the  ORM is geared toward web applications.</p> <p>Has full compatibility with Postgres, MySQL, SQLite, SQL Server, and Oracle.</p> <p>Connection strings look like this:</p> <pre><code>db_str = 'dialect+driver://username:password@host:port/database'\n\npostgres = \"postgresql+psycopg2://scott:tiger@localhost/public\"\n\nsql_server = 'mssql+pyodbc://host/database?trusted_connection=yes&amp;driver=ODBC+Driver+18+for+SQL+Server&amp;TrustServerCertificate=yes'\n</code></pre> <p>Modeling Tables</p> <pre><code>from sqlalchemy import (create_engine, Table, MetaData, Column, String, \n                        Integer, Double, Date, DateTime, Boolean, func)\n\nengine = create_engine('postgresql+psycopg2://scott:tiger@localhost/public')  # engines are what connects to the db\n\nmeta = MetaData()  # create a metadata object to attach tables to\n\n# model out the table\ntable1 = Table('table_name',\n               meta,\n               Column('id', Integer, primary_key=True),\n               Column('column1', String(255), unique=True),\n               Column('column2', Double),\n               Column('column3', Boolean, default=False),\n               Column('column4', Date),\n               Column('created_date', DateTime,\n                        server_default=func.now()),\n               schema='public'\n               )\n\nmeta.create_all(engine)  # create all the tables that are connected to the metadata object, in the database\n</code></pre> <p>Once you have your model and engine ready, you can connect and start running operations:</p> <p><pre><code>from sqlalchemy import create_engine, text\n\nengine = create_engine('connection_string_to_db')\n\nwith engine.connect() as conn:\n    query = 'SELECT * FROM table_name'\n    result = conn.execute(text(query)).fetchall()\n</code></pre> This will return a list of Row objects which contain the values in tuples (even if you only selected one column). To pull these out of the nested tuples, you can use list comprehension.</p> <pre><code>result = [r[0] for r in result]\n</code></pre> <p>Note that to run a raw SQL query we had to put it into a text() function. This function will sanitize the query to  make sure there is no malicious injection happening. Sometimes it is difficult to transform data values in python into a raw SQL query. SQLAlchemy has objects that can perform the same database functions, but in a more pythonic way.</p> <pre><code>from sqlalchemy import select, insert, update, delete\n\ndf = pd.DataFrame(values)\ndata_dict = df.to_dict(orient='records')  # transform pandas dataframe into a dictionary of values\n\nwith engine.connect() as conn:                              # connect to database\n    insert_stmt = insert(table_object).values(data_dict)    # create the insert statement\n    conn.execute(insert_stmt)                               # execute the statement\n    conn.commit()                                           # commit the changes\n\n    value = conn.execute(select(table_object)).fetchone()\n    values = conn.execute(select(table_object)).fetchall()\n\n    update_stmt = update(user_table).where(user_table.c.id == 5).values(name=\"user #5\")\n    conn.execute(update_stmt)\n\n    delete_stmt = delete(user_table).where(user_table.c.id == 5)\n    conn.execute(delete_stmt)\n    conn.commit()\n</code></pre>"},{"location":"Python/docs_7_important_libraries/#sqlite","title":"SQLite","text":"<p>SQLite is an embeddable OLTP database. In other words, it is a database that can be run in memory or from a file. It is the most common database in the world being used in web apps, phone apps, IoT devices, and more.</p> <p>SQLite Docs</p>"},{"location":"Python/docs_7_important_libraries/#duckdb","title":"DuckDB","text":"<p>DuckDB is similar to SQLite, but is an OLAP database. It is incredibly fast and effective and processing large amounts of data in memory. It can connect to other databases and perform direct queries on it, even being able to  write queries on multiple separate databases  Mix and Match.</p> <p>DuckDB Docs</p>"},{"location":"Python/docs_7_important_libraries/#pyspark","title":"Pyspark","text":"<p>Spark is a distributed data handling library written in Java. It has a python API that allows users to use Spark with Pyspark. In some ways it is very similar to pandas, but it can directly read and write to Delta Lake tables, and it spreads the data handling tasks across multiple machines (distributed).</p> <pre><code>df = spark.createDataFrame(data)\ncurrent_records = spark.sql(\"SELECT COALESCE(MAX(created_date), '2000-01-01') FROM lh_gold_01.fact_cms_star_rating\").first()[0]\n</code></pre>"},{"location":"Python/docs_7_important_libraries/#dotenv","title":"dotenv","text":"<p>dotenv is a lightweight package that allows you to load environment variables into memory from a file. </p> <p><code>pip install python-dotenv</code></p> <p>Create a <code>.env</code> file in your project like:</p> <pre><code>ENV_VAR_NAME='ENV_VAR_VALUE'\nAPI_KEY_01='2JHDKFJH3KF'\n</code></pre> <pre><code>import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nenv_var = os.getenv('ENV_VAR_NAME')\napi_key = os.getenv('API_KEY_01')\n</code></pre>"},{"location":"Python/docs_7_important_libraries/#requests","title":"Requests","text":"<p>Requests is a user-friendly way to make HTTP calls. Very good for API's.</p> <p><code>pip install requests</code></p> <pre><code>import requests\n\nurl = 'https://data.cms.gov/provider-data/api/1/metastore/schemas/dataset/items/4pq5-n9py?show-reference-ids=false'\nresponse = requests.get(url)\nprint(response.status_code)\nprint(response.headers)\nprint(response.text)\n</code></pre> <p>Send post requests with auth details and payload.</p> <pre><code>import requests\nfrom io import BytesIO\n\nresponse = requests.get(url, \n                        auth={'user': 'username', 'password': 'password_here'},\n                        data={'payload_json': {'data': 'value'}},\n                        files={'file': BytesIO()})\n</code></pre>"},{"location":"Python/docs_7_important_libraries/#airflow","title":"Airflow","text":"<p>Airflow is an orchestration tool to run and monitor jobs. It allows you to program it in a way that it can have dependencies. For example, only run one script after another has run successfully.</p> <p>Airflow Docs</p>"},{"location":"Python/docs_7_important_libraries/#flask","title":"Flask","text":"<p>Flask is a popular web app framework. It is very lightweight and has many \"plugin\" type packages that are built to  be pieced together to achieve all desired features.</p> <p>Web workers rule of thumb:</p> <ul> <li>Sync workers: ~50 - 100 concurrent users per worker.</li> <li>Async workers: ~500 - 1000+ concurrent users per worker.</li> </ul> <p>Running a flask server:</p> <ul> <li>Standard run Flask: <code>flask --app &lt;.py file&gt; run</code> like <code>flask --app app run</code></li> <li>Run Flask on different port: <code>flask --app app run --port 8080</code></li> <li>Run Flask and expose to network: <code>flask --app web_app/app run --host 0.0.0.0 --port 8080</code></li> <li>Run Flask in prod with gunicorn: <code>gunicorn -w 4 -b 0.0.0.0:8080 web_app.app:app</code></li> </ul>"},{"location":"Python/docs_7_important_libraries/#self-hosting","title":"Self Hosting","text":"<p>Make sure to activate your python interpreter!</p> <p>If you want to host your own flask app, gunicorn is a WSGI pure python server used for production. You'll need to <code>pip install gunicorn</code> and then run the commands.</p> <p>Running locally:</p> <p><code>gunicorn -w 4 'module_name:app_name'</code></p> <p><code>-w</code> is the number of workers, default is 1.</p> <p>If you want to be able to access the site on the network, you need to bind to 0.0.0.0:</p> <p><code>gunicorn -w 4 -b 0.0.0.0:8000 'app:app''</code></p> <p><code>0.0.0.0</code> binds the app to all available network interfaces, making it accessible on your network. This is a special ip address that tells your application to listen on all available network interfaces of the machine, instead of just localhost. </p> <p>To make this accessible from the internet, you need to configure your router to forward traffic that  goes to your chosen port (8000 in this case) to the machine that is running the app. This is done with port forwarding in your router admin settings. Visually it might look like  <code>public_ip:8000 -&gt; server_local_ip:8000</code>. BTW this is called port forwarding.</p> <p>You may need to update firewall config on the server to allow external connections  on port 8000 (linux <code>sudo ufw allow 8000</code>). </p> <p>If your ISP changes your public IP, you make need to use a dynamic DNS (DDNS) service like No-IP or DynDNS.</p> <p>Security Warning: exposing your app to the internet comes with security risks. To mitigate:</p> <ol> <li>Use HTTPS: Serve your app behind a reverse proxy like nginx with an SSL certificate.</li> <li>Restrict Access: Limit access to specific IPs or use authentication to secure your app.</li> <li>Monitor Logs: Monitor your server logs for unusual activity.</li> </ol> <p>Stopping the app:</p> <p>Use <code>pgrep -fl gunicorn</code> to show a list of gunicorn processes with their PIDs.</p> <p>kill by bid: <code>kill &lt;pid&gt;</code> (kill by process ID)</p> <p>OR</p> <p>kill all gunicorn: <code>pkill gunicorn</code> (kill by process name)</p> <p>For a more stable production environment, a process manager is better.</p> <p>Create a systemd service file.</p> <p>Then you can enable/disable/start/stop the app as a service.</p> <pre><code>sudo systemctl enable gunicorn  # auto start on boot\nsudo systemctl start gunicorn  # prevent auto start on boot\nsudo systemctl status gunicorn\nsudo systemctl stop gunicorn\nsudo systemctl restart &lt;service-name&gt;\n</code></pre> <p>When you make changes to a service file, you need to reload the service:</p> <p><code>sudo systemctl daemon-reload</code></p> <p>Flask Docs</p>"},{"location":"Python/docs_7_important_libraries/#fastapi","title":"FastAPI","text":"<p>FastAPI is one of the most performant web frameworks available for python.</p>"},{"location":"Python/docs_7_important_libraries/#pyinstaller","title":"Pyinstaller","text":"<p>Pyinstaller is a great way to create executable files. This makes your python program run without needing python on the host machine. Great for sharing a python program with others who do not have python on their computer.</p> <pre><code># install\npip install pyinstaller\n\n# create executable in one file.\npyinstaller --onefile script_name.py\n\n# there are several flags that can be used\npyinstaller --onefile --noconsole --icon=your_icon.ico --add-data \"config.json;.\" script_name.py\n</code></pre> <p>Pyinstaller will capture other .py files if you have more than one and they are imported.</p>"},{"location":"Python/docs_7_important_libraries/#other-interesting-libraries","title":"Other Interesting Libraries","text":""},{"location":"Python/docs_7_important_libraries/#locust","title":"Locust","text":"<p>Locust is a load testing framework that can send millions of requests.</p> <p>Locust Docs</p>"},{"location":"Python/docs_7_important_libraries/#reportlab","title":"Reportlab","text":"<p>Creating PDF's can be difficult or require external software like LaTeX or wkhtmltopdf.</p> <p>If you don't want to have these dependencies, reportlab is a pure python library to create PDFs.</p> <p><code>pip install reportlab</code></p> <p>ReportLab includes a low-level API for generating PDF documents directly from Python,  and a higher-level template language\u2014similar to HTML and the template systems used in  web development\u2014called RML. Generally, the second option is usually more convenient  for those who must make exhaustive use of the capabilities of the library when  generating documents. For the rest of the cases, the low-level API that we will  describe in this article will suffice. However, you can find the official  documentation here https://docs.reportlab.com/.</p> <pre><code>from reportlab.pdfgen import canvas\n\nc = canvas.Canvas(\"hello-world.pdf\")\nc.save()\n</code></pre> <pre><code># import the canvas object\nfrom reportlab.pdfgen import canvas\n\n# create a Canvas object with a filename\nc = canvas.Canvas(\"rl-hello_again.pdf\", pagesize=(595.27, 841.89))  # A4 pagesize\n# draw a string at x=100, y=800 points\n# point ~ standard desktop publishing (72 DPI)\n# coordinate system:\n#   y\n#   |\n#   |   page\n#   |\n#   |\n#   0-------x\nc.drawString(50, 780, \"Hello Again\")\n# finish page\nc.showPage()\n# construct and save file to .pdf\nc.save()\n</code></pre> <p>Use standard page sizes like letter, A4, and more.</p> <pre><code>from reportlab.lib.pagesizes import letter\n\nc = canvas.Canvas(\"hello-world.pdf\", pagesize=letter)\n</code></pre> <p>Make shapes in PDF.</p> <pre><code>from reportlab.lib.pagesizes import A4\nfrom reportlab.pdfgen import canvas\n\nw, h = A4\nc = canvas.Canvas(\"shapes.pdf\", pagesize=A4)\nc.drawString(30, h - 50, \"Line\")\nx = 120\ny = h - 45\nc.line(x, y, x + 100, y)\nc.drawString(30, h - 100, \"Rectangle\")\nc.rect(x, h - 120, 100, 50)\nc.drawString(30, h - 170, \"Circle\")\nc.circle(170, h - 165, 20)\nc.drawString(30, h - 240, \"Ellipse\")\nc.ellipse(x, y - 170, x + 100, y - 220)\nc.showPage()\nc.save()\n</code></pre> <p>You can make tables from a pandas dataframe</p> <pre><code>from reportlab.lib.pagesizes import letter, landscape\nfrom reportlab.platypus import SimpleDocTemplate, Table, TableStyle\nfrom reportlab.lib import colors\n\nfilename = \"landscape_table_fit.pdf\"\ndoc = SimpleDocTemplate(filename, pagesize=landscape(letter))\n\n# Example DataFrame\nimport pandas as pd\ndf = pd.DataFrame({\n    \"Column 1\": [\"A\", \"B\", \"C\"],\n    \"Column 2\": [1, 2, 3],\n    \"Column 3\": [4.5, 5.5, 6.5],\n    \"Column 4\": [\"Long Text Example\", \"Another Example\", \"More Text\"],\n    \"Column 5\": [\"X\", \"Y\", \"Z\"]\n})\n\n# Convert DataFrame to list of lists\ndata = [df.columns.tolist()] + df.values.tolist()\n\n# Calculate dynamic column widths to fit the page\npage_width = landscape(letter)[0]\navailable_width = page_width - 40  # Subtract margins\nnum_columns = len(df.columns)\ncol_width = available_width / num_columns\n\n# Create a table with fixed column widths\ntable = Table(data, colWidths=[col_width] * num_columns)\ntable.setStyle(TableStyle([\n    ('BACKGROUND', (0, 0), (-1, 0), colors.lightgrey),\n    ('TEXTCOLOR', (0, 0), (-1, 0), colors.black),\n    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n    ('GRID', (0, 0), (-1, -1), 1, colors.grey),\n]))\n\n# Add the table to the elements list\nelements = [table]\n\n# Build the document\ndoc.build(elements)\n\nprint(f\"PDF saved as {filename}\")\n</code></pre>"},{"location":"Python/docs_8_documentation/","title":"Documentation Tools","text":""},{"location":"Python/docs_8_documentation/#sphinx","title":"Sphinx","text":""},{"location":"Python/docs_8_documentation/#mermaid","title":"Mermaid","text":""},{"location":"Python/docs_8_documentation/#mkdocs","title":"MKDocs","text":""},{"location":"Python/docs_8_documentation/#_1","title":"Documentation Libraries","text":""},{"location":"Python/docs_9_std_lib/","title":"Notes on Standard Library","text":""},{"location":"Python/docs_9_std_lib/#io-package","title":"IO package","text":"<p>https://docs.python.org/3/library/io.html</p> <p>The io module provides Python\u2019s main facilities for dealing with various types of I/O.  There are three main types of I/O: text I/O, binary I/O and raw I/O.  These are generic categories, and various backing stores can be used for each of them.  A concrete object belonging to any of these categories is called a file object.  Other common terms are stream and file-like object.</p> <p>Independent of its category, each concrete stream object will also have various  capabilities: it can be read-only, write-only, or read-write. It can also allow  arbitrary random access (seeking forwards or backwards to any location), or  only sequential access (for example in the case of a socket or pipe).</p>"},{"location":"Python/docs_9_std_lib/#text-io","title":"Text I/O","text":"<p>Text I/O expects and produces str objects. This means that whenever the backing store  is natively made of bytes (such as in the case of a file), encoding and decoding of  data is made transparently as well as optional translation of platform-specific newline characters.</p> <p>The easiest way to create a text stream is with open(), optionally specifying an encoding:</p> <p><code>f = open(\"myfile.txt\", \"r\", encoding=\"utf-8\")</code></p> <p>In-memory text streams are also available as StringIO objects:</p> <p><code>f = io.StringIO(\"some initial text data\")</code></p> <p>StringIO is useful when you need to read or write data to a string buffer as  if it were a file, rather than creating an actual file on disk. It can be  used to create strings that mimic file objects, allowing you to read and  write data to them in the same way you would with a file.</p> <p></p>"},{"location":"Python/docs_9_std_lib/#binary-io","title":"Binary I/O","text":"<p>Binary I/O (also called buffered I/O) expects bytes-like objects and produces bytes objects.  No encoding, decoding, or newline translation is performed. This category of streams can  be used for all kinds of non-text data, and also when manual control over  the handling of text data is desired.</p> <p>The easiest way to create a binary stream is with open() with 'b' in the mode string:</p> <p><code>f = open(\"myfile.jpg\", \"rb\")</code></p> <p>In-memory binary streams are also available as BytesIO objects:</p> <p><code>f = io.BytesIO(b\"some initial binary data: \\x00\\x01\")</code></p>"},{"location":"Python/docs_9_std_lib/#smtp-sending-emails","title":"SMTP - Sending Emails","text":"<p>You can send emails directly through python using the simple mail transmission protocol.</p> <pre><code>import smtplib\nfrom email.message import EmailMessage\n\nmsg = EmailMessage()\nmsg['From'] = 'email@address.com'\nmsg['To'] = 'recipient@address.com'\nmsg['Subject'] = 'This is an Email'\n\nmsg.set_content('Body of email here.')\n\nmailserver = smtplib.SMTP('smtp.office365.com', 587)  # connect to email service\nmailserver.starttls()  # starts TLS (security protocol that encrypts data sent over the internet\nmailserver.login(msg['From'], 'password')  # login to the email service account\nmailserver.sendmail(msg['From'], msg['To'], msg.as_string())  # send the email\nmailserver.quit()\n</code></pre> <p>Mail with attachment</p> <pre><code>import os\nimport smtplib\nfrom email.message import EmailMessage\n\nmsg = EmailMessage()\nmsg['From'] = 'email@address.com'\nmsg['To'] = 'recipient@address.com'\nmsg['Subject'] = 'This is an Email'\n\nmsg.set_content('Body of email here.')\n\nfile = 'path/to/file.txt'\nextension = os.path.basename(file).split('.')[1]\nwith open(file, 'rb') as f:\n    file_data = f.read()\nmsg.add_attachment(file_data, maintype='application', subtype=extension, filename=os.path.basename(file))\n\nmailserver = smtplib.SMTP('smtp.office365.com', 587)  # connect to email service\nmailserver.starttls()  # starts TLS (security protocol that encrypts data sent over the internet\nmailserver.login(msg['From'], 'password')  # login to the email service account\nmailserver.sendmail(msg['From'], msg['To'], msg.as_string())  # send the email\nmailserver.quit()\n</code></pre> <p>You can also send nicely formatted HTML emails with MIME</p> <pre><code>import smtplib\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\n\nmsg = MIMEMultipart('mixed')\nmsg['From'] = 'email@address.com'\nmsg['To'] = 'recipient@address.com'\nmsg['Subject'] = 'Subject Here'\n\nhtml_body = MIMEMultipart('alternative')\n\nhtml_body_text = MIMEText('&lt;html&gt;&lt;div&gt;HTML Content here&lt;/div&gt;&lt;/html&gt;', 'html')\nhtml_body.attach(html_body_text)\n\n# Attach the multipart/alternative part to the multipart/mixed message\nmsg.attach(html_body)\n\n# Remaining code for sending the email\nmailserver = smtplib.SMTP('smtp.office365.com', 587)\nmailserver.starttls()\nmailserver.login(msg['From'], 'password here')\nmailserver.send_message(msg)\nmailserver.quit()\n</code></pre>"},{"location":"Python/docs_9_std_lib/#python-http-server","title":"Python HTTP Server","text":"<p>Python has an HTTP server built into the standard library.</p> <p>Serving static files: <code>python -m http.server 8008</code> this will look for an index.html file to start serving, and can be accessed on <code>localhost:8008</code>.</p>"},{"location":"Python/docs_9_std_lib/#python-on-ios-and-android","title":"Python on iOS and Android","text":"<p>This info comes from this article Medium Article</p> <p>Python 3.13 has brought official support for iOS and Android as a platform.  On iOS, since Apple restricts being able to install system resources or run binaries,  developers are forced to run python in embedded mode. In other words, writing native iOS applications and embedding a python interpreter using <code>libPython</code>. This bundles the python interpreter and your code into a bundle that gets uploaded to the App store.</p> <p>Here is the offical docs https://docs.python.org/3/using/ios.html</p>"},{"location":"Python/python_uv/","title":"UV - Modern Python Packaging/Build/Publisher","text":"<p>https://docs.astral.sh/uv/</p> <p>uv is the future of python package management.</p>"},{"location":"Python/python_uv/#commands","title":"Commands","text":"<pre><code># start a new uv project in current dir\nuv init\n\n# setup your python venv to match your pyproject.toml file\nuv sync\n\n# install package and add it to pyproject.toml\nuv add requests\n\n# install package and add it to dev dependencies of pyproject.toml file\nuv add --dev requests \n</code></pre>"},{"location":"Python/testing/","title":"Testing","text":"<p>Testing is a powerful way to catch any errors in your code, with code. Instead of  trying to run your code and catch errors, testing libraries can help catch things faster.</p>"},{"location":"Python/testing/#pytest","title":"pytest","text":"<p><code>pytest</code> is a 3rd party library, built on the standard library <code>unittest</code>.</p>"},{"location":"Python/testing/#install","title":"Install","text":"<p><code>pip install pytest</code> or <code>uv add pytest</code></p>"},{"location":"Python/testing/#quickstart","title":"Quickstart","text":"<p>In your project create a folder called <code>test</code>. Add a test module for each  module you want to test. Make sure to include the word \"test\" before or after the name, like below.</p> <pre><code>- project\n  - src\n    - adapters\n    - app\n      - my_module.py\n      - utils\n      - tests\n        - test_my_module.py\n  - pyproject.toml\n</code></pre> <p>Inside the module you can add functions or classes/methods that do the testing. Here is a simple example:</p> <pre><code># my_module.py\n\ndef add_numbers(*args):\n    return sum(args)\n</code></pre> <pre><code># test_my_module.py\n\nfrom my_module import add_numbers\n\ndef test_add_numbers():\n    assert add_numbers(1, 2) == 3\n</code></pre> <p>Once your tests are built, run <code>pytest</code>. Pytest will search recursively for:</p> <ul> <li>All modules named <code>test_*.py</code> or <code>*_test.py</code> and then inside they look for...</li> <li>functions that start with <code>test_</code></li> <li>Classes with Test (that don't have an init)</li> <li>Methods inside those classes that start with <code>test_</code></li> </ul> <p>Then run those tests.</p>"},{"location":"Python/testing/#strategy","title":"Strategy","text":"<p>It's best practice to write tests and run as you go. If you add new logic, create a test for it. Otherwise you'll accumulate too much technical debt and it will be a big job to  write tests for everything. Higher chance of missing something. </p>"},{"location":"Python/testing/#commands","title":"Commands","text":"<pre><code># run all tests\npytest\n\n# run single test file\npytest tests/test_my_module.py\n\n# run single test function\npytests tests/test_my_module.py::test_add_numbers\n</code></pre>"},{"location":"Rust/rust01/","title":"Rust Notes","text":""},{"location":"Rust/rust01/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Cargo</li> <li>Running a Program - Detailed</li> <li>Rust Basics<ul> <li>Data Types</li> <li>Variables</li> <li>References</li> <li>Shadowing</li> <li>Scope</li> <li>Statements &amp; Expressions</li> <li>Functions</li> <li>Returning from Function</li> <li>If Expressions</li> <li>Loops</li> <li>Comments</li> </ul> </li> <li>Ownership<ul> <li>Ownership Rules</li> <li>References, Pointers, and Borrowing</li> <li>Moving Variables</li> <li>Copying Variables</li> <li>Dangling References</li> <li>Stack and Heap</li> </ul> </li> <li>Structs</li> <li>Enums</li> <li>Managing Projects</li> <li>Collections</li> <li>Error Handling</li> <li>Generics</li> </ol> <p>Official Docs: https://doc.rust-lang.org/book/ch01-03-hello-cargo.html</p>"},{"location":"Rust/rust01/#1-cargo","title":"1. Cargo","text":"<p>Cargo is Rusts build system and package manager.</p> <p>A rust project will have the following files: * src/     * main.rs * Cargo.toml * .gitignore * .git</p> <p>Create a project in new directory: <code>cargo new project_name</code> \\ Create a project in existing directory: <code>cargo init</code></p> <p>Cargo.toml is the config file for your project and looks like this:</p> <pre><code>[package]  # SECTION HEADING\nname = \"rust_learning\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html\n\n[dependencies]  # PACKAGE DEPENDENCIES - KNOWN AS CRATES\n</code></pre>"},{"location":"Rust/rust01/#file-structure","title":"File Structure","text":"<p>Cargo expects your code to live in the src folder. The top-level project directory should only be used for README files, licenses, config files, and other things not directly related to your code.</p>"},{"location":"Rust/rust01/#create-and-run-w-cargo","title":"Create and Run w/ Cargo:","text":"<ol> <li>Create a rust project <pre><code>cargo new project_name\ncd project_name\n</code></pre></li> <li>Update main.rs files in src folder. This the source code of your project.</li> <li>Compile and run <code>cargo run</code></li> <li>To build non-debug file <code>cargo build --release</code></li> </ol> <p>Back to Top</p>"},{"location":"Rust/rust01/#2-running-a-program-detailed","title":"2. Running a Program - Detailed","text":""},{"location":"Rust/rust01/#build-process","title":"Build Process","text":"<p>When your project was created with cargo, compile the project with: <code>cargo build</code>. The default build made is debug, so cargo puts your file .exe file into the following directory <code>./target/debug/&lt;filename&gt;.exe</code></p> <p>But this is not very efficient.  Instead of compiling and running in two steps, we can do both with: <code>cargo run</code></p>"},{"location":"Rust/rust01/#compile-check","title":"Compile Check","text":"<p>You can also check if your code will compile, but not creating an .exe file, by running: <code>cargo check</code></p>"},{"location":"Rust/rust01/#build-profiles-dev-and-release","title":"Build Profiles, Dev and Release","text":"<p>When your project is finally ready for release, you can use <code>cargo build --release</code> to compile it with optimizations. This command will create an executable in <code>target/release</code> instead of <code>target/debug</code>. The optimizations make your Rust code run faster, but turning them on lengthens the time it takes for your program to compile. This is why there are two different profiles: 1. Development, when you want to rebuild quickly and often 2. Building the final program you\u2019ll give to a user that won\u2019t be rebuilt repeatedly and that will run as fast as possible.</p>"},{"location":"Rust/rust01/#benchmarking","title":"Benchmarking","text":"<p>If you\u2019re benchmarking your code\u2019s execution time, be sure to run <code>cargo build --release</code> and benchmark with the executable in <code>target/release</code>.</p>"},{"location":"Rust/rust01/#cargolock","title":"Cargo.lock","text":"<p>Running cargo build for the first time also causes Cargo to create a new file at the top level: Cargo.lock. This file keeps track of the exact versions of dependencies in your project. This project doesn\u2019t have dependencies, so the file is a bit sparse. You won\u2019t ever need to change this file manually; Cargo manages its contents for you.</p> <p>From https://doc.rust-lang.org/book/ch01-03-hello-cargo.html?search=</p>"},{"location":"Rust/rust01/#running-with-rustc","title":"Running with <code>rustc</code>","text":"<p><code>rustc</code> is the basic rust compiler.  It is not recommended to use this for building projects, but it is useful for testing code.</p> <p><code>rustc filename.rs</code> will create the .exe file. You can then run the <code>./filename.exe</code> file, or you can run a command in the terminal. On Windows, when the file is compiled, there is a .pdb file created which is debugging information.</p> <p>Back to Top</p>"},{"location":"Rust/rust01/#3-rust-language-basics","title":"3. Rust Language Basics","text":""},{"location":"Rust/rust01/#data-types","title":"Data Types","text":"<p>Rust is a statically typed language, which means that it must know the types of all variables at compile time.</p> <ul> <li> <p>Scalar Types - single value types</p> <ul> <li>Integers<ul> <li>Signed (has a negative and positive)</li> <li>i8, i16, i32, i64, i128, isize</li> </ul> </li> <li>Unsigned (only positive)<ul> <li>u8, u16, u32, u64, u128, usize</li> </ul> </li> <li>Floating-Point Numbers<ul> <li>f32, f64 with f64 being the default. Always signed.</li> </ul> </li> <li>Booleans<ul> <li>bool (true, false)</li> </ul> </li> <li>Characters<ul> <li>char (single quotes)   <code>rust fn main() {     let c = 'z';     let z: char = '\u2124'; // with explicit type annotation     let heart_eyed_cat = '\ud83d\ude3b'; }</code></li> </ul> </li> </ul> </li> <li> <p>Compound Types - grouped values</p> <ul> <li>Tuples<ul> <li>Fixed length, non-mutable, can contain different types.</li> <li>Parenthesis <code>let tup = (500, 6.4, 1);</code></li> <li>Can destructure a tuple: <code>let (x, y, z) = tup;</code></li> <li>Can access a tuple by index:   <pre><code>fn main() {\n    let x = (500, 6.4, 1);\n    let five_hundred = x.0;\n    let six_point_four = x.1;\n    let one = x.2;\n    }\n</code></pre></li> </ul> </li> <li>Arrays<ul> <li>Fixed length, non-mutable, must contain same type.</li> <li>Square brackets <code>let a = [1, 2, 3, 4, 5];</code></li> <li>Can access an array by index: <code>let first = a[0];</code></li> <li>Arrays are useful when you want your data allocated on the stack rather than the heap. or when you want to ensure you always have a fixed number of elements</li> <li>An array isn\u2019t as flexible as the vector type, though. A vector is a similar collection type provided by the standard library that is allowed to grow or shrink in size. If you\u2019re unsure whether to use an array or a vector, chances are you should use a vector.</li> </ul> </li> </ul> </li> </ul>"},{"location":"Rust/rust01/#variables","title":"Variables","text":"<p>Variables are created by using the let keyword: <code>let x = 5;</code> Variables are immutable by default.  To make a variable mutable, use the mut keyword: <code>let mut x = 5;</code></p>"},{"location":"Rust/rust01/#references","title":"References","text":"<p>Using a reference: <code>read_line(&amp;mut x)</code>. The <code>&amp;</code> before a variable name indicates that it\u2019s a reference. This gives you a way to access one piece of data without needing to copy that data into memory multiple times. References are also immutable by default.</p>"},{"location":"Rust/rust01/#shadowing","title":"Shadowing","text":"<p>Overwriting a variable instead of making a new one.</p>"},{"location":"Rust/rust01/#scope","title":"Scope","text":"<p>Scope is the area of code where a variable is valid.  This is usually expressed within curly braces: <pre><code>fn main() {\n    // Outer scope\n    let outer_variable = 5; // x is valid from here until the end of the function\n    {\n        // Inner scope\n        let inner_variable = 6; // y is valid from here until the end of the inner scope\n        println!(\"inner_variable: {}\", inner_variable);\n\n        // You can access outer variables from inner scopes\n        println!(\"outer_variable: {}\", outer_variable);\n    }\n\n    // inner_variable is not valid here\n    // we can still access outer_variable here\n    println!(\"outer_variable: {}\", outer_variable);\n}\n</code></pre></p>"},{"location":"Rust/rust01/#statements-expressions","title":"Statements &amp; Expressions","text":"<ul> <li>Statements are instructions that perform some action and do not return a value.<ul> <li>Functions</li> <li>Setting variables</li> <li>Loops</li> <li>Note: statements in rust must end with a semicolon.</li> </ul> </li> <li>Expressions evaluate to a resultant value. Expressions do not end in semicolon.<ul> <li>5</li> <li>10+6</li> <li>Calling a function that returns</li> <li>Calling a macro</li> <li>If expressions</li> <li>Note: expressions can be part of statements (let x = 6; 6 is an expression)</li> <li>Note: expressions that are followed by a semicolon are treated as statements</li> </ul> </li> </ul>"},{"location":"Rust/rust01/#functions","title":"Functions","text":"<p><pre><code>fn main() {\n    println!(\"Hello, world!\");\n}\n</code></pre> Functions are a central design of rust. The main function will always be the first code to run.</p>"},{"location":"Rust/rust01/#returning-from-function","title":"Returning from Function","text":"<p><pre><code>fn repeat_value(i: i32) -&gt; i32 {\n    i\n}\n</code></pre> To return a value from a function, you need to have the arrow and return type. The last will be what a function returns if it has a return type. You can also return early with the return keyword <pre><code>fn repeat_value(i: i32) -&gt; i32 {\n    if i &gt; 5 {\n        return i+5;\n    }\n    i\n}\n</code></pre></p> <p>If a function will not return anything you can exclude the arrow and return type. <pre><code>fn just_print(i: i32) {\n    println!(\"i: {}\", i);\n}\n</code></pre></p>"},{"location":"Rust/rust01/#if-expressions","title":"If Expressions","text":"<p>Standard if expressions, pretty similar to python.  Be sure to put the contents in the curly braces. <pre><code>fn if_control(i: i32) -&gt; i32 {\n    if i &gt; 5 {\n        i+5\n    } else if x &lt; 0 {\n        i-5\n    }\n    else {\n        i\n    }\n}\n</code></pre></p> <p>If statements can also be used on the right side of a variable. This is because if statements evaluate to a value and are actually called if expressions</p> <pre><code>fn main() {\n    let condition = true;\n    let number = if condition {\n        5\n    } else {\n        6\n    };\n    println!(\"The value of number is: {}\", number);\n}\n</code></pre>"},{"location":"Rust/rust01/#loops","title":"Loops","text":"<p>There are 3 kinds of loops: 1. Loop     1. Infinite loop - continuously execute something until a condition is met and then either use the continue or break keyword.    <pre><code>fn main() {\n    loop {\n        println!(\"again!\");\n    }\n</code></pre>    You can return values from a loop by using break.     <pre><code>fn main() {\n    let mut counter = 0;\n    let result = loop {\n        counter += 1;\n        if counter == 10 {\n            break counter * 2;\n        }\n    };\n    println!(\"The result is {}\", result);\n}\n</code></pre></p> <p>Loop labels help with control flow of multiple loops. The default for break is to break the inner most loop. You can    use loop labels to break out of higher loops within inner loops     <pre><code>fn main() {\n    let mut count = 0;\n    let remaining = 10;\n    'outer_loop: loop {\n        println!(\"Count = {}\", count);\n        loop {\n            count += 1;\n            println!(\"Loop {} of {}\", count, remaining);\n            if count == remaining {\n                break;\n            } else if count == 100 {\n                break 'outer_loop;\n            }\n        }\n    }\n    println!(\"End of program\");\n}\n</code></pre></p> <ol> <li> <p>While - Good for looping until a condition is met.     <pre><code>fn main() {\n    let mut number = 3;\n    while number != 0 {\n        println!(\"{}!\", number);\n        number -= 1;\n    }\n    println!(\"LIFTOFF!!!\");\n}\n</code></pre></p> </li> <li> <p>For - Good for looping through a collection     <pre><code>fn main() {\n    let a = [10, 20, 30, 40, 50];\n    for element in a.iter() {\n        println!(\"the value is: {}\", element);\n    }\n}\n</code></pre></p> </li> </ol>"},{"location":"Rust/rust01/#comments","title":"Comments","text":"<p><code>//</code> just like javascript</p> <p>Back to Top</p>"},{"location":"Rust/rust01/#4-ownership","title":"4. Ownership","text":"<p>Three rules of ownership: 1. Each value in Rust has a variable that\u2019s called its owner. 2. There can only be one owner at a time. 3. When the owner goes out of scope, the value will be dropped.</p> <p>Ownership is Rust\u2019s most unique feature, and it enables Rust to make memory safety guarantees without needing a garbage collector.</p>"},{"location":"Rust/rust01/#ownership-behaviors","title":"Ownership Behaviors","text":"<ul> <li>Ownership can be moved or copied.<ul> <li>If data is on the stack, it will be copied.  If on the heap, it will be moved.</li> <li>Copying is cheap, moving is expensive.</li> </ul> </li> <li>If a variable goes out of scope, it gets dropped automatically which frees up the memory.</li> <li>When passing a variable into a function, ownership of the variable goes to the function.</li> <li>If the variable is not on the stack, it will not exist outside of that function it was passed into, unless it returns a value. <pre><code>fn main() {\n    let s = String::from(\"hello\");  // s comes into scope\n\n    takes_ownership(s);             // s's value moves into the function...\n                                    // ... and so is no longer valid here\n\n    let x = 5;                      // x comes into scope\n\n    makes_copy(x);                  // x would move into the function,\n                                    // but i32 is Copy, so it\u2019s okay to still\n                                    // use x afterward\n    }\n</code></pre></li> </ul>"},{"location":"Rust/rust01/#references-and-pointers","title":"References and Pointers","text":"<p>There are two types of pointers in rust: 1. References 2. Raw pointers</p> <p>A reference is like a pointer in that its an address we can follow to access the data stored at that address, without taking ownership of the variable.</p> <p>References are a safe way to access memory and are used by the <code>&amp;</code> character:</p> <pre><code>let x = 42;\nlet y = &amp;x;  // y is a reference to x\n\nprintln!(\"The value of x is {}\", x);\nprintln!(\"The value of y is {}\", y);\n</code></pre> <p>Raw pointers have no safety guarantees and are used by the <code>*</code> <pre><code>let x = 42;\nlet y = &amp;x as *const i32;  // y is a raw pointer to x\n\nunsafe {\n    println!(\"The value of x is {}\", *y);  // this will print the value of x\n}\n</code></pre></p> <p>References are created by using the <code>&amp;</code>, creating a reference is called borrowing. Borrowing allows you to pass a variable into a function without taking ownership of it.</p> <pre><code>fn main(){\n    let s1 = String::from(\"hello\");\n    let len = calculate_length(&amp;s1);\n    println!(\"The length of '{}' is {}.\", s1, len);\n}\n\nfn calculate_length(s: &amp;String) -&gt; usize {\n    s.len()\n}\n</code></pre> <p>Just as when you borrow something from a person, you have to give it back the same way you received it.  This means you can't modify it when borrowing AKA REFERENCES ARE IMMUTABLE BY DEFAULT, JUST LIKE VARIABLES</p> <p>To make a reference mutable looks very similar to variables: <pre><code>fn main(){\n    let mut s = String::from(\"hello\");\n    change(&amp;mut s);\n}\n\nfn change(some_string: &amp;mut String) {\n    some_string.push_str(\", world\");\n}\n</code></pre></p> <p>NOTE: If you have a mutable reference to a value, there can be no other simultaneous references to that value.  This prevents issues with data races.</p> <p>Note that a reference\u2019s scope starts from where it is introduced and continues through the last time that reference is used. For instance, this code will compile because the last usage of the immutable references, the println!, occurs before the mutable reference is introduced:</p> <pre><code>fn main() {\n    let mut s = String::from(\"hello\");\n\n    let r1 = &amp;s; // no problem\n    let r2 = &amp;s; // no problem\n    println!(\"{} and {}\", r1, r2);\n    // r1 and r2 are no longer used after this point\n\n    let r3 = &amp;mut s; // no problem\n    println!(\"{}\", r3);\n}\n</code></pre> <p>The example above is ok because by the time r3 (the mutable referece) is created, the immutable references are out of scope.  They drop after their final usage.</p>"},{"location":"Rust/rust01/#moving-variables","title":"Moving Variables","text":"<p>When setting a variable equal to a new variable, Rust invalidates the first variable. This is to prevent double free errors which are when the ownership model tries to drop one of the variables but not the other that is also referencing the same memory.</p> <p>s1 becomes invalid when s2 is assigned to s1.  This is because s1 and s2 are both pointing to the same memory address.  When s2 is assigned to s1, s1 is no longer valid because it is no longer the only owner of the memory address.</p>"},{"location":"Rust/rust01/#stack-data-copy","title":"Stack Data - Copy","text":"<p>Data that is stored on the stack is so efficient that its not a problem to manage.  As a convience, Rust is not afraid to duplicate data that\u2019s stored on the stack.  In other words, references on the stack that point to data on the heap need much more management which is why there are more rules and limiations to follow. Like moving variables,</p> <p>For stack data, there's no differences between shallow and deep copys because of something called the copy trait.  Variables that hold data with types that have the copy trait, are copied instead of moved so they are still valid after the copy.</p> <p>Here are some of the types that implement Copy: * All the integer types, such as u32. * The Boolean type, bool, with values true and false. * All the floating-point types, such as f64. * The character type, char. * Tuples, if they only contain types that also implement Copy. For example, (i32, i32) implements Copy, but (i32, String) does no</p> <p>How does this work with functions?</p> <p>Ownership is passed through functions using this same concept of ownership.  If the variable is a type that has the copy trait, the variable is still valid after its passed. Otherwise ownership is passed into the function and then no goes out of scope once the function ends.</p> <pre><code>fn main() {\n    let s = String::from(\"hello\");  // s comes into scope\n\n    takes_ownership(s);             // s's value moves into the function...\n                                    // ... and so is no longer valid here\n\n    let x = 5;                      // x comes into scope\n\n    makes_copy(x);                  // x would move into the function,\n                                    // but i32 is Copy, so it\u2019s okay to still\n                                    // use x afterward\n    }\n\nfn takes_ownership(some_string: String) { // some_string comes into scope\n    println!(\"{}\", some_string);\n} // Here, some_string goes out of scope and `drop` is called. The backing\n  // memory is freed.\n\nfn makes_copy(some_integer: i32) { // some_integer comes into scope\n    println!(\"{}\", some_integer);\n} // Here, some_integer goes out of scope. Nothing special happens.\n</code></pre> <p>But what if you want a function to use a value but not take ownership of it.  Its annoying to have to constantly return the variable to use it again.</p>"},{"location":"Rust/rust01/#dangling-references","title":"Dangling References","text":"<p>Dangling references are when a variable is created and a reference to that variable created, and the variable goes out of scope but the program tries to keep the reference alive.</p> <p><pre><code>fn dangle() -&gt; &amp;String { // dangle returns a reference to a String\n    let s = String::from(\"hello\"); // s is a new String\n\n    &amp;s // we return a reference to the String, s\n} // Here, s goes out of scope, and is dropped. Its memory goes away.\n  // Danger!\n</code></pre> The variable <code>s</code> will go out of scope at the end of the function above but its trying to return the reference, which will reference something that no longer exits.  DANGER!  Rust will throw an error to catch these.</p>"},{"location":"Rust/rust01/#side-note-stack-and-heap","title":"Side Note: Stack and Heap","text":"<p>About memory allocation</p> <ul> <li>Data is stored in either the stack or the heap when in memory.</li> <li>The stack is more organized and quicker.  This holds data where the length is known and fixed.</li> <li>The heap is a less organized, less efficeint data store.  This holds data where the length is not known at runtime or might change.  This is because the program has to find a space for the memory but a stack just goes right on top every single time.</li> <li>Data that is on the stack is directly related.  So <code>let x = 6;</code> is actually connected to 6.  Data on the heap creates a pointer that points to the address of the data on the heap.  To access a piece of memory on the heap, you need to follow a pointer.  This means the program is bouncing around the memory to find what its looking for, instead of using the data that\u2019s right next to it already.</li> <li>Data on the stack can be copied but data on the heap can only be referenced (pointed).  Sort of like shallow copy and deep copy.</li> </ul> <p>If you want to \"deepcopy\" data on the heap, you need to call the method clone.  But keep in mind this is expensive.</p> <pre><code>let s1 = String::from(\"hello\");\nlet s2 = s1.clone();\n\nprintln!(\"s1 = {}, s2 = {}\", s1, s2);\n</code></pre> <p>But to \"deep copy\" data on the stack, its as simple as creating another variable <pre><code>fn main() {\n    let x = 5;\n    let y = x;\n\n    println!(\"x = {}, y = {}\", x, y);\n}\n</code></pre> Data on the stack is very efficient so this is fine.</p> <p>Back to Top</p>"},{"location":"Rust/rust01/#5-structs","title":"5. Structs","text":"<p>Structs are sort of like python classes. They are used to create custom data types, they must be declared before use, and they can be mutable or immutable. They are good for grouping related data together.</p> <pre><code>struct User {\n    username: String,\n    email: String,\n    sign_in_count: u64,\n    active: bool,\n}\n</code></pre> <p>To create an instance of a struct, you can use the curly braces and key value pairs:</p> <pre><code>let user1 = User {\n    email: String::from(\"user@web.com\"),\n    username: String::from(\"user1\"),\n    active: true,\n    sign_in_count: 1,\n};\n</code></pre> <p>To make mutable, use the mut keyword:</p> <pre><code>let mut user1 = User {\n    ..etc..\n</code></pre> <p>Accessing values in a struct is done with dot notation:</p> <pre><code>println!(\"user1 email: {}\", user1.email);\n</code></pre> <p>Structs can also have methods, which are created in the impl block:</p> <pre><code>struct Rectangle {\n    width: u32,\n    height: u32\n}\n\nimpl Rectangle {\n    fn area(&amp;self) -&gt; u32 {\n        self.width * self.height\n    }\n}\n</code></pre> <p>The first parameter of a method is always self.  Self refers to the instance of the struct that the method is being called on.  You can have multiple parameters in a method, but the first one must always be self.</p> <p>To call a method on a struct, use dot notation:</p> <pre><code>fn main() {\n    let rect1 = Rectangle {\n        width: 30,\n        height: 100\n    };\n    println!(\"The area of rect1 is {}\", rect1.area());\n}\n</code></pre> <p>Back to Top</p>"},{"location":"Rust/rust01/#6-enums","title":"6. Enums","text":"<p>Enumerations (or enums) are like structs but different in that they can only be one of a few values.  They are useful for when you want to limit the possible values of a variable.</p> <pre><code>enum IpAddrKind {\n    V4,\n    V6,\n}\n</code></pre> <p>To create an instance of an enum, use the double colon syntax:</p> <pre><code>let four = IpAddrKind::V4;\nlet six = IpAddrKind::V6;\n</code></pre> <p>Enums can also have data associated with them:</p> <pre><code>enum IpAddr {\n    V4(String),\n    V6(String),\n}\n</code></pre> <p>To create an instance of an enum with data:</p> <pre><code>let home = IpAddr::V4(String::from(\"127.0.0.1\"));\nlet loopback = IpAddr::V6(String::from(\"::1\"));\n</code></pre> <p>Enums can also have methods, which are created in the impl block:</p> <pre><code>enum IpAddr {\n    V4(String),\n    V6(String),\n}\n\nimpl IpAddr {\n    fn print(&amp;self) {\n        println!(\"ip address: {}\", self.0);\n    }\n}\n</code></pre> <p>To call a method on an enum, use dot notation:</p> <pre><code>fn main() {\n    let home = IpAddr::V4(String::from(\"127.0.0.1\"));\n    home.print();\n}\n</code></pre> <p>Enums can also have multiple types of data associated with them:</p> <pre><code>enum IpAddr {\n    V4(u8, u8, u8, u8),\n    V6(String),\n}\n</code></pre> <p>Once you create an enum, you can use it with pattern matching or the <code>let if</code> syntax. There are some tricks you can use with pattern matching such as <code>other =&gt; function(other)</code> to call a function with the value of other.</p> <p>More Here</p> <p><code>Option&lt;T&gt;</code> is a built-in enum that lets you easily evaluate whether a value is present or not.  It is useful for when you want to return a value or nothing at all.</p> <pre><code>enum Option&lt;T&gt; {\n    Some(T),\n    None,\n}\n</code></pre> <pre><code>fn divide(numerator: i32, denominator: i32) -&gt; Option&lt;i32&gt; {\n    if denominator == 0 {\n        None\n    } else {\n        Some(numerator / denominator)\n    }\n}\n\nfn main() {\n    let result = divide(10, 2);\n    match result {\n        Some(x) =&gt; println!(\"Result: {}\", x),\n        None =&gt; println!(\"Cannot divide by 0\"),\n    }\n}\n</code></pre> <p>Option will evaluate to either Some(T) or None, but not both! Once you have an Option, you will need to extract the T out to perform operations on it. Choices for extracting this data include pattern matching with <code>match</code>, <code>if let</code>, or Option methods such as <code>.unwrap()</code>, <code>.unwrap_or()</code>, <code>.map()</code>, and others. <pre><code>let result = divide(10, 0);\n// this will give 0 because the result is None and unwrap_or returns the value or the default given.\nprintln!(\"Result: {}\", result.unwrap_or(0));\n\n// this will panic because unwrap returns the value or panics if None\nprintln!(\"Result: {}\", result.unwrap());\n</code></pre> <p>Docs</p> <p>Back to Top</p>"},{"location":"Rust/rust01/#7-managing-projects","title":"7. Managing Projects","text":"<p>When building a project its important to organize the code for readability and re-usability.</p> <ul> <li>Packages: A Cargo feature that lets you build, test, and share crates</li> <li>Crates: A tree of modules that produces a library or executable</li> <li>Modules and use: Let you control the organization, scope, and privacy of paths</li> <li>Paths: A way of naming an item, such as a struct, function, or module</li> </ul> <p>Crate: Smallest amount of code the compiler considers at one time. A crate is either a 'binary crate' or a 'library crate'. * A binary crate must have a 'main' function, which is the starting point of the program. * A library crate doesn't need 'main', and its not compiled into an executable. Its meant to share functionality with other projects. Package: A package is a rust project that Cargo can manage. A grouping of one or more crates that does things, contains 'Cargo.toml'. Modules: A way to group code. A module can either be its own .rs file or a subgroup within a .rs file (called in-line module). Inline modules are setup similar to functions but use the <code>mod</code> keyword and can help control privacy. Modules are a good way to group related activities. For example, a car needs to accelerate, brake, steer, and each of these have different details to make them happen. Paths: Paths for modules use <code>::</code> ie <code>crate::front_of_house::hosting::add_to_waitlist();</code> this is like import for python. To import a module use the keyword <code>use</code> ie <code>use crate::front_of_house::hosting;</code> Modules are set to private by default, use <code>pub</code> to make them public.</p> <p>Back to Top</p>"},{"location":"Rust/rust01/#8-collections-data-structures","title":"8. Collections (data structures)","text":"<ol> <li>Vector (vec!)</li> <li>string</li> <li>Hash map</li> </ol> <p>Vectors detail:</p> <pre><code>// create a vector\nlet v: Vec&lt;i32&gt; = Vec::new();\nor\nlet v = vec![1,2,3];\n</code></pre> <p>Update a vector</p> <p><pre><code>let mut v = vec![1,2,3];\nv.push(4);\n</code></pre> Get elements from a vector with index or <code>get</code></p> <pre><code>let mut v = vec![1,2,3];\nprintln!(v[1]);\nprintln!(v.get(2));\n</code></pre> <p>Back to Top</p>"},{"location":"Rust/rust01/#9-error-handling","title":"9. Error Handling","text":"<p>There are 2 kinds of error handling: 1. panic! (unrecoverable) 2. Result (recoverable)</p> <p>How to use result:</p> <pre><code>use std::fs::File;\n\nfn main() {\n    let greeting_file_result = File::open(\"hello.txt\");\n\n    let greeting_file = match greeting_file_result {\n        Ok(file) =&gt; file,\n        Err(error) =&gt; panic!(\"Problem opening the file: {:?}\", error),\n    };\n}\n</code></pre> <p>The Result type returns either an Ok or an Err type.</p> <p>Back to Top</p>"},{"location":"Rust/rust01/#10-generic-types-traits-lifetimes","title":"10. Generic Types, Traits, Lifetimes","text":"<p>Generics: If you're creating a function or struct, anything with a data type, you can use generics to make that type variable.</p> <p>For example:</p> <pre><code>fn largest(list: &amp;[T]) -&gt; &amp;T {\n    let mut largest = &amp;list[0];\n\n    for item in list{\n        if item &gt; largest {\n            largest = item;\n        }\n    }\n    largest\n}\n</code></pre> <p>This function allows for any data type now.</p>"},{"location":"Video%20Processing/docs_200_ffmpeg/","title":"FFmpeg","text":"<p>ffmpeg is a video editing command tool. </p>"},{"location":"Video%20Processing/docs_200_ffmpeg/#how-a-video-works","title":"How a Video Works","text":"<ol> <li>A video file consists of a container (like MP4, MKV) and encoded video/audio streams inside.</li> <li>The codec is what encodes (compresses) the video/audio when saving it and decodes (decompresses) it when playing it.</li> </ol>"},{"location":"Video%20Processing/docs_200_ffmpeg/#converting-a-video-file-transcoding","title":"Converting a Video File (Transcoding)","text":"<p>If you want to convert a video file from one format to another (e.g., MKV to MP4 or H.264 to H.265),  you use transcoding software (like HandBrake, FFmpeg, or VLC). The process involves:</p> <ol> <li>Decoding (using the current codec).</li> <li>Re-encoding (using a new codec or settings).</li> <li>Packaging the video/audio into a new container format.</li> </ol> <p>For example:</p> <ul> <li>MKV (H.264 + AAC) \u2192 MP4 (H.264 + AAC) \u2192 Just a container change (remuxing, no transcoding needed).</li> <li>MP4 (H.264) \u2192 MP4 (H.265) \u2192 A full re-encoding is required.</li> </ul> <p>So while codecs are a part of the conversion process, they don't handle file conversion by themselves\u2014you need transcoding software for that.</p> <ul> <li> <p>Codec - software that compresses or decompresses media files (audio or video). Determines the encoding.  Important for compression, compatability, and streaming efficiency.</p> </li> <li> <p>Video Codecs - These compress video data to reduce file size while maintaining visual quality.</p> </li> <li> <p>H.264 (AVC) \u2013 Most common, widely supported, good balance of quality and size.</p> </li> <li>H.265 (HEVC) \u2013 More efficient than H.264 (better quality at smaller sizes), but requires more processing power.</li> <li>VP9 \u2013 Open-source alternative to H.265, widely used on YouTube.</li> <li>AV1 \u2013 Newer, even better compression than VP9 and H.265, but requires more CPU power to decode.</li> <li> <p>MPEG-2 \u2013 Older, used in DVDs and some broadcast TV.</p> </li> <li> <p>Audio Codecs - These compress audio data for storage and streaming.</p> </li> <li> <p>AAC \u2013 Common in MP4 files, better quality than MP3 at the same bitrate.</p> </li> <li>MP3 \u2013 Universal compatibility, but lower efficiency.</li> <li>Opus \u2013 High efficiency, used in VoIP and streaming.</li> <li>FLAC \u2013 Lossless, used for high-quality music storage.</li> <li>Dolby Digital (AC-3) / DTS \u2013 Used in surround sound systems.</li> </ul>"},{"location":"Video%20Processing/docs_200_ffmpeg/#codec-vs-container-mkv-mp4","title":"Codec vs. Container (MKV, MP4)","text":"<ul> <li>Container formats (like MKV, MP4, AVI, MOV) hold video, audio, and subtitle tracks.</li> <li>Codecs determine how the actual video and audio inside the container are compressed and played.</li> <li>Example: An MP4 file can contain H.264 video + AAC audio.</li> </ul> <p>So, when choosing a video format, you need to consider both the container (MKV, MP4, etc.) and the codec (H.264, H.265, etc.).</p>"},{"location":"Video%20Processing/docs_200_streaming/","title":"Streaming","text":"<p>The \"best\" streaming protocol depends on the use case. Here\u2019s a breakdown of what\u2019s ideal for different scenarios:</p>"},{"location":"Video%20Processing/docs_200_streaming/#best-streaming-protocols-by-use-case","title":"Best Streaming Protocols by Use Case","text":"<p>On-Demand Video Streaming (e.g., Netflix, YouTube)</p> <ul> <li>Best: DASH (MPEG-DASH) or HLS</li> <li>Why? Adaptive bitrate streaming (ABR) ensures smooth playback across different devices and network conditions.</li> <li>Used By: Netflix, YouTube (DASH), Apple (HLS), Amazon Prime Video (DASH + HLS)</li> </ul> <p>Live Streaming (e.g., Twitch, YouTube Live)</p> <ul> <li>Best: HLS (Low-Latency), DASH (LL-DASH), or WebRTC for real-time interactivity</li> <li>Why? HLS and DASH offer scalability, while WebRTC provides ultra-low latency for live interaction.</li> <li>Used By: Twitch (RTMP for ingest, HLS for playback), YouTube Live (DASH + HLS)</li> </ul> <p>Real-Time Communication (e.g., Zoom, Google Meet)</p> <ul> <li>Best: WebRTC</li> <li>Why? Peer-to-peer, ultra-low latency, no need for external servers.</li> <li>Used By: Google Meet, Zoom, Microsoft Teams</li> </ul> <p>Professional Broadcast / Contribution (e.g., TV Networks, Sports)</p> <ul> <li>Best: SRT or RTMP (for ingest)</li> <li>Why? SRT provides reliable, high-quality, low-latency transmission, while RTMP is still widely supported for ingestion.</li> <li>Used By: ESPN, news networks, live sports broadcasting</li> </ul>"},{"location":"Video%20Processing/docs_200_streaming/#what-does-netflix-use","title":"What Does Netflix Use?","text":"<p>Netflix uses MPEG-DASH for most of its streaming, alongside HLS for Apple devices. Their stack includes:</p> <ul> <li>DASH (MPEG-DASH) \u2013 Adaptive bitrate streaming for optimal playback across devices.</li> <li>HLS (HTTP Live Streaming) \u2013 For Apple devices, since Apple mandates HLS.</li> <li>CDN Optimization \u2013 Uses its own Open Connect CDN to reduce latency and improve video delivery.</li> <li>Per-Title Encoding \u2013 Dynamically adjusts compression based on the content to optimize quality vs. bandwidth.</li> </ul> <p>So HLS can be used to stream an mp4 file to another device?</p> <p>Yes, HLS (HTTP Live Streaming) can be used to stream an MP4 file to another device, but it requires some processing first. </p>"},{"location":"Video%20Processing/docs_200_streaming/#how-hls-streams-an-mp4-file","title":"How HLS Streams an MP4 File","text":"<p>Segment the MP4 File </p> <ul> <li>HLS requires the video to be split into small chunks (typically <code>.ts</code> files, though newer versions support fragmented MP4 <code>.m4s</code>).</li> <li>A tool like FFmpeg can do this.</li> </ul> <p>Create an M3U8 Playlist </p> <ul> <li>HLS uses a <code>.m3u8</code> file (a playlist) that lists the segmented video files.</li> <li>The player downloads and plays the segments in order.</li> </ul> <p>Host the Segments &amp; Playlist on a Server </p> <ul> <li>You need an HTTP server (e.g., Nginx, Apache, or a cloud service like S3) to serve the files.</li> </ul> <p>Play the Stream on Another Device </p> <ul> <li>Open the <code>.m3u8</code> URL in a compatible player (e.g., Safari, VLC, ExoPlayer, or hls.js in a browser).</li> </ul>"},{"location":"Video%20Processing/docs_200_streaming/#example-convert-mp4-to-hls-using-ffmpeg","title":"Example: Convert MP4 to HLS Using FFmpeg","text":"<p><pre><code>ffmpeg -i input.mp4 -codec: copy -start_number 0 -hls_time 10 -hls_list_size 0 -f hls output.m3u8\n</code></pre> - <code>-hls_time 10</code> \u2192 Splits video into 10-second segments. - <code>-hls_list_size 0</code> \u2192 Keeps all segments in the playlist. - <code>output.m3u8</code> \u2192 The playlist file.</p>"},{"location":"Video%20Processing/docs_200_streaming/#how-to-serve-the-stream","title":"How to Serve the Stream","text":"<ol> <li>Move the <code>output.m3u8</code> and <code>.ts</code> files to a web server.</li> <li> <p>Access the stream via: <pre><code>http://yourserver.com/path/output.m3u8\n</code></pre></p> </li> <li> <p>Play it in:</p> </li> </ol> <p>Web browser with <code>hls.js</code> OR    VLC media player (Open Network Stream) OR    iOS Safari (built-in HLS support)</p>"},{"location":"Video%20Processing/docs_200_streaming/#dash-command-with-ffmpeg","title":"\ud83c\udfaf DASH Command with FFmpeg","text":"<p>Run: <pre><code>ffmpeg -i vid_file.mp4 -codec: copy -map 0 -f dash output.mpd\n</code></pre></p>"},{"location":"Video%20Processing/docs_200_streaming/#explanation-of-flags","title":"\ud83d\udd0d Explanation of Flags:","text":"<ul> <li><code>-i vid_file.mp4</code> \u2192 Input video file.</li> <li><code>-codec: copy</code> \u2192 Copies the streams without re-encoding.</li> <li><code>-map 0</code> \u2192 Ensures all streams (audio &amp; video) are included.</li> <li><code>-f dash</code> \u2192 Specifies DASH output format.</li> <li><code>output.mpd</code> \u2192 The DASH manifest file.</li> </ul>"},{"location":"Video%20Processing/docs_200_streaming/#dash-output-files","title":"\ud83d\udcc2 DASH Output Files","text":"<p>This will generate:</p> <ul> <li><code>output.mpd</code> \u2192 The DASH manifest (index) file.</li> <li>Segmented <code>.m4s</code> video/audio chunks.</li> </ul>"},{"location":"Video%20Processing/docs_200_streaming/#serving-dash-with-python-http-server","title":"\ud83d\udda5\ufe0f Serving DASH with Python HTTP Server","text":"<p>Just like HLS, you can serve DASH using:</p> <pre><code>python3 -m http.server 9000\n</code></pre> <p>Then play in VLC: <pre><code>http://localhost:9000/output.mpd\n</code></pre> \u2714\ufe0f VLC supports DASH, so this should work without additional configuration.</p>"},{"location":"Video%20Processing/docs_200_streaming/#play-dash-in-a-browser","title":"\ud83c\udf10 Play DASH in a Browser","text":"<p>Browsers don\u2019t natively support DASH, so you need a DASH player like dash.js.</p> <p>To embed DASH in HTML, use: <pre><code>&lt;video id=\"dashPlayer\" controls&gt;\n    &lt;source src=\"http://localhost:9000/output.mpd\" type=\"application/dash+xml\"&gt;\n&lt;/video&gt;\n&lt;script src=\"https://cdn.dashjs.org/latest/dash.all.min.js\"&gt;&lt;/script&gt;\n&lt;script&gt;\n    var player = dashjs.MediaPlayer().create();\n    player.initialize(document.querySelector(\"#dashPlayer\"), \"http://localhost:9000/output.mpd\", true);\n&lt;/script&gt;\n</code></pre></p>"},{"location":"Video%20Processing/docs_200_streaming/#advanced-dash-multi-bitrate-encoding","title":"\ud83d\udccc Advanced DASH: Multi-Bitrate Encoding","text":"<p>To support adaptive streaming, generate multiple bitrates:</p> <pre><code>ffmpeg -i vid_file.mp4 \\\n  -map 0:v -map 0:a \\\n  -b:v:0 2000k -b:v:1 1000k \\\n  -s:v:0 1920x1080 -s:v:1 1280x720 \\\n  -c:v libx264 -c:a aac \\\n  -f dash -seg_duration 4 -use_timeline 1 -use_template 1 \\\n  output.mpd\n</code></pre> <p>\u2714\ufe0f This creates multiple video quality options that DASH players can switch between dynamically.</p>"},{"location":"Video%20Processing/docs_200_streaming/#summary","title":"\ud83d\ude80 Summary","text":"<ul> <li>Basic DASH: <code>ffmpeg -i vid_file.mp4 -codec: copy -map 0 -f dash output.mpd</code></li> <li>Serve DASH: <code>python3 -m http.server 9000</code></li> <li>Play in VLC: <code>http://localhost:9000/output.mpd</code></li> <li>Play in Browser: Use <code>dash.js</code></li> <li>Adaptive Streaming: Encode multiple bitrates with <code>-b:v</code> and <code>-s:v</code></li> </ul>"},{"location":"self_host/frigate/","title":"Frigate","text":"<p>Open source video security feed.</p> <p>There are several ways to handle streams. Different protocols exist to  handle this.</p> <ul> <li>RTSP - Realtime streaming protocol</li> <li>RTMP - not as common</li> <li>HTTP/MJPEG - video over HTTP, slower.</li> </ul> <p>If a camera can expose a URI to connect to, then Frigate can probably connect. It uses ffmpeg under the hood. Here's an example of how you can try to see if a camera is working over a stream:</p> <pre><code># ffplay is a ffmpeg module\nffplay rtsp://user:password@CAMERA_IP:554/stream_path\n\n# basic view stream over http if no auth required\nffplay http://192.168.1.205:8081/video\n\n# connect with username and password\nffplay http://admin:admin@192.168.1.205:8081/video\n\n# use rtsp\nffplay rtsp://user:password@192.168.1.50:554/stream1\n</code></pre>"},{"location":"self_host/frigate/#docker-compose-setup","title":"Docker Compose Setup","text":"<p>Lets setup Frigate.</p> <p>Create the directory:</p> <pre><code>mkdir /srv/frigate\nmkdir /srv/frigate/config\ntouch /srv/frigate/config/config.yaml\ntouch /srv/frigate/docker-compose.yaml\n</code></pre> <p>Docker Compose file: <pre><code>services:\n  frigate:\n    container_name: frigate\n    image: ghcr.io/blakeblackshear/frigate:stable\n    restart: unless-stopped\n    privileged: true\n    shm_size: \"64mb\"  # needed for ffmpeg buffer\n    ports:\n      - \"5000:5000\"    # Web UI\n      - \"1935:1935\"    # RTMP streaming (optional)\n    environment:\n      - FRIGATE_RTSP_PASSWORD=KineWind88\n      - NVIDIA_VISIBLE_DEVICES=all\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - ./config:/config\n      - ./media:/media/frigate\n      - type: tmpfs\n        target: /tmp/cache\n        tmpfs:\n          size: 1000000000\n    runtime: nvidia  # enable gpu acceleration with nvidia gpu\n</code></pre></p> <p>Once the docker compose is working, the config file is how you add  cameras and adjust settings.</p> <p>config.yaml <pre><code>mqtt:\n  enabled: False\n\ncameras:\n  iphone_camera:\n    ffmpeg:\n      hwaccel_args: []\n      inputs:\n        - path: http://username:password@192.168.1.205:8081/video\n          input_args: -avoid_negative_ts make_zero -fflags nobuffer -flags low_delay -analyzeduration 1000000 -probesize 1000000 -rw_timeout 5000000 -use_wallclock_as_timestamps 1\n          roles:\n            - detect\n            - record\n    detect:\n      width: 1280\n      height: 720\n      fps: 5\n\n    record:\n      enabled: true\n      retain:\n        days: 2\n        mode: motion\n      events:\n        pre_capture: 5   # seconds before motion\n        post_capture: 5  # seconds after motion\n\n    snapshots:\n      enabled: true\n      timestamp: true\n      bounding_box: true\n      retain:\n        default: 2     # keep snapshots for 2 days\n</code></pre></p> <p>This config is used for testing an http stream with an iphone running  ip camera lite. The additional input args are used to turn off gpu  accleration since its not supported by http protocol.</p> <p>Here is a config that uses RTSP instead of HTTP. This is preferred if the camera supports it.</p> <p>config.yaml with RTSP: <pre><code>mqtt:\n  enabled: false\n\ncameras:\n  iphone_camera:\n    ffmpeg:\n      hwaccel_args: preset-nvidia-h264\n      inputs:\n        - path: rtsp://admin:admin@192.168.1.205:8554/live\n          input_args: preset-rtsp-restream\n          roles:\n            - detect\n            - record\n\n    detect:\n      width: 1280\n      height: 720\n      fps: 5\n\n    record:\n      enabled: true\n      retain:\n        days: 2\n        mode: motion\n      events:\n        pre_capture: 5   # seconds before motion\n        post_capture: 5  # seconds after motion\n\n    snapshots:\n      enabled: true\n      timestamp: true\n      bounding_box: true\n      retain:\n        default: 2     # keep snapshots for 2 days\n</code></pre></p>"},{"location":"self_host/nas/","title":"NAS (Network Attached Storage)","text":"<p>A NAS system is a dedicated server that is used ONLY for network storage. It runs a NAS operating system such as TrueNAS, Unraid, or Linux with  <code>mdadm</code> or <code>zfs</code> and has a motherboard that can support multiple hard drives. </p> <p>A NAS system should only be used for storage, not running any apps or processes.</p>"},{"location":"self_host/samba/","title":"Shared File Server","text":"<p>Samba is an open source file server. You can host a network drive to share files across different computers.</p> <p>You can dictate account creations, read/write access. You can create multiple shared folders and assign access to each. This means you could have a share one for people to maybe download photos from but keep a private one for your own files you don't want to share.</p> <p>Warning: There are several different versions of samba, some are older  and have bugs. The version below is current as of 2025-10-24.</p> <p>Here is a good config. It does the following:</p> <ul> <li>Uses .env file for user creds.</li> <li>Mounts several different directories for managing public/private access.</li> <li>Allows a guest to view files without any creds.</li> </ul> <pre><code>services:\n  samba:\n    image: ghcr.io/servercontainers/samba:latest\n    container_name: samba\n    restart: unless-stopped\n\n    # --- SMB port ---\n    ports:\n      - \"445:445\"            # SMB direct (modern clients only)\n\n    # --- Host folders mapped into container ---\n    volumes:\n      - /srv/plex_media:/mount/media\n      - /srv/samba_file_share/shared_private:/mount/shared_private\n      - /srv/samba_file_share/shared_public:/mount/shared_public\n\n    # --- User and share definitions ---\n    environment:\n      # --- Create the admin account ---\n      ACCOUNT_tyler: \"${ADMIN_PASS}\"     # user=tyler, password from .env or defaults to 'changeme'\n      UID_tyler: 1000                              # match your host user ID if you like\n\n      # --- MEDIA share (authenticated only) ---\n      SAMBA_VOLUME_CONFIG_Media: &gt;\n        [Media];\n        path = /mount/media;\n        read only = no;\n        browsable = yes;\n        guest ok = no;\n        valid users = tyler;\n        create mask = 0666;\n        directory mask = 0777\n\n      # --- PRIVATE share (authenticated only) ---\n      SAMBA_VOLUME_CONFIG_Private: &gt;\n        [Private];\n        path = /mount/shared_private;\n        read only = no;\n        browsable = yes;\n        guest ok = no;\n        valid users = tyler;\n        create mask = 0666;\n        directory mask = 0777\n\n      # --- PUBLIC share (guest access allowed) ---\n      SAMBA_VOLUME_CONFIG_Public: &gt;\n        [Public];\n        path = /mount/shared_public;\n        read only = no;\n        browsable = yes;\n        guest ok = yes;\n        public = yes;\n        available = yes;\n        create mask = 0666;\n        directory mask = 0777\n\n    # Optional logging/diagnostics\n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n</code></pre>"},{"location":"self_host/samba/#connect-other-computers","title":"Connect Other Computers","text":"<p>There are two methods of connecting to a samba shared drive.</p> <ol> <li>Session based (temporary logged).</li> <li>Auto mount (stays logged in).</li> </ol> <p>To connect your other computers to the shared drive.</p>"},{"location":"self_host/samba/#mac","title":"Mac","text":"<p>Temporary login:</p> <ul> <li>In Finder, select Go from the drop down menu.</li> <li>Select \"Connect to a Server\".</li> <li>Enter the SMB path like <code>smb://&lt;server-ip&gt;/Media</code> or <code>smb://&lt;server-ip&gt;/Private</code></li> <li>Click \"Connect\".</li> <li>Enter credentials.</li> </ul> <p>Full auto-mount login:</p> <p>If you prefer the graphical route:</p> <ul> <li>Mount the shares once manually in Finder (\u2318 + K \u2192 connect)</li> <li>Open System Settings \u2192 Users &amp; Groups \u2192 Login Items</li> <li>Under Open at Login, click +</li> <li>Select your mounted share(s)</li> </ul> <p>They\u2019ll automatically connect when you log in.</p>"},{"location":"self_host/samba/#windows","title":"Windows","text":"<ul> <li>Open File Explorer</li> <li>Right-click This PC \u2192 Map network drive\u2026</li> <li>Assign a drive letter. Choose a letter (for example <code>M:</code> for Media, <code>P:</code> for Private).</li> <li>Enter the network path For example:</li> </ul> <pre><code>\\\\192.168.1.50\\Media\n</code></pre> <p>or</p> <pre><code>\\\\192.168.1.50\\Private\n</code></pre> <ul> <li>Authentication</li> <li>For the Media share: check \u201cConnect using different credentials\u201d and log in as guest (no password).</li> <li>For the Private share: log in as <code>tyler</code> with your Samba password.</li> <li>Check \u201cReconnect at sign-in\u201d \u2014 Windows will mount these every time you log in.</li> </ul>"},{"location":"self_host/storage_vs_compute/","title":"General Architecture - Storage vs Compute","text":"<p>TLDR: Separate storage into a dedicated NAS and then use another server for running programs on docker. Pull from and save to the NAS. A NAS should not be used for compute processes.</p>"},{"location":"self_host/storage_vs_compute/#the-core-idea-separate-compute-from-storage","title":"\ud83e\udde9 The Core Idea: Separate Compute from Storage","text":"<p>\u2699\ufe0f Compute = running apps, containers, AI detection, transcoding, etc. \ud83d\udcbe Storage = reliable, redundant data layer (your NAS).</p> <p>When you keep these roles separate:</p> <ul> <li>Each system can specialize in what it does best.</li> <li>You reduce the risk of data loss or performance issues.</li> <li>It\u2019s far easier to scale or troubleshoot later.</li> </ul>"},{"location":"self_host/storage_vs_compute/#why-its-best-to-keep-nas-dedicated","title":"\ud83e\uddf1 Why It\u2019s Best to Keep NAS Dedicated","text":""},{"location":"self_host/storage_vs_compute/#1-nas-file-systems-zfs-btrfs-etc-are-tuned-for-data-integrity-not-app-workloads","title":"\ud83e\udde0 1. NAS file systems (ZFS, Btrfs, etc.) are tuned for data integrity, not app workloads","text":"<p>They protect your data through:</p> <ul> <li>checksums</li> <li>scrubbing</li> <li>snapshots</li> <li>RAID/parity</li> </ul> <p>Running constant compute (e.g., Frigate writing video, or Immich generating thumbnails) can interfere with these optimizations and make rebuilds or snapshots slower.</p>"},{"location":"self_host/storage_vs_compute/#2-cpu-and-memory-resources-get-shared","title":"\u26a1 2. CPU and memory resources get shared","text":"<p>NAS tasks like ZFS compression, parity calculations, and indexing already use a fair bit of CPU/RAM. If you also run:</p> <ul> <li>Frigate (AI inference + decoding)</li> <li>Immich (image processing + face recognition)</li> <li>or Jellyfin (video transcoding)</li> </ul> <p>\u2026then your NAS\u2019s primary job \u2014 protecting and serving data \u2014 might get delayed or throttled.</p>"},{"location":"self_host/storage_vs_compute/#3-stability-and-safety","title":"\ud83d\udd12 3. Stability and safety","text":"<p>If one container crashes, hangs, or fills the filesystem, it can:</p> <ul> <li>Interrupt the NAS\u2019s storage pools</li> <li>Corrupt running writes</li> <li>Even cause pool degradation if disks get overloaded</li> </ul> <p>Keeping your NAS \u201cclean\u201d isolates your data from application risk.</p>"},{"location":"self_host/storage_vs_compute/#the-professional-model-and-the-smart-home-lab-model","title":"\ud83e\udde0 The Professional Model (and the Smart Home-Lab Model)","text":"Layer Purpose Examples Compute Layer Run containers, AI, video processing Frigate, Immich, Home Assistant, Plex Storage Layer (NAS) Serve and protect data TrueNAS, Unraid, Synology, ZFS pool Network Layer Link them via fast Ethernet 1 Gb or 2.5 Gb+ LAN, NFS or SMB shares <p>They\u2019re connected by the network \u2014 the NAS exports shared folders, and the compute node mounts them.</p>"},{"location":"self_host/storage_vs_compute/#typical-setup-example","title":"\ud83e\udde9 Typical Setup Example","text":"<pre><code>        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502  Cameras / Clients \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502\n            \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n            \u2502 Frigate \u2502   \u2190 Compute (Docker, GPU)\n            \u2502 Immich  \u2502\n            \u2502 Plex    \u2502\n            \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n                 \u2502  (NFS/SMB)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502      NAS        \u2502  \u2190 Storage (ZFS/RAID, backups)\n        \u2502  TrueNAS / RAID \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"self_host/storage_vs_compute/#when-you-can-combine-them","title":"\ud83e\udde9 When You Can Combine Them","text":"<p>It\u2019s perfectly fine to run extra apps on your NAS if:</p> <ul> <li>It\u2019s powerful (e.g., Xeon/i7 CPU, 32 GB+ RAM)</li> <li>You\u2019re okay with minor slowdowns</li> <li>The apps aren\u2019t CPU-intensive (e.g., photo viewer, simple sync service)</li> </ul> <p>For example:</p> <ul> <li>Synology users often run Docker containers on the NAS directly (for convenience).</li> <li>Unraid and TrueNAS SCALE support running VMs or containers safely alongside storage \u2014 but it still shares the same hardware, so performance depends on your use case.</li> </ul>"},{"location":"self_host/storage_vs_compute/#in-short","title":"\u2705 In short","text":"<p>For maximum reliability and scalability, yes \u2014 it\u2019s best to have:</p> <ul> <li>\ud83d\udda5\ufe0f One machine dedicated to compute (Frigate, Immich, Plex, etc.)</li> <li>\ud83d\udcbe One machine dedicated to storage (your NAS)</li> </ul> <p>This mirrors how enterprise datacenters and well-designed homelabs are built \u2014 storage servers feed compute servers.</p> <p>Would you like me to show what a recommended 2-node home setup looks like (with network speeds, mount types, and backup flow)? It\u2019s a great visual for planning how to physically wire and configure it.</p>"},{"location":"self_host/syncthing/","title":"Syncthing","text":"<p>syncthing - Realtime file sync between your devices.  Creates its own files on each device and syncs changes between all devices.  It is direct peer-to-peer file sync, meaning there is no server hosting  the files and copying between devices. </p> <p>Whats the difference between samba and syncthing?</p> <p>TLDR: Samba keeps a central place for files, and you need internet access.  Syncthing stores copies of the files on each device and you can use them offline.</p>"},{"location":"self_host/syncthing/#samba-or-smb-file-sharing","title":"\ud83e\udde9 Samba (or SMB file sharing)","text":"<ul> <li>Acts like a network drive or shared folder.</li> <li>Files live in one place (usually on your server).</li> <li>Other devices access them over the network.</li> <li>When you open or edit a file, it\u2019s being read/written directly on the server.</li> <li>If you\u2019re offline or disconnected from the network, you can\u2019t access the files.</li> <li>Great for: centralized storage, multi-user access, shared drives.</li> </ul> <p>\ud83d\udda5\ufe0f Analogy: Like a shared filing cabinet \u2014 everyone opens the same drawer to get files.</p>"},{"location":"self_host/syncthing/#syncthing_1","title":"\ud83d\udd01 Syncthing","text":"<ul> <li>Each device has its own local copy of the files.</li> <li>Changes on one device automatically sync to all others.</li> <li>You can work offline \u2014 edits will sync once devices reconnect.</li> <li>Works over local network or internet, with encryption.</li> <li>Great for: personal sync, backups, or distributed teams that need offline access.</li> </ul> <p>\ud83d\udce6 Analogy: Everyone has a copy of the same binder \u2014 when someone updates a page, everyone else\u2019s binder updates too.</p>"},{"location":"self_host/syncthing/#summary-comparison","title":"\u26a1 Summary comparison","text":"Feature Samba Syncthing File location Central server Each device has a copy Offline access \u274c No \u2705 Yes Sync direction None (live access only) Two-way sync Network needed Local (LAN) LAN or Internet (peer-to-peer) Use case Shared drive File replication / personal sync Security LAN authentication End-to-end encryption Storage usage One copy on server Multiple copies across devices <ul> <li>Samba = one shared \u201clive\u201d copy (best for collaboration on a central server).</li> <li>Syncthing = synced local copies (best for redundancy and offline access).</li> </ul> <p>To use:</p> <p>Install on each device you want to use it with: https://syncthing.net/</p>"},{"location":"self_host/video_streaming/","title":"Video Streaming Service","text":""},{"location":"self_host/video_streaming/#jellyfin","title":"Jellyfin","text":"<p>Docker compose file: <pre><code>services:\n  jellyfin:\n    image: jellyfin/jellyfin\n    container_name: jellyfin\n    user: \"1000:1000\"\n    network_mode: 'host'\n    volumes:\n      - /srv/jellyfin/config:/config\n      - /srv/jellyfin/cache:/cache\n      - type: bind\n        source: /srv/plex_media\n        target: /plex_media\n        read_only: true\n    restart: 'unless-stopped'\n    # Optional - alternative address used for autodiscovery\n    environment:\n      - JELLYFIN_PublishedServerUrl=https://jelly.healthfin.solutions\n</code></pre></p> <p>Some notes: Jellyfin requires a valid domain to be used if you want to cast to Chromecast.  This is because Chromecast's require a valid certificate to stream from. To avoid this you would have to install your self signed certificate onto your casting device.</p> <p>Steps:</p> <ol> <li>Host jellyfin</li> <li>Setup Caddy to be a reverse proxy from your domain to the jellyfin IP.</li> <li>Caddy will automatically issue a valid certificate for this allowing you to stream.</li> </ol>"},{"location":"self_host/video_streaming/#plex","title":"Plex","text":"<p>Docker compose file:</p> <pre><code>services:\n  plex:\n    container_name: plex\n    image: linuxserver/plex\n    environment:\n      - PUID=1000           # Replace with your user ID\n      - PGID=1000           # Replace with your group ID\n      - VERSION=docker\n      - PLEX_CLAIM=claim-fzdegAmx-S6iP68ysfp2\n    volumes:\n      - /srv/plex_media:/media    # Your media files\n      - /srv/plex/config:/config   # Plex configuration files\n    network_mode: host\n    restart: unless-stopped\n</code></pre> <p>Plex does a lot of the certificate management stuff for you. You can connect your server to the Plex system to stream, but its not quite a true self hosted solution. After you setup your plex server you must go to Plex.com and sign up then add your server to your account under their domain. You still end up relying on their system. It does come with a lot of other free content through Plex which is a benefit.</p>"},{"location":"self_host/video_streaming/#notes","title":"Notes","text":"<p>Note that both of these require some kind of external internet to work. If you want to truely isolated setup that will work if the internet as a whole goes down, you will need streaming devices on your TV that can install the Jellyfin client app. </p> <p>These include:</p> <ul> <li>NVIDIA Shield TV (best all-around).</li> <li>Cheap Android TV box (Amlogic, etc.) sideloaded with Jellyfin app.</li> <li>Intel NUC, Raspberry Pi, or old mini PC hooked up via HDMI and running Linux + Jellyfin Desktop/Browser.</li> </ul>"},{"location":"self_host/web_server/","title":"Web Servers","text":"<p>A web server is a server that serves data over the HTTP/HTTPS protocols. Typically it is HTML, JSON, XML, or some other type of data. </p>"},{"location":"self_host/web_server/#reverse-proxy","title":"Reverse Proxy","text":"<p>A reverse proxy is a process that will reroute network traffic that flows through the HTTP/HTTPS protocols, to other machines, using IP addresses. One example is if you setup a proxy server to take any networks requests to the public ip address A, it will take those requests and push them to the machine with local ip address B.</p> <p>Think of it like a concierge for your website. You have a website welcome.com. When someone goes to welcome.com, the public DNS will use that name to go to the public IP you configured, and then when the traffic reaches your server, your concierge takes them to the correct room (machine).</p>"},{"location":"self_host/web_server/#caddy","title":"Caddy","text":"<p>Caddy is a very easy web server/reverse proxy. It handles certificate creation automatically which is super helpful. It also can be used to distribute traffic for load balancing, serve static web files, handle simple auth, and more.</p> <p>The way you setup caddy is with docker compose:</p>"},{"location":"self_host/web_server/#docker-compose","title":"Docker Compose","text":"<pre><code>services:\n  caddy:\n    image: caddy:latest\n    container_name: caddy\n    ports:\n      - \"80:80\"        # HTTP\n      - \"443:443\"\n      - \"8009:8009\"\n    volumes:\n      - ./Caddyfile:/etc/caddy/Caddyfile   # Mount your Caddyfile\n      - ./data:/data                       # Let\u2019s Encrypt certificates\n      - ./config:/config                   # Caddy configuration\n      - /srv/web_apps:/srv/web_apps        # custom directory I want caddy to have access to for static web pages\n    dns:\n      - 1.1.1.1\n      - 8.8.8.8\n    restart: unless-stopped\n</code></pre>"},{"location":"self_host/web_server/#caddyfile","title":"Caddyfile","text":"<p>Once the server is running, the caddyfile is the base config for what you want it to do. Create a file called <code>Caddyfile</code> and then update based on what you want. Here are the offical docs: https://caddyserver.com/docs/caddyfile</p> <p>Basic File: <pre><code>localhost {\n    respond \"Hello, world!\"\n}\n\nlocalhost:2016 {\n    respond \"Goodbye, world!\"\n}\n</code></pre></p>"},{"location":"self_host/web_server/#certificates","title":"Certificates","text":"<p>Using certain names in your caddyfile have important caveats. For example, LetsEncrypt, the certificate authority that Caddy uses will not issue certificates for localhost.  They only do for public domains. Also your browser will show a \"Not Trusted\" message when you try to go to it. </p> <p>You can get a local cert program like mkcert and set that up.</p> <p>Caddy has a self signed cert with TLS. You can use this like below:</p> <pre><code>localhost {\n    root * /srv/web_apps/dev_site\n    file_server\n    tls internal\n}\n</code></pre>"},{"location":"self_host/web_server/#authentication","title":"Authentication","text":"<p>You can add basic HTTP authentication to any of your routes. Use bcrypt to generate a password hash like </p> <pre><code># with caddy\ncaddy hash-password --plaintext \"yourpassword\"\n\n# docker\nsudo docker exec -it caddy caddy hash-password --plaintext \"yourpassword\"\n</code></pre> <p>Then add your auth to your route in Caddyfile. <pre><code># add auth to entire site\nyourdomain.com {\n    basicauth {\n        user password_hash\n    }\n\n    root * /srv/web_apps/site\n    file_server\n}\n\n# add auth to specific route\nexample.com {\n    handle /admin* {\n        basicauth {\n            user password_hash\n        }\n        root * /srv/web_apps/admin\n        file_server\n    }\n\n    handle / {\n        root * /srv/web_apps/public\n        file_server\n    }\n}\n</code></pre></p>"},{"location":"self_host/web_server/#full-file","title":"Full File","text":"<pre><code>(basic_auth_users) {\n    @protected {\n        not path /public_notes*   # everything EXCEPT /public_notes*\n    }\n\n    basic_auth @protected  {\n            user password_hash\n        }\n    }\n\nhome.server {\n    tls internal\n\n    handle / {\n        root * /srv/web_apps/home\n        file_server\n    }\n\n    handle /test* {\n        respond \"Hello from test path\"\n    }\n\n    handle /notes* {\n        root * /srv/web_apps\n        file_server browse\n    }\n\n    handle {\n        respond \"\u274c Unmatched path: {uri}\" 404\n    }\n}\n\n\nhealthfin.solutions {\n    handle /public_notes* {\n            root * /srv/web_apps\n            file_server\n        }\n\n    import basic_auth_users\n\n    handle / {\n        root * /srv/web_apps/home\n        file_server\n    }\n\n    handle /confetti {\n        rewrite * /confetti.html\n        root * /srv/web_apps/home\n        file_server\n    }\n\n    handle /timeline {\n        rewrite * /timeline.html\n        root * /srv/web_apps/home\n        file_server\n    }\n\n    handle /notes* {\n            root * /srv/web_apps\n            file_server\n        }\n\n    handle {\n        respond \"\u274c Unmatched path: {uri}\" 404\n    }\n}\n\n\n# billboard analytics\nbillboard.anderson {\n    tls internal\n    reverse_proxy 192.168.1.104:8000\n}\n\nbillboard.healthfin.solutions {\n    import basic_auth_users\n\n    reverse_proxy 192.168.1.104:8000 {\n        header_up Host {http.request.host}  # fixes the issue of static files in the website.\n    }\n}\n\n# immich\npics.anderson {\n    tls internal\n    reverse_proxy 192.168.1.104:2283\n}\n\npics.healthfin.solutions {\n    import basic_auth_users\n\n    reverse_proxy 192.168.1.104:2283 {\n        header_up Host {http.request.host}  # fixes the issue of static files in the website.\n    }\n}\n\n# LLM\nollama.anderson {\n    tls internal\n    reverse_proxy 192.168.1.104:3001\n}\n\nai.healthfin.solutions {\n    reverse_proxy 192.168.1.104:3001\n}\n\nyt.healthfin.solutions {\n    import basic_auth_users\n    reverse_proxy 192.168.1.104:8091\n}\n\njelly.healthfin.solutions {\n    reverse_proxy 192.168.1.104:8096\n}\n\ndocs.anderson {\n    tls internal\n    root * /srv/web_apps/notes\n    file_server\n}\n\ndns.anderson {\n    import basic_auth_users\n    tls internal\n    reverse_proxy 192.168.1.104:3000\n}\n\nresources.anderson {\n    import basic_auth_users\n    tls internal\n    reverse_proxy 192.168.1.104:19999\n}\n\njdownloader.anderson {\n    import basic_auth_users\n    tls internal\n    reverse_proxy 192.168.1.104:5800\n}\n\ntorrent.anderson {\n    import basic_auth_users\n    tls internal\n    reverse_proxy 192.168.1.104:8080\n}\n</code></pre>"},{"location":"self_host/wireguard/","title":"Wireguard","text":"<p>Wireguard (wg) is an open source VPN software.</p> <p>You'll need to update the docker compose file with your hosts public ip and also create a bcrypt hashed password. You can do the password with python like this:</p> <pre><code>python3 -m pip install bcrypt\npython3 -c \"import bcrypt; print(bcrypt.hashpw(b'yourpassword', bcrypt.gensalt()).decode())\"\n</code></pre> <pre><code>volumes:\n  etc_wireguard:\n\nservices:\n  wg-easy:\n    environment:\n      # Change Language:\n      # (Supports: en, ua, ru, tr, no, pl, fr, de, ca, es, ko, vi, nl, is, pt, chs, cht, it, th, hi, ja, si)\n      - LANG=en\n      # \u26a0\ufe0f Required:\n      # Change this to your host's public address\n      - WG_HOST=&lt;host.public.ip.address&gt;\n\n      # Optional:\n      - PASSWORD_HASH=&lt;password_hash&gt;\n      # - PORT=51821\n      # - WG_PORT=51820\n      # - WG_CONFIG_PORT=92820\n      # - WG_DEFAULT_ADDRESS=10.8.0.x\n      # - WG_DEFAULT_DNS=1.1.1.1\n      # - WG_MTU=1420\n      # - WG_ALLOWED_IPS=192.168.15.0/24, 10.0.1.0/24\n      # - WG_PERSISTENT_KEEPALIVE=25\n      # - WG_PRE_UP=echo \"Pre Up\" &gt; /etc/wireguard/pre-up.txt\n      # - WG_POST_UP=echo \"Post Up\" &gt; /etc/wireguard/post-up.txt\n      # - WG_PRE_DOWN=echo \"Pre Down\" &gt; /etc/wireguard/pre-down.txt\n      # - WG_POST_DOWN=echo \"Post Down\" &gt; /etc/wireguard/post-down.txt\n      # - UI_TRAFFIC_STATS=true\n      # - UI_CHART_TYPE=0 # (0 Charts disabled, 1 # Line chart, 2 # Area chart, 3 # Bar chart)\n      # - WG_ENABLE_ONE_TIME_LINKS=true\n      # - UI_ENABLE_SORT_CLIENTS=true\n      # - WG_ENABLE_EXPIRES_TIME=true\n      # - ENABLE_PROMETHEUS_METRICS=false\n      # - PROMETHEUS_METRICS_PASSWORD=$$2a$$12$$vkvKpeEAHD78gasyawIod.1leBMKg8sBwKW.pQyNsq78bXV3INf2G # (needs double $$, hash of 'prometheus_password'; see \"How_to_generate_an_bcrypt_hash.md\" for generate the hash)\n\n    image: ghcr.io/wg-easy/wg-easy\n    container_name: wg-easy-compose\n    volumes:\n      - etc_wireguard:/etc/wireguard\n    ports:\n      - \"51820:51820/udp\"\n      - \"51821:51821/tcp\"\n    restart: unless-stopped\n    cap_add:\n      - NET_ADMIN\n      - SYS_MODULE\n      # - NET_RAW # \u26a0\ufe0f Uncomment if using Podman\n    sysctls:\n      - net.ipv4.ip_forward=1\n      - net.ipv4.conf.all.src_valid_mark=1\n</code></pre> <p>Once the container is running you'll need to setup other machines to have access. </p> <ol> <li>Go to the UI: http://:51821 <li>Add Client. Name it something descriptive. wg-easy automatically:</li> <li>Creates a new private/public key pair for that client</li> <li>Assigns a VPN IP address</li> <li>Generates a .conf file and a QR code</li> <li>Connect Devices</li>"},{"location":"self_host/wireguard/#desktop-laptop","title":"\ud83e\uddd1\u200d\ud83d\udcbb Desktop / Laptop","text":"<ol> <li> <p>Install WireGuard:</p> <ul> <li>macOS \u2192 App Store (\u201cWireGuard\u201d)</li> <li>Windows \u2192 wireguard.com/install</li> <li>Linux \u2192 <code>sudo apt install wireguard</code></li> </ul> </li> <li> <p>In wg-easy, click \u201cDownload config\u201d for your client.    It\u2019ll be a file like <code>tyler-laptop.conf</code>.</p> </li> <li> <p>Open the WireGuard app \u2192 \u201cImport Tunnel\u201d \u2192 select that file.</p> </li> <li> <p>Toggle Activate to connect.</p> </li> </ol>"},{"location":"self_host/wireguard/#mobile-iphone-android","title":"\ud83d\udcf1 Mobile (iPhone / Android)","text":"<ol> <li>Install the official WireGuard app.</li> <li>On the wg-easy dashboard, click \u201cShow QR code\u201d next to the client.</li> <li>In the WireGuard app \u2192 \u201cAdd Tunnel\u201d \u2192 \u201cScan from QR Code.\u201d</li> <li>Scan it, save, and toggle Activate to connect.</li> </ol> <p>That\u2019s it \u2014 your phone now routes through your home network via VPN!</p>"},{"location":"self_host/wireguard/#4-test-the-connection","title":"\ud83c\udf0e 4. Test the Connection","text":"<p>Once connected:</p> <ul> <li>Visit https://ipinfo.io \u2014 it should show your home\u2019s public IP, not your phone/laptop\u2019s local network IP.</li> <li>If you can reach internal devices like <code>192.168.x.x</code>, that means local routing is working too.</li> </ul>"},{"location":"self_host/wireguard/#5-optional-enable-port-forwarding","title":"\u2699\ufe0f 5. (Optional) Enable Port Forwarding","text":"<p>If you want to access wg-easy from outside your LAN:</p> <ul> <li>Forward UDP port 51820 on your router \u2192 your server\u2019s LAN IP.</li> <li> <p>(Optionally) Forward TCP 51821 if you want to reach the dashboard remotely.</p> <ul> <li>Though best practice: leave 51821 closed and use a VPN client only.</li> </ul> </li> </ul>"},{"location":"self_host/wireguard/#6-add-more-clients-later","title":"\ud83d\udd10 6. Add More Clients Later","text":"<p>Each client is isolated, so you can add as many as you want:</p> <ul> <li>\u201cAdd client\u201d \u2192 scan QR or download config \u2192 done.   wg-easy handles all the routing automatically.</li> </ul>"}]}